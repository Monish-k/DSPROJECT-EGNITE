2021,6,25,Text Classification: Predicting ‘Good’ or ‘Bad’ Statements using Natural Language Processing,https://towardsdatascience.com/text-classification-predicting-good-or-bad-statements-using-natural-language-processing-e3a4edb07118?source=rss----7f60cf5620c9---4,Create and train an NLP model using spaCy library to predict and classify the input linesPicture by Ramdlon from PixbayThis blog will cover a very fundamental method of predicting whether the given input statement should be classified as ‘Good’ or ‘Bad’. To do so we will first train a Natural Language Processing (NLP) model utilizing the past dataset. In this way how about we begin!Pre-requisite:You should be aware of the BOW (Bag of Word) approach. You may check [1] out for more details. BOW approach essentially converts the text to numeric making it simpler for the NLP model to learn.In this tutorial Google Colab is used to run the script. You may choose any other platform of your choice. Also the scripting language used is Python.The DatasetAs this is a very introductory blog I have written the dataset on my own with only 7 rows in total as shown in table 1. Once you get familiar with the essentials I highly recommend that you select a bigger dataset and try to apply similar processing to acquire experiences. Kaggle is a great spot to track down countless datasets.https://medium.com/media/07ccc45f15f4f5bc387bfa265d48c982/hrefPython ScriptYou can find the entire codebase in this Github repository. In this blog I will explain the important code snippets only. If still need assistant with other parts of the code do comment and I will be happy to help.We are going to use the spaCy package. It is a free open-source library for Natural Language Processing in Python. I highly recommend you to visit this package’s website and just have a quick glance as to what this offers.After importing the dataset we will create a blank model. We are using a text categorizer as provided by spaCy. Note that there are many versions of text categorizer. But we are going to use textcat as shown in the code below. textcatis being used because we want to predict only one true label which will be either Good or Bad.nlp = spacy.blank(&quot;en&quot;)  #model is named as nlp# text categroizer wit standard settingstextcat = nlp.create_pipe(&quot;textcat&quot; config={                &quot;exclusive_classes&quot;: True                &quot;architecture&quot;: &quot;bow&quot;}) #bow = bag of wordsnlp.add_pipe(textcat) #add textcat to nlpTraining the ModelIn order to train a model you will need an optimizer and for that the spaCy package comes to the rescue. An optimizer will keep on updating the model during the training phase using minibatch function. Notice the code below that does what we just talked about.from spacy.util import minibatchoptimizer = nlp.begin_training() #create optmizer to be used by spacy to update the modelbatches = minibatch(train_data size=8) #spacy provides minibatch fnfor batch in batches:    texts labels = zip(*batch)    nlp.update(texts labels sgd=optimizer)Making PredictionsIn the previous step we trained the model with the input dataset. Once that is completed you can use the trained model to make predictions on input statements or lines using predict() method as shown below:# i mentioned all lines to be predicted in a &#39;texts&#39; array Lines = [&quot;I look awesome&quot;] docs = [nlp.tokenizer(text)for text in Lines]      textcat = nlp.get_pipe(&#39;textcat&#39;) scores _ = textcat.predict(docs)  #Prob score for both classes (Good/bad) print(scores)Also do notice one thing. When the above code runs the output will be something like:[[0.50299996 0.49700007]]The above output is the probability score for both the class labels. In the current scenario the possible labels are- Good or Bad. According to the above output the probability of the given input line being Good is more (0.50299996) and hence the model classifies the line as Good.To make the predictions more direct let’s print the class label for the given input line instead of probability scores.predicted_labels = scores.argmax(axis=1)print([textcat.labels[label] for label in predicted_labels])And you will see the output as below:[&#39;Good&#39;]Next StepsAs a matter of first importance Congratulations! You just learned how to construct a text classifier using the spaCy library. There are absolutely numerous alternate approaches to do as such and I will come up with more instructional blogs later on. Until further notice I would like to request my readers to take the learnings from this instructional exercise and apply it to some relatively greater datasets.References[1] Ismayil M. (2021 February 10). From Words To Vectors — Towards Data Science. Medium. https://towardsdatascience.com/from-words-to-vectors-e24f0977193eText Classification: Predicting ‘Good’ or ‘Bad’ Statements using Natural Language Processing was originally published in Towards Data Science on Medium where people are continuing the conversation by highlighting and responding to this story.
2021,6,25,The Best Machine Learning Blogs & Resources To Follow,https://towardsdatascience.com/the-best-machine-learning-blogs-resources-to-follow-f8177ab4f08f?source=rss----7f60cf5620c9---4,Resources That Are Constantly Being UpdatedContinue reading on Towards Data Science »
2021,6,25,Speed Up your Command-Line Navigation with These 3 Tools,https://towardsdatascience.com/speed-up-your-command-line-navigation-with-these-3-tools-f90105c9aa2b?source=rss----7f60cf5620c9---4,Fix your Typo in One Word Save your Command-line Snippet and Much More!Continue reading on Towards Data Science »
2021,6,25,Functional Connectivity and Similarity Analysis of Human Brain (Part-III),https://towardsdatascience.com/functional-connectivity-and-similarity-analysis-of-human-brain-part-iii-c427c88ca5bb?source=rss----7f60cf5620c9---4,Spatial Analysis of the Human BrainSource: https://nilearn.github.io/auto_examples/03_connectivity/plot_multi_subject_connectome.html#sphx-glr-auto-examples-03-connectivity-plot-multi-subject-connectome-pyMaterialsThis is the third article of the series namely “Cognitive Computational Modelling for Spatio-Temporal fMRI in Ventral Temporal Cortex”. If you want to check out the whole series go to the following link.cankocagil/Cognitive-Computational-Modelling-for-Spatio-Temporal-fMRI-in-Ventral-Temporal-CortexI will introduce the topic of functional connectivity and similarity analysis their use case in research of brain decoding. Let’s get started.All related materials are hosted in my github page. Don’t forget to check it out. If you are paper lover you can read the paper version of this series of articles that can also be found in my repo.cankocagil/Cognitive-Computational-Modelling-for-Spatio-Temporal-fMRI-in-Ventral-Temporal-CortexBefore diving into analysis and code let’s manipulate the Haxby dataset a little bit and apply ventral temporal masks to extract signals from region of interests of the human brain. The installation and importing part is done in previous article. If you did not check out my previous articles I think stop here and take a quick look. But don’t worry. All parts are “nearly” independent from each other. Let’s start coding.https://medium.com/media/ca18e4b261e0c0b41c3eee1ef29fb506/hrefHere we fetch the Haxby dataset. Next we’ll understand the data structure and convert it to NumPy array for further processing.https://medium.com/media/e67bc664a3f0802a2a31cb30e3d14344/hrefAs we can see there 8 categories. (We’ll ignore “rest” category as it did not provide additional information.)https://medium.com/media/58476b6e03495e55bd211b15ba06f2fc/hrefLet’s remove the “rest” condition data and explore the shape of the data as follows.https://medium.com/media/d5f12bad9bdabb207de31267c4d98f4d/hrefSo there are 864 samples that connected temporally i.e. time series data. The fMRI data in fixed time have 40x64x64 dimension where 40 refers to the depth of the 3-D image and 64s refer to spatial dimensions. Hence we are 4-D time series image data. Recall that there are 6 subjects in the experiment so let’s look at the all subjects as follows.https://medium.com/media/73fd5d35a2bf04dd7d4ea3e8ab0846d4/hrefSo all subjects have 864 time series data. Please refer the part-I for detailed description of the dataset.Then let’s perform masking to extract region of interest to reduce the dimensionality of the fMRI. Masked fMRI samples represent the region where neural activity possible occurred.https://medium.com/media/d95e9c9daf5dc968e8624a87c74c2a34/hrefYes this script enables us toFetch fMRI data and convert into NumPy matrixCreate and apply spatio-temporal masks to extract region of interestsPrepare the supervisions (target/labels)So let’s run this function and get the data.https://medium.com/media/d7357d2aeedbceed7f22e80dbb8c4537/hrefYes! Finally we prepared our dataset. Now we can perform any analysis we want. In this article we’ll perform functional connectivity and similarity analysis of the human brain.Functional Connectivity and Similarity AnalysisFunctional connectivity is defined as the temporal dependency of neuronal activation patterns of anatomically separated brain regions and in the past years an increasing body of neuroimaging studies has started to explore functional connectivity by measuring the level of co-activation of resting-state fMRI time-series between brain regions [23]. These functional connections are important in establishing statistical connections in brain regions. Functional connectivity can be obtained by estimating a covariance (or correlation) matrix for signals from different brain regions decomposed for example on resting-state or naturalistic- stimuli datasets. Here we performed functional connectivity analysis based on the correlation precision and partial correlation. Then similarity analysis based on the cosine minkowski and euclidean distance is performed to further extend statistical findings in masked fMRI data.Functional Connectivity: CorrelationFunctional connectivity based on pearson correlation is performed on subject 1. We can see that in the ventral temporal cortex of subject 1 there are strong correlations when the stimuli of faces are presented.https://medium.com/media/a09c8f0fb21f093a7b2f0375a76f606b/hrefCorrelation Matrix (Image By Author)Functional Connectivity: PrecisionAs shown in the papers [20 24] it is more interesting to use the inverse covariance matrix i.e. the precision matrix. It gives only direct connections between regions as it contains partial covariances which are covariances between two regions conditioned on all the others. Moreover we performed functional connectome based on precision score to extract signals on RoI’s of subject 1 . Here with the change in the connectivity measure we see direct changes in spatial correlations in the ventral cortex of subject 1. With precision measure we further get understanding in brain organization and brain networks.https://medium.com/media/81192d624a68d4c0f9fd7df41da411df/hrefPrecision Matrix (Image By Author)Functional Connectivity: Partial CorrelationAmong the range of network modeling methods partial correlation has shown great promises in accurately detecting true brain network connections [25]. So we performed functional connectivity analysis based on partial correlation. Visualization of partial correlation in RoI fMRI data demonstrate that the ventral temporal cortex of the subject 1 is not much correlated.https://medium.com/media/a239a9b30bb6ce83ce865c855651b186/hrefPartial Correlation Matrix (Image By Author)Similarity Analysis: Cosine DistanceTo facilitate the geodesic understanding in the context of statistical connections in the brain we performed cosine similarity analysis on subject 1 and the obtained matrix is visualized. The results demonstrate that there are highly overlapping regions in terms of neural activity when visual stimuli is presented.https://medium.com/media/3c3b437e29041f8580ec024fce5f8ad2/hrefCosine Matrix (Image By Author)Similarity Analysis: Minkowski DistanceTo experiment with different similarity metrics we utilized the minkowski distance that is a generalization of both the Euclidean and the Manhattan distance. Hence it is useful in fMRI temporal similarity analysis.https://medium.com/media/106a27168d5bc512ad972f90b78b2366/hrefMinkowski Matrix (Image By Author)Similarity Analysis: Euclidean DistanceLastly we performed similarity analysis based on classical euclidean distance. It is a very classical measure of the distance in terms of cartesian coordinates of the points using the Pythagorean theorem [13]. From the statistical and structural patterns exposed by functional connectivity and similarity analysis we can conclude that the neural activity evoked in the ventral temporal cortex of the human brain is highly overlapping and distributed.https://medium.com/media/b277e3721543c7ee87d9ffcea62af1b3/hrefEuclidean Matrix (Image By Author)Yeah! That’s it for this article. I covered functional connectivity and similarity analysis techniques for fMRI data in depth.Congratulations! You completed the third article and took a step through cognitive computational approaches for decoding human brain.In the next article we’ll perform unsupervised representation learning to extract latency in human brain.Links of ArticlesPublished ArticlesIntroduction to Cognitive Computational Modelling of Human Brain (Part-I)2.Discovery Neuroimaging Analysis (Part-II)3.Functional Connectivity and Similarity Analysis of Human Brain (Part-III)2. On the Way (Coming soon…)Placeholder for Part-IVPlaceholder for Part-VFurther Readinghttps://www.hindawi.com/journals/cmmm/2012/961257/The following list of references are utilized in my research for both machine learning and neuroscience sides. I highly recommend copy-and-paste the references and review in brief.References[1] J. L. Ba J. R. Kiros and G. E. Hinton. Layer normalization 2016.[2] L. Buitinck G. Louppe M. Blondel F. Pedregosa A. Mueller O. Grisel V. Niculae P. Prettenhofer A. Gramfort J. Grobler R. Layton J. VanderPlas A. Joly B. Holt 10 and G. Varoquaux. API design for machine learning software: experiences from the scikit-learn project. In ECML PKDD Workshop: Languages for Data Mining and Machine Learning pages 108–122 2013.[3] X. Chu Z. Tian Y. Wang B. Zhang H. Ren X. Wei H. Xia and C. Shen. Twins: Revisiting the design of spatial attention in vision transformers 2021.[4] K. Crammer O. Dekel J. Keshet S. Shalev-Shwartz and Y. Singer. Online passive aggressive algorithms. 2006.[5] K. J. Friston. Statistical parametric mapping. 1994.[6] C. G. Gross C. d. Rocha-Miranda and D. Bender. Visual properties of neurons in inferotemporal cortex of the macaque. Journal of neurophysiology 35(1):96–111 1972.[7] S. J. Hanson T. Matsuka and J. V. Haxby. Combinatorial codes in ventral temporal lobe for object recognition.[8] J. Haxby M. Gobbini M. Furey A. Ishai J. Schouten and P. Pietrini. ”visual object recognition” 2018.[9] R. A. Heckemann J. V. Hajnal P. Aljabar D. Rueckert and A. Hammers. Automatic anatomical brain mri segmentation combining label propagation and decision fusion. NeuroImage 33(1):115–126 2006.[10] D. Hendrycks and K. Gimpel. Gaussian error linear units (gelus) 2020.[11] S. Huang W. Shao M.-L. Wang and D.-Q. Zhang. fmribased decoding of visual information from human brain activity: A brief review. International Journal of Automation and Computing pages 1–15 2021.[12] R. Koster M. J. Chadwick Y. Chen D. Berron A. Banino E. Duzel D. Hassabis and D. Kumaran. Big-loop recurrence ¨ within the hippocampal system supports integration of information across episodes. Neuron 99(6):1342–1354 2018.[13] E. Maor. The Pythagorean theorem: a 4000-year history. Princeton University Press 2019.[14] K. A. Norman S. M. Polyn G. J. Detre and J. V. Haxby. Beyond mind-reading: multi-voxel pattern analysis of fmri data. Trends in cognitive sciences 10(9):424–430 2006.[15] A. J. O’toole F. Jiang H. Abdi and J. V. Haxby. Partially distributed representations of objects and faces in ventral temporal cortex. Journal of cognitive neuroscience 17(4):580–590 2005.[16] F. Pedregosa G. Varoquaux A. Gramfort V. Michel B. Thirion O. Grisel M. Blondel P. Prettenhofer R. Weiss V. Dubourg J. Vanderplas A. Passos D. Cournapeau M. Brucher M. Perrot and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research 12:2825–2830 2011.[17] R. A. Poldrack. Region of interest analysis for fmri. Social cognitive and affective neuroscience 2(1):67–70 2007.[18] M. Poustchi-Amin S. A. Mirowitz J. J. Brown R. C. McKinstry and T. Li. Principles and applications of echo-planar imaging: a review for the general radiologist. Radiographics 21(3):767–779 2001.[19] R. P. Reddy A. R. Mathulla and J. Rajeswaran. A pilot study of perspective taking and emotional contagion in mental health professionals: Glass brain view of empathy. Indian Journal of Psychological Medicine page 0253717620973380 2021.[20] S. M. Smith K. L. Miller G. Salimi-Khorshidi M. Webster C. F. Beckmann T. E. Nichols J. D. Ramsey and M. W. Woolrich. Network modelling methods for fmri. Neuroimage 54(2):875–891 2011.[21] K. Tanaka. Inferotemporal cortex and object vision. Annual review of neuroscience 19(1):109–139 1996.[22] M. S. Treder. Mvpa-light: a classification and regression toolbox for multi-dimensional data. Frontiers in Neuroscience 14:289 2020.[23] M. P. Van Den Heuvel and H. E. H. Pol. Exploring the brain network: a review on resting-state fmri functional connectivity. European neuropsychopharmacology 20(8):519–534 2010.[24] G. Varoquaux A. Gramfort J. B. Poline and B. Thirion. Brain covariance selection: better individual functional connectivity models using population prior. arXiv preprint arXiv:1008.5071 2010.[25] Y. Wang J. Kang P. B. Kemmer and Y. Guo. An efficient and reliable statistical method for estimating functional connectivity in large scale brain networks using partial correlation. Frontiers in neuroscience 10:123 2016.[26] S. Wold K. Esbensen and P. Geladi. Principal component analysis. Chemometrics and intelligent laboratory systems 2(1–3):37–52 1987.[27] S. Wold K. Esbensen and P. Geladi. Principal component analysis. Chemometrics and intelligent laboratory systems 2(1–3):37–52 1987.Functional Connectivity and Similarity Analysis of Human Brain (Part-III) was originally published in Towards Data Science on Medium where people are continuing the conversation by highlighting and responding to this story.
2021,6,25,Discovery Neuroimaging Analysis (Part-II),https://towardsdatascience.com/discovery-neuroimaging-analysis-part-ii-b2cdbdc6e6c3?source=rss----7f60cf5620c9---4,Echo-Planar Imaging (Image By Author)MaterialsThis is the second article of the series namely “Cognitive Computational Modelling for Spatio-Temporal fMRI in Ventral Temporal Cortex”. If you want to check out the whole series go to the following link.cankocagil/Cognitive-Computational-Modelling-for-Spatio-Temporal-fMRI-in-Ventral-Temporal-CortexI will introduce the topic of discovery neuroimaging analysis their use case in research of brain decoding. Let’s get started.All related materials are hosted in my github page. Don’t forget check it out. If you are paper lover you can read the paper version of this series of articles that can also be found in my repo.cankocagil/Cognitive-Computational-Modelling-for-Spatio-Temporal-fMRI-in-Ventral-Temporal-CortexIn this article we utilize state-of-the-art explanatory neuroimaging technologies such as echo-planar region-of-interest (RoI) statistical map anatomical and glass brain methods to visualize and pre-analyze the visual structure of fMRI samples.As always in the case ML problem it is crucial to understand analyze and visualize the data before processing.Here we utilize the “Nilearn” framework that is statistical computing and neuroimaging platform built on top of ”Scikit-Learn” to enable researchers to manage mask preprocess and decode neuroscientific data. In the previous article I showed how to install and import Nilearn framework. Don&#39;t worry. I will repeat the process in this article also. Let’s get started.Discovery fMRI Analysis by NeuroimagingI performed pre-analysis based on the neuroimaging technologies to visualize the dataset. To accomplish that I utilized the Echo-planar Averaging for 4-D visualization region of interest (RoI) statistical map anatomic glass brain visualization tools embedded in statistical learning and neuroimaging framework.From now on I introduce the different technologies and visualization tools with its simple yet effective implementations in Python. Fasten the belts!But before that in the case of you missed the last article (part-I) I provide installation guidelines as follows.https://medium.com/media/4e1d1900606bc69f98d184d0de16035e/hrefhttps://medium.com/media/ffb22af01a5f9bfcf387df13acd0d9c3/hrefhttps://medium.com/media/06ba8647e6bfa8b33b9a5884d449cc8e/hrefNow we can start actual visualization process.4-D Visualization of fMRI Volume and EPIEcho-planar imaging is a very fast magnetic resonance (MR) imaging technique capable of acquiring an entire MR image in only a fraction of a second [18]. In single-shot echo-planar imaging all the spatial-encoding data of an image can be obtained after a single radio-frequency excitation [18]. Here we visualize the EPI for subjects of the Haxby experiment from cuts of frontal axial and lateral regions. EPI visualization provides realistic insights from the activated regions in the brain that plays a crucial role in spatio-temporal brain decoding. I provide same image as in the beginning of the article for the sake of simplicity.https://medium.com/media/f99e22ec2ed33fe9f10e9082799e2828/hreffMRI Volume (Image By Author)https://medium.com/media/458ac7730b0a04ceca1b591b179e4494/hrefEcho-Planar Imaging (Image By Author)Region of Interest AnalysisA common way to analyze fMRI data is performing the region of interest (RoI) analysis that involves the extraction of signals from specified areas. The most theoretically agnostic use of ROI analysis is to simply explore the underlying signal behind a whole-brain voxel-wise analysis [17]. After extracting statistically meaningful areas we can perform severity of correction for multiple statistical tests instead of large number of voxels in brain. Further most ML decoders are performed on the RoI’s of subjects instead of whole brain medium.https://medium.com/media/67a45345c1b7c1a939091001761cb501/hrefRoI Visualization (Image By Author)Statistical MapsStatistical Parametric Mapping refers to the construction and assessment of spatially extended statistical processes used to test hypotheses about functional imaging data [5]. Generally the prior step in statistical fMRI is to create a thresholded statistical map representing the regions that are active (above a threshold) [17]. Hence it is useful in examining differences in brain activity recorded during neuroscientific experiments.https://medium.com/media/bc37d251dcadabdc9bc2e00ef2212bbe/hrefStatistical Map Visualization (Image By Author)Direct fMRI VisualizationsSimple and compact visualization of fMRI data is quite important topic in the context of neuroimaging since it enables researchers to view cortical brain activity. So I directly plot the temporally averaged fMRI data of subject 2 to further visualization.https://medium.com/media/0b9ef9db0babba3692f82cabe4fd1862/hrefVisualization of the fMRI data (Image By Author)Anatomic VisualizationsI visualized the anatomical structure of the fMRI (by default 3 cuts: Frontal Axial and Lateral) obtained by the temporally averaged fMRI data of the subject 2 to generate insights before decoding.https://medium.com/media/145787f0a2b6c37e672678a8025f4d3d/hrefAnatomic Visualizations (Image By Author)Glass BrainThe Glass Brain is a state-of-the-art real-time brain visualization technology that is created on the Unity 3D game-engine and powered by NVIDIA’s GPU computing [19]. Its inputs include an individual’s brain structure both tissue and fiber tract architecture obtained from high-resolution MRI-DTI brain scans. Real-time brain activity and functional interactions among networks are superimposed on the brain structure using high-density EEG (electroencephalography) [19]. Here I project frontal axial and lateral sides of temporally averaged fMRI data of subject 5.https://medium.com/media/43355cc40503680f4d3e755aaa74d735/hrefGlass Brain Visual (Image By Author)If you want to go further and visualize the data in interactive 3D fashion here is the code.https://medium.com/media/bb7f8f647b058983166ef34fa69fdf5e/hrefYeah! That’s it for this article. I covered most common visualization techniques for fMRI data in depth.Congratulations! You completed the second article and took a step through cognitive computational approaches for decoding human brain.In the next article we’ll perform functional connectivity and similarity analysis to further capture statistical properties for human brain.Links of ArticlesPublished ArticlesIntroduction to Cognitive Computational Modelling of Human Brain (Part-I)2.Discovery Neuroimaging Analysis (Part-II)3.Functional Connectivity and Similarity Analysis of Human Brain (Part-III)2. On the Way (Coming soon…)Placeholder for Part-IVPlaceholder for Part-VFurther Readinghttps://www.hindawi.com/journals/cmmm/2012/961257/The following list of references are utilized in my research for both machine learning and neuroscience sides. I highly recommend copy-and-paste the references and review in brief.References[1] J. L. Ba J. R. Kiros and G. E. Hinton. Layer normalization 2016.[2] L. Buitinck G. Louppe M. Blondel F. Pedregosa A. Mueller O. Grisel V. Niculae P. Prettenhofer A. Gramfort J. Grobler R. Layton J. VanderPlas A. Joly B. Holt 10 and G. Varoquaux. API design for machine learning software: experiences from the scikit-learn project. In ECML PKDD Workshop: Languages for Data Mining and Machine Learning pages 108–122 2013.[3] X. Chu Z. Tian Y. Wang B. Zhang H. Ren X. Wei H. Xia and C. Shen. Twins: Revisiting the design of spatial attention in vision transformers 2021.[4] K. Crammer O. Dekel J. Keshet S. Shalev-Shwartz and Y. Singer. Online passive aggressive algorithms. 2006.[5] K. J. Friston. Statistical parametric mapping. 1994.[6] C. G. Gross C. d. Rocha-Miranda and D. Bender. Visual properties of neurons in inferotemporal cortex of the macaque. Journal of neurophysiology 35(1):96–111 1972.[7] S. J. Hanson T. Matsuka and J. V. Haxby. Combinatorial codes in ventral temporal lobe for object recognition.[8] J. Haxby M. Gobbini M. Furey A. Ishai J. Schouten and P. Pietrini. ”visual object recognition” 2018.[9] R. A. Heckemann J. V. Hajnal P. Aljabar D. Rueckert and A. Hammers. Automatic anatomical brain mri segmentation combining label propagation and decision fusion. NeuroImage 33(1):115–126 2006.[10] D. Hendrycks and K. Gimpel. Gaussian error linear units (gelus) 2020.[11] S. Huang W. Shao M.-L. Wang and D.-Q. Zhang. fmribased decoding of visual information from human brain activity: A brief review. International Journal of Automation and Computing pages 1–15 2021.[12] R. Koster M. J. Chadwick Y. Chen D. Berron A. Banino E. Duzel D. Hassabis and D. Kumaran. Big-loop recurrence ¨ within the hippocampal system supports integration of information across episodes. Neuron 99(6):1342–1354 2018.[13] E. Maor. The Pythagorean theorem: a 4000-year history. Princeton University Press 2019.[14] K. A. Norman S. M. Polyn G. J. Detre and J. V. Haxby. Beyond mind-reading: multi-voxel pattern analysis of fmri data. Trends in cognitive sciences 10(9):424–430 2006.[15] A. J. O’toole F. Jiang H. Abdi and J. V. Haxby. Partially distributed representations of objects and faces in ventral temporal cortex. Journal of cognitive neuroscience 17(4):580–590 2005.[16] F. Pedregosa G. Varoquaux A. Gramfort V. Michel B. Thirion O. Grisel M. Blondel P. Prettenhofer R. Weiss V. Dubourg J. Vanderplas A. Passos D. Cournapeau M. Brucher M. Perrot and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research 12:2825–2830 2011.[17] R. A. Poldrack. Region of interest analysis for fmri. Social cognitive and affective neuroscience 2(1):67–70 2007.[18] M. Poustchi-Amin S. A. Mirowitz J. J. Brown R. C. McKinstry and T. Li. Principles and applications of echo-planar imaging: a review for the general radiologist. Radiographics 21(3):767–779 2001.[19] R. P. Reddy A. R. Mathulla and J. Rajeswaran. A pilot study of perspective taking and emotional contagion in mental health professionals: Glass brain view of empathy. Indian Journal of Psychological Medicine page 0253717620973380 2021.[20] S. M. Smith K. L. Miller G. Salimi-Khorshidi M. Webster C. F. Beckmann T. E. Nichols J. D. Ramsey and M. W. Woolrich. Network modelling methods for fmri. Neuroimage 54(2):875–891 2011.[21] K. Tanaka. Inferotemporal cortex and object vision. Annual review of neuroscience 19(1):109–139 1996.[22] M. S. Treder. Mvpa-light: a classification and regression toolbox for multi-dimensional data. Frontiers in Neuroscience 14:289 2020.[23] M. P. Van Den Heuvel and H. E. H. Pol. Exploring the brain network: a review on resting-state fmri functional connectivity. European neuropsychopharmacology 20(8):519–534 2010.[24] G. Varoquaux A. Gramfort J. B. Poline and B. Thirion. Brain covariance selection: better individual functional connectivity models using population prior. arXiv preprint arXiv:1008.5071 2010.[25] Y. Wang J. Kang P. B. Kemmer and Y. Guo. An efficient and reliable statistical method for estimating functional connectivity in large scale brain networks using partial correlation. Frontiers in neuroscience 10:123 2016.[26] S. Wold K. Esbensen and P. Geladi. Principal component analysis. Chemometrics and intelligent laboratory systems 2(1–3):37–52 1987.[27] S. Wold K. Esbensen and P. Geladi. Principal component analysis. Chemometrics and intelligent laboratory systems 2(1–3):37–52 1987.Discovery Neuroimaging Analysis (Part-II) was originally published in Towards Data Science on Medium where people are continuing the conversation by highlighting and responding to this story.
2021,6,25,Introduction to Cognitive Computational Modelling of Human Brain (Part-I),https://towardsdatascience.com/introduction-to-cognitive-computational-modelling-of-human-brain-part-i-90c61e0e24c9?source=rss----7f60cf5620c9---4,Visualization of Anatomic fMRI (Image By Author)MaterialsThis is the very first article of the series namely “Cognitive Computational Modelling for Spatio-Temporal fMRI in Ventral Temporal Cortex”. If you want to check out the whole series go to the following link.Cognitive Computational Modelling for Spatio-Temporal fMRI in Ventral Temporal CortexI will introduce the topic of cognitive computational modelling and its use case in the research of brain decoding. Let’s get started.All related materials are hosted in my github page. Don’t forget to check it out. If you are paper lover you can read the paper version of this series of articles that can also be found in my repo.cankocagil/Cognitive-Computational-Modelling-for-Spatio-Temporal-fMRI-in-Ventral-Temporal-CortexThe ventral temporal cortex in the human brain is selective to the different representations of the visual stimuli from nature and ventral object vision pathway generates distributed and overlapping neural responses [21]. Single cell studies are conducted to demonstrate that the differential tuning of individual neurons in the ventral temporal cortex in nonhuman primates are selective the objects from different kinds and form representative features [6 21]. However their order of selectivity is not generalizable and scalable to higher degree of object representations [8]. To model neuro architecture of the ventral cortex statistical algorithms are developed but the uncertainty in pathway remains. Recent developments regarding neuroimaging have demonstrated that spatio-temporal decoding of human’s perception memories and thoughts are decodable via functional magnetic resonance imaging (fMRI) methods [11]. However the complexity and the distribution of fMRI data require sophisticated scientific tools because of its neural capacity of the spatio-temporal resolution. With the advancements of machine learning neuroscientists discover statistical and structural patterns in large scale fMRI datasets to solve various tasks in the context of neuroscience. Further recent advances in deep learning enable researchers to solve unsolved neuroscientific tasks [12] and concretely show the importance of deep learning. In this study we build an end-to-end discovery machine learning pipelines to decode the category of visual stimuli viewed by a human subject based on fMRI data. We utilize the state-of-the-art explanatory neuroimaging technologies such as echo-planar region-of interest (RoI) statistical map anatomical and glass brain methods to visualize and pre-analyze the visual structure of fMRI samples.My experiments are based on a block-designed 4-D time-series fMRI dataset namely Haxby dataset [7 15 8] from the study of face and object representation. It consists of 6 subjects with 12 runs per subject [8]. In each run the subjects passively viewed grey-scale images of eight object categories grouped in 24s blocks separated by rest periods [8 7]. Each image was shown for 500ms and was followed by a 1500ms inter-stimulus interval [7]. Full-brain fMRI data were recorded with a volume repetition time of 2.5s thus a stimulus block was covered by roughly 9 volumes [8]. It consist of per-subject high resolution anatomical images except for the sixth 4D fMRI time-series image data in the shape of 1452 volumes with 40x64x64 voxels (corresponding to a voxel size of 3.5 x 3.75 x 3.75 mm and a volume repetition time of 2.5 seconds) [8]. We have 8 different stimuli categories that are scissors face cat scrambled pix bottle chair shoe and house. The chunks of resting-state is eliminated as it provides no additional information on decoding visual stimuli [8].Examples of Visual Stimuli (Image By Author)Before diving into Python code of fetching Haxby dataset and its exploratory fMRI analysis let’s look at from the bird’s eye view to whole analysis and how cognitive computational modelling can be performed in the context of neural decoding.1. Discovery Neuroimaging AnalysisAs discovery neuroimaging analysis we performed the state-of-the-art explanatory neuroimaging technologies such as echo-planar region-of interest (RoI) statistical map anatomical and glass brain methods to visualize and pre-analyze the visual structure of fMRI samples. We’ll discuss in depth in part-II of our article series.2. Functional Connectivity and Similarity Analysis of Ventral Temporal CortexWe performed functional connectivity analysis based on the correlation precision and partial correlation and similarity analysis based on the cosine minkowski and euclidean distance to discover overlapping representation in ventral temporal cortex.This is quite useful in explanatory fMRI analysis as it shows how distributed regions in human brain shares similar features from statistical and mathematical perspective. We’ll discuss in depth in part-III of our article series.3. Manifold Learning and Dimension Reduction in the Distributed Regions in Human BrainManifold learning and dimensionality reduction methods are performed on the per subject ventral temporal masks to extract latent variables of spatio-temporal masks that will help further decoding of human brain. As dimensionality reduction methods we applied Principal Component Analysis (PCA) Linear Discriminant Analysis (LDA) Independent Component Analysis (ICA) Non-Negative Matrix Factorization (NNMF) and Multidimensional Scaling (MDS) then compare these obtained subspaces by their 3D visualization. Additionally we performed manifold learning algorithms to extract underlying manifold distribution in masked ventral temporal regions. We performed t-Stochastic Neighbour Embedding (t-SNE) Uniform Manifold Approximation and Projection (UMAP) ISOMAP Locally Linear Embedding (LLE) and Spectral Embedding (SE) then compare their lower dimensional manifolds by their 3D visualizations that further help in the decoding process.This will give comprehensive introduction to understand geodesic relation in ventral temporal cortex of human brain. From the ML perspective it will be a depth review of the intersection of the unsupervised learning and human brain. We’ll discuss in depth in part-IV of our article series.At this point we only performed discovery analysis to understand fMRI data samples and its distribution. Next we’ll dive into decoding process.4. Spatio-Temporal fMRI Decoding: ML &amp; DL AlgorithmsEnd-to-end machine learning algorithms are developed to categorize the stimuli based on distributed and overlapping regions in the ventral temporal cortex. Precisely we performed the following machine learning algorithms; Linear support vector classifier (LinearSVC) Stochastic Gradient Descent Classifier (SGDClassifier) Multi-Layer Perceptron (MLP) Perceptron Logistic Regression Logistic Regression Cross-Validation Support Vector Classifier (SVC) Calibrated Classifier (Probability calibration with isotonic regression) Passive Aggressive Classifier Label Propagation Classifier Random Forest Classifier Gradient Boosting Classifier Quadratic Discriminant Classifier Ridge Classifier Cross-Validation Ridge Classifier AdaBoost Classifier Extra Trees Classifier K-Neighbors Classifier Bernoulli Naive Bayes Classifier Gaussian Naive Bayes Classifier Nu-Support Vector Classifier Nearest Centroid Classifier and Bagging Classifier. As a robust ensemble decoding we applied novel ensemble of regularized models; FREM: Cross-Validated Ensemble of L2 regularized SVCs FREM: Cross-Validated Ensemble of L2 regularized Logistic Regressions. We further constructed cognitive neural networks precisely MLPs with GELU nonlinearity [10] 2D and 3D Convolutional Neural Network by taking the advantage of interactions between different streams of visual representations. We’ll discuss in depth in part-V of our article series and this will be a last article of our series.Yes that’s a huge. I know but it is crucial to perform many decoding experiments and compare their results. No worries we’ll implement nearly all ML algorithms in just “two” line of codes. Yeah I am serious. We’ll implement nearly all algorithms in just a two lines of code. The power of the ML tools!Let’s start coding. In this article we’ll only download Haxby dataset from web with just one line of code using “nilearn” framework that will be introduced later in series of articles and explore the structure of the dataset.First thing first we need to install necessary Python packages. Open your favorite Jupyter Notebook and copy-and-paste the following code for necessary installations.https://medium.com/media/4e1d1900606bc69f98d184d0de16035e/hrefThen let’s import all necessary packages that we will be using along this journey. If you want to save your results create a folders called “images” and “results” or you can just remove the lines 66–67 in below.https://medium.com/media/ffb22af01a5f9bfcf387df13acd0d9c3/hrefWe are ready to start! Please read the docstring below for Haxby dataset understanding it may not be easy to understand in first glimpse but no worries. We’ll discuss more later on. Note that downloading data from web takes approximately 30 minutes depends on your download speed etc.https://medium.com/media/06ba8647e6bfa8b33b9a5884d449cc8e/hrefYes we download fMRI dataset. When you print haxby_dataset you’ll see a screen like that.https://medium.com/media/62e480f79b71f1373f30d40be793d505/hrefLet’s briefly discuss the output. Detailed description are made in above and you can also go the references for Haxby dataset. There areanat: Anatomical structure of fMRI datafunc: Nifti Images of fMRI data (Will be converted NumPy Matrix)session_target: Files corresponds to our target variable (Will be discussed later)mask mask_vt mask_faces … (Different spatial masks for extracting activated areas in ventral temporal cortex)Let’s dive deeper. Here we print the filenames of the fMRI data of the subjects. Don’t worry about “nii.gz” format. It is just a efficient representation of the fMRI data. We’ll decode this extension using “Nilearn” library with ease and convert into NumPy array.https://medium.com/media/e83ef9075e1795dfbd11bb061a9809fc/hrefThat’s it for this article. We covered how and why we are constructing spatio-temporal computational techniques for understanding human brain. We downloaded Haxby dataset from web. Discussed about the dataset. Finally briefly reviewed the dataset. Congratulations! You completed the first article and took a step through cognitive computational approaches for decoding human brain.In the next article we’ll analyze and visualize the fMRI dataset using state-of-the-art neuroimaging approaches.Links of ArticlesPublished ArticlesIntroduction to Cognitive Computational Modelling of Human Brain (Part-I)2.Discovery Neuroimaging Analysis (Part-II)3.Functional Connectivity and Similarity Analysis of Human Brain (Part-III)2. On the Way (Coming soon…)Placeholder for Part-IVPlaceholder for Part-VFurther Readinghttps://www.hindawi.com/journals/cmmm/2012/961257/The following list of references are utilized in my research for both machine learning and neuroscience sides. I highly recommend copy-and-paste the references and review in brief.References[1] J. L. Ba J. R. Kiros and G. E. Hinton. Layer normalization 2016.[2] L. Buitinck G. Louppe M. Blondel F. Pedregosa A. Mueller O. Grisel V. Niculae P. Prettenhofer A. Gramfort J. Grobler R. Layton J. VanderPlas A. Joly B. Holt 10 and G. Varoquaux. API design for machine learning software: experiences from the scikit-learn project. In ECML PKDD Workshop: Languages for Data Mining and Machine Learning pages 108–122 2013.[3] X. Chu Z. Tian Y. Wang B. Zhang H. Ren X. Wei H. Xia and C. Shen. Twins: Revisiting the design of spatial attention in vision transformers 2021.[4] K. Crammer O. Dekel J. Keshet S. Shalev-Shwartz and Y. Singer. Online passive aggressive algorithms. 2006.[5] K. J. Friston. Statistical parametric mapping. 1994.[6] C. G. Gross C. d. Rocha-Miranda and D. Bender. Visual properties of neurons in inferotemporal cortex of the macaque. Journal of neurophysiology 35(1):96–111 1972.[7] S. J. Hanson T. Matsuka and J. V. Haxby. Combinatorial codes in ventral temporal lobe for object recognition.[8] J. Haxby M. Gobbini M. Furey A. Ishai J. Schouten and P. Pietrini. ”visual object recognition” 2018.[9] R. A. Heckemann J. V. Hajnal P. Aljabar D. Rueckert and A. Hammers. Automatic anatomical brain mri segmentation combining label propagation and decision fusion. NeuroImage 33(1):115–126 2006.[10] D. Hendrycks and K. Gimpel. Gaussian error linear units (gelus) 2020.[11] S. Huang W. Shao M.-L. Wang and D.-Q. Zhang. fmribased decoding of visual information from human brain activity: A brief review. International Journal of Automation and Computing pages 1–15 2021.[12] R. Koster M. J. Chadwick Y. Chen D. Berron A. Banino E. Duzel D. Hassabis and D. Kumaran. Big-loop recurrence ¨ within the hippocampal system supports integration of information across episodes. Neuron 99(6):1342–1354 2018.[13] E. Maor. The Pythagorean theorem: a 4000-year history. Princeton University Press 2019.[14] K. A. Norman S. M. Polyn G. J. Detre and J. V. Haxby. Beyond mind-reading: multi-voxel pattern analysis of fmri data. Trends in cognitive sciences 10(9):424–430 2006.[15] A. J. O’toole F. Jiang H. Abdi and J. V. Haxby. Partially distributed representations of objects and faces in ventral temporal cortex. Journal of cognitive neuroscience 17(4):580–590 2005.[16] F. Pedregosa G. Varoquaux A. Gramfort V. Michel B. Thirion O. Grisel M. Blondel P. Prettenhofer R. Weiss V. Dubourg J. Vanderplas A. Passos D. Cournapeau M. Brucher M. Perrot and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research 12:2825–2830 2011.[17] R. A. Poldrack. Region of interest analysis for fmri. Social cognitive and affective neuroscience 2(1):67–70 2007.[18] M. Poustchi-Amin S. A. Mirowitz J. J. Brown R. C. McKinstry and T. Li. Principles and applications of echo-planar imaging: a review for the general radiologist. Radiographics 21(3):767–779 2001.[19] R. P. Reddy A. R. Mathulla and J. Rajeswaran. A pilot study of perspective taking and emotional contagion in mental health professionals: Glass brain view of empathy. Indian Journal of Psychological Medicine page 0253717620973380 2021.[20] S. M. Smith K. L. Miller G. Salimi-Khorshidi M. Webster C. F. Beckmann T. E. Nichols J. D. Ramsey and M. W. Woolrich. Network modelling methods for fmri. Neuroimage 54(2):875–891 2011.[21] K. Tanaka. Inferotemporal cortex and object vision. Annual review of neuroscience 19(1):109–139 1996.[22] M. S. Treder. Mvpa-light: a classification and regression toolbox for multi-dimensional data. Frontiers in Neuroscience 14:289 2020.[23] M. P. Van Den Heuvel and H. E. H. Pol. Exploring the brain network: a review on resting-state fmri functional connectivity. European neuropsychopharmacology 20(8):519–534 2010.[24] G. Varoquaux A. Gramfort J. B. Poline and B. Thirion. Brain covariance selection: better individual functional connectivity models using population prior. arXiv preprint arXiv:1008.5071 2010.[25] Y. Wang J. Kang P. B. Kemmer and Y. Guo. An efficient and reliable statistical method for estimating functional connectivity in large scale brain networks using partial correlation. Frontiers in neuroscience 10:123 2016.[26] S. Wold K. Esbensen and P. Geladi. Principal component analysis. Chemometrics and intelligent laboratory systems 2(1–3):37–52 1987.[27] S. Wold K. Esbensen and P. Geladi. Principal component analysis. Chemometrics and intelligent laboratory systems 2(1–3):37–52 1987.Introduction to Cognitive Computational Modelling of Human Brain (Part-I) was originally published in Towards Data Science on Medium where people are continuing the conversation by highlighting and responding to this story.
2021,6,25,Visualizing Satellite Data Using Matplotlib and Cartopy,https://towardsdatascience.com/visualizing-satellite-data-using-matplotlib-and-cartopy-8274acb07b84?source=rss----7f60cf5620c9---4,Explore our changing climate using Python and passive microwave dataContinue reading on Towards Data Science »
2021,6,25,First Steps Into Building Your Data Science Portfolio Website,https://towardsdatascience.com/first-steps-into-building-your-data-science-portfolio-website-94c3443597ff?source=rss----7f60cf5620c9---4,Becoming a World-Class Data Scientistimage from Shutterstock.comI’ve been in some discussions lately where data scientists complain about how uneasy it is to secure a data science job. Meanwhile if you’re someone like me who doesn’t possess any relevant degree in Technology then the idea of building a portfolio website of your own should hit your mind a way of telling a story about yourself to improve your chances of getting noticed by employers and to make you stand out among other data scientists in the market.image from Shutterstock.comData Science Portfolio should be seen to contain more and more real-life projects and less fancy designs unlike web developers&#39; or designers’ portfolios.As we data scientists most times only focus on data analysis visualization and modeling getting to build a website from scratch might be a little bit difficult since I personally decided to build from scratch so as to be vast with the area of web development. However there are dynamic and robust portfolio builders out there you can easily use like urspace.io WordPress Squarespace and many more.As far as the idea of building this can be so great and intriguing after reading the above few lines there are some things you definitely need to know in order to get started.Getting StartedWebsite StructureFirst thing before getting started you must have an idea of what you want your website to look like. A rough structure of what you want to see and display on your pages how to configure the things potential recruiters and employers will definitely be looking for. You can go on the web search and check the layouts of some popular sites. The best way is to check other people’s portfolio websites for inspiration. I did a lot of research while trying to build mine. You can check it out here. Including the ones that helped me out;Julia NikulskiNatasha Selvaraj.Define the Pages or RoutesQuite some pages are mandatory to be included in your websites.Home Page: This is the starting/landing page of your website. This should include a brief introduction of who you are and few direct links to other pages on your website.About Page: This is where you tell the world your story qualifications interests skills journey and many other things you’d like to say about yourself. This page should have the ability to engage and inform your site visitors in a personal and friendly way.Portfolio Page: If truly you’re building a data science portfolio then this has to be the best of all your pages. Since most of us aspiring data scientists don’t have experience working with great companies however your lists of projects should show and tell that truly you’re well vast with all the skills highlighted in the about section.On this page of each project links to deployed projects codes on GitHub also if you’ve written articles related to the project such links are advised to be added. Regardless you’re expected to add only distinct and unique projects. Kaggle Titanic Challenge House Pricing Challenge Iris Dataset for flower classification MNIST digit classification and other beginners’ related projects are not to be seen on the page as these wouldn’t make you better off to other data scientists out there.project from my portfolio websiteThe above image shows how each of the projects in my project portfolio was placed in a container images shrunk to a specific size brief description of the project skills used highlighted at the bottom with two links one to the blog post and the other to the codes on GitHub. These are just to make your portfolio impressive as it needs to be.Blog Page: This can be a link directing site visitors to your blog but can also be a page containing all your written articles. Whichever way the presence of your blog page is required.Contact Me: For potential employers to reach you this page should do the job. This should contain your contact details and your online presence. Links to your profile on at least GitHub LinkedIn and Twitter are expected to be there.Precious Kolawole’s Github profileThis is nothing to talk about as we are all walking towards perfection hoping this would be better in months to come but having a good GitHub profile with many contributions on your repositories would make you count as a competitive data scientist. LinkedIn has been one of the best places for getting connections and securing jobs having an outstanding profile is a plus.Start BuildingI was able to build this using HTML CSS Bootstrap Python &amp; Flask. However there’s no cause for alarm if you have little knowledge of any of these programming languages and frameworks. The explanation below should be able to get you started.Flask App: pertaining to all the pages and routes mentioned above this small flexible lightweight python web framework is available for creating web applications in python. You haven’t worked with Flask before? I have a tutorial for you ranging from library installations instantiating the application getting it live on your local machine. Check here for the link.https://medium.com/media/fa2404f772c3f19277afda90a6450c32/hrefIn the run.py file above this is where we created all pages/sections. We then used render template a flask function known as a Jinja 2 engine templating package for adding HTML files to our pages for displaying outputs from the template files found in the application’s templates folder.Templates Folder: contained in this folder are your HTML files to be displayed on each of your pages. Mandatorily a layout.html which is going to serve as a parent template should be created. This is going to contain the layouts that are supposed to be repeated on each of the pages one of the numerous examples is the navigation bars which has to be implemented once and must be seen on all pages. This helps instead of writing the entire HTML structure in each template. Here is an example of what layout.html looks like;https://medium.com/media/522669c6861519625094682a8e828f76/hrefHaving known this layout.html will be extended and each page in the application will have the same basic layout around a different body. Below is a way of extending the layout.html in the template file.https://medium.com/media/c76402cc2767f8863582e87b3dc9ef0e/hrefIt’s the job of the child templates(e.g home.html) to fill the block with its own contents. Lies within the spaces are the things you want to display on your website.Looking at the layout.html file above we didn’t really write that much code ourselves. We have BootStrap! A potent front-end framework integrated with Javascript that helps in creating beautiful and responsive web designs without much CSS code. About half of the codes were gotten from the Bootstrap starter template. Check it out to build yours.Static Folder: we must have this in our project directory. The static folder must contain files like CSS JS images just to add styles to the HTML layout you constructed.Alright! So if you are familiar with HTML &amp; CSS before start implementing them making the web designs to your taste. If not I’ve got some tutorials for you which you can learn from the concepts are quite easy to grasp.FreeCodeCamp HTML &amp;CSS TutorialBuild From Scratch on FreeCodeCampFlask Tutorial by Corey SchaferReady To Deploy?After implementing that portfolio website you’ve always wanted now is the time to make it live and accessible online.In deploying your portfolio website if you’d prefer the paid web-hosting providers you can try out Hostgator or Digital Ocean. I’ve particularly been using Heroku for my deployments. I wrote a step by step article on how to deploy your portfolio website on Heroku. Do check it out.Get Your Python Flask App Deployed In a SwiftFollowing the steps to deploy after adding the Procfile the Requirements.txt your project directory’s tree should look like this.You did a well-done job to have made it to this point. The codes that built mine are readily available on my GitHub page if you need clarification on something. Check the website too for inspiration. This can be pretty tiring while building but it definitely worth the time. This is one of the thousand ways to leveraging your career as a Data Scientist.You have more ideas to share questions and critiques. The Contact Me page of my portfolio website has links from which you can reach me. Let’s start from there. I’m just a text away.I hope this article is helpful and enlightening enough for you to get started. Happy Building!First Steps Into Building Your Data Science Portfolio Website was originally published in Towards Data Science on Medium where people are continuing the conversation by highlighting and responding to this story.
2021,6,25,Understanding Depthwise Separable Convolutions and the efficiency of MobileNets,https://towardsdatascience.com/understanding-depthwise-separable-convolutions-and-the-efficiency-of-mobilenets-6de3d6b62503?source=rss----7f60cf5620c9---4,Explanation of MobileNets and Depthwise Separable ConvolutionsContinue reading on Towards Data Science »
2021,6,25,"Let’s build a simple distributed computing system, for modern cloud",https://towardsdatascience.com/lets-build-a-simple-distributed-computing-system-for-modern-cloud-part-one-e2b745126211?source=rss----7f60cf5620c9---4,Design develop and test a fresh distributing computing framework from scratch while focusing being Simple Modern and Extensible“The joy of building something new” — https://unsplash.com/photos/nGwyaWKFRVI?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditShareLinkThe way of writing software is much different now. Being cloud native is one of the key goals for almost every software system architecture out there.In this article we will explore how can we build a simple and extensible distributed computing framework tailored for the modern cloud. We are going to design build and test a fresh framework from scratch while discussing the key design decisions.This article provides a good guide to understand how distributed systems work and implementing extensible systems targetting modern cloud infrastructures.Ok here is our distributed computing system.High level architectureAt a high level there are three major components in this system.Client — Actual users or user agents submitting jobsMaster — Accepts jobs from clients orchestrate the system and get the jobs executed and finally return expected outcomes to clientsWorker — Accepts a job execute it and provide the expected outcomes.Altogether clients submit jobs to the master master schedules and get them executed with the available workers and finally return job outcome back to the client.Three crucial factors before dive into the system designWe are developing this system for the modern cloud and in a community first approach. Therefore throughout the design we will keep following three things in our mind.Simple — Simple systems are easy to implement and easy to diagnoseModern — Systems run on the latest infrastructures utilizing the latest technologies are usually much faster efficient and obviously up-to date.Extensible — Extensible systems are vital for community as requirements differ from one use-case to other thus allow developers to avoid implementing the common components re-use and achieve use-cases with minimum implementation.System architectureThis is the system architecture of the distributed computing framework.Our system architecture for the distributed computing frameworkThe above image is pretty self-explanatory. To explain some of the key elements of itWorker microserviceA worker has a self-isolated workspace which allows it to be containarized and act independantly. Also the system expects that all workers are homogenious in terms of the resource capacities. i.e. Fixed CPU power RAM Storage etc. In other words workers do not scale.This makes workers simple thus easier to manage. Workers are actually gRPC servers in-terms of the default implementation. But it is also extensible thus can be implemented with any framework or even from scratch.The default gRPC implementation ensure that master-worker communication happens via RPC(remote procedure calls). This takes away all the complexity when compared to the other popular method RESTful APIs. Besides gRPC is also scalable code-friendly and fast(about 7 times faster that the RESTful APIs).Master microserviceMaster talks with clients and negtiates with workers. It acts as a gRPC client to the workers and a server serving RESTful APIs to the clients. RESTful APIs are built on top of Spring Boot framework + Open API 3 code generator in an API first development approach.RESTful APIs are much human friendly thus suits better for Master and Client communications. Also these API calls do not require to be blazing fast as they are not frequent.Master is also expected to be operating on its own self-isolated environment thus making it ideal for containerisation.The master consists of several loosely coupled components. Each of these components have their own responsibilities and are independant in-terms of the work they do. This makes the Master itself much simpler to understand. Also each of the components are extensible which makes the behaviour of the master can be modified and extended at a more fine-grained level.Internal components of the system and data flow scenarios. This framework is named as ORION.Task Manager — Orchestrate data flow among the component. It includes a periodically running task which checks and copies completed jobs from worker services and dispatch scheduled jobs afterwards.Task Scheduler — Persist jobs in a priority queue until they get dispatched to workers.Task Prioratizer — Assign a priority value based on diffrent attributes of the job such as deadline etc. The default implementation implements the “Earliest deadline first” approach.Task Distributor — Communicates with the worker pool and dispatch a task to a free worker. If there are no free workers available then it puts the task back to scheduling.Worker Pool Manager — Manages the communication with worker services.Central Store — Store job data until they get completed and to be obtained by clients. The default implementation uses local storage based store in which tasks are stored in folders created in a given isolated workspace.As mentioned earlier anyone can plug-in their own implementation of any of the above components thus making the system extensible by design.ImplementationBefore we begin the complete implementation of the system that we discussed so far can be found in the link below. It is an open source project distributed under the Apache 2.0 licence.crunchycookie/orionThe system we just designed is named as ORION. This project has four main components.Master — Contains the Spring boot + Open API 3 + gRPC client code based implementation of the Master microservice. This is a pure JAVA module.Worker — Cotains the gRPC server based implementation of the Worker microservice. This also is a pure JAVA module.Monitor App — Contains the React + Material UI based implementation of the ORION monitoring SPA(single page application). This is a pure JavaScript module.Integration — Contains test scripts and test resources to configure the test environment. The scripts are Postman test collections and test resources includes the data requires to execute the collection.Configuring and running the test suitWe will be using the v0.1 release of the ORION to conduct our test scenarios.Download the v0.1 release artifacts and extract both zipped files.Create an empty folder for the testing workspace. Let’s name it as theworkspace .Open the following file&lt;downloaded-artifacts&gt;/integration/environment/init-setup.shEdit the following variables with the correct file paths.WORKER_JAR_PATH=&lt;path to the orion-v0.1-worker.jar file&gt;MASTER_JAR_PATH=&lt;path to the orion-v0.1-master.jar file&gt;WORKSPACE=&lt;path to the workspace directory&gt;Save the changes.Open the following file.&lt;downloaded-artifacts&gt;/integration/environment/stop-setup.shEdit the following variable with the correct file path.WORKSPACE=&lt;path to the workspace directory&gt;Save the changes.Now we are ready to start the ORION system. Open a terminal window and execute the following to run the init-setup.sh script.cd &lt;downloaded-artifacts&gt;/integration/environmentsh init-setup.shThis copies relavant files to the workspace directory and starts three microservices for the master and two workers.If you observe the workspace directory following folders are created.active-processes — PIDs of the three services.logs — Log files from the three services.master — Artifacts related to the master service.worker-1 — Artifacts related to the first worker service.worker-2 — Artifacts related to the second worker service.Execute the following command to observe master logs.tail -f &lt;workspace&gt;/logs/master.logFollowing tailing log should be there if the service started successfully....2021-06-25 02:14:22.022  INFO 10125 --- [           main] o.c.orion.master.RESTfulEndpoint         : Started RESTfulEndpoint in 1.194 seconds (JVM running for 1.658)Likewise logs of the worker services can be tailed in the same manner using the correct file name in the same folder.Now start Postman and import the following test script collection.&lt;downloaded-artifacts&gt;/integration/test-scripts/ORION Test Suit - Executing heavy tasks.postman_collectionThere are four task submission API calls in this collection. You need to set the executable shell script and input text file for each of them(these files are submitted as multi-part form data). Relevant artifacts can be found in the corresponding folder.&lt;downloaded-artifacts&gt;/integration/test-scripts/task-files/task-&lt;number&gt;Tasks we are using in this test are nothing more than a shell script and an input file. Following is one of those scripts....sleep 50cat in.txt &gt; out.txt 2&gt;&amp;1What it does it wait for some time and then create a new file named out.txt and copy everything from in.txt to the out.txt. This is to mimic a task that takes some time to process and involves input and output files.Postman test suite API collectionWe are now almost ready to execute the test suit. But before that lets start the monitoring app.Execute the following in a terminal(you need to have npm installed at this point).cd &lt;downloaded-artifacts&gt;/sdnpm inpm startThis should start the monitoring app.Initial look of the monitoring appOnce tasks are submitted this will show how they first inserted into the queue sent to workers for execution and finally stored in the central store upon completion.Let’s start the test. In the postman collection submit all four tasks. You should recieve the following response saying each task is executing in the ORION.Submission responseNow open the monitoring app window and observe the life cycle of the submitted jobs!Submit tasks and monitoring them via the monitor appFor each task progress and it’s unique id is shown.Finally stop the ORION system by executing the following.cd &lt;downloaded-artifacts&gt;/integration/environmentsh stop-setup.shTo summarize our work…In this article we gradually developed a distributed computing system tailored for the modern cloud. We started by designing the architecture focusing on being simple modern and extensible. Then we moved to test the implemented system and successfully ran a test suite and observed how submitted tasks are handled.Thank you for staying until the end of this article. Your ideas suggestions and questions regarding this work are highly appreciated.Let’s build a simple distributed computing system for modern cloud was originally published in Towards Data Science on Medium where people are continuing the conversation by highlighting and responding to this story.
2021,6,25,Text Preprocessing in NLP with Python codes,https://www.analyticsvidhya.com/blog/2021/06/text-preprocessing-in-nlp-with-python-codes/,"ArticleVideo Book This article was published as a part of the Data Science Blogathon Introduction Natural Language Processing (NLP) is a branch of Data Science ... 
The post Text Preprocessing in NLP with Python codes appeared first on Analytics Vidhya."
2021,6,25,Part 14: Step by Step Guide to Master NLP – Basics of Topic Modelling,https://www.analyticsvidhya.com/blog/2021/06/part-14-step-by-step-guide-to-master-nlp-basics-of-topic-modelling/,"ArticleVideo Book This article was published as a part of the Data Science Blogathon Introduction This article is part of an ongoing blog series on ... 
The post Part 14: Step by Step Guide to Master NLP &#8211; Basics of Topic Modelling appeared first on Analytics Vidhya."
2021,6,25,Part 13: Step by Step Guide to Master NLP – Regular Expressions,https://www.analyticsvidhya.com/blog/2021/06/part-13-step-by-step-guide-to-master-nlp-regular-expressions/,"ArticleVideo Book This article was published as a part of the Data Science Blogathon Introduction This article is part of an ongoing blog series on ... 
The post Part 13: Step by Step Guide to Master NLP &#8211; Regular Expressions appeared first on Analytics Vidhya."
2021,6,25,Decide Best Learning Rate with LearningRateScheduler in Tensorflow,https://www.analyticsvidhya.com/blog/2021/06/decide-best-learning-rate-with-learningratescheduler-in-tensorflow/,"ArticleVideo Book This article was published as a part of the Data Science Blogathon Validation for your correct decisions feels so much better and relaxing ... 
The post Decide Best Learning Rate with LearningRateScheduler in Tensorflow appeared first on Analytics Vidhya."
2021,6,25,"Generate Reports Using Pandas Profiling, Deploy Using Streamlit",https://www.analyticsvidhya.com/blog/2021/06/generate-reports-using-pandas-profiling-deploy-using-streamlit/,"ArticleVideo Book This article was published as a part of the Data Science Blogathon Introduction Pandas library offers a wide range of functions. It helps ... 
The post Generate Reports Using Pandas Profiling Deploy Using Streamlit appeared first on Analytics Vidhya."
2021,6,25,Plotly and cufflinks : Advanced Python Data Visualization Libraries,https://www.analyticsvidhya.com/blog/2021/06/advanced-python-data-visualization-libraries-plotly/,"ArticleVideo Book This article was published as a part of the Data Science Blogathon Introduction Data Visualization helps to bridge the gap between numbers and ... 
The post Plotly and cufflinks : Advanced Python Data Visualization Libraries appeared first on Analytics Vidhya."
2021,6,25,Linear Regression using Neural Networks – A New Way,https://www.analyticsvidhya.com/blog/2021/06/linear-regression-using-neural-networks/,"ArticleVideo Book This article was published as a part of the Data Science Blogathon Source The Tech industry is filled with buzz words like &#8220;Neural ... 
The post Linear Regression using Neural Networks &#8211; A New Way appeared first on Analytics Vidhya."
2021,6,25,EDA: Exploratory Data Analysis With Python,https://www.analyticsvidhya.com/blog/2021/06/eda-exploratory-data-analysis-with-python/,"ArticleVideo Book This article was published as a part of the Data Science Blogathon Introduction Exploratory data analysis is the first and most important phase ... 
The post EDA: Exploratory Data Analysis With Python appeared first on Analytics Vidhya."
2021,6,25,Ridgeline Plots: Visualize Data with a Joy!,https://www.analyticsvidhya.com/blog/2021/06/ridgeline-plots-visualize-data-with-a-joy/,"ArticleVideo Book This article was published as a part of the Data Science Blogathon Introduction The DIstribution of data plays an important role in model ... 
The post Ridgeline Plots: Visualize Data with a Joy! appeared first on Analytics Vidhya."
2021,6,25,Bootstrap: the source of its power,https://www.analyticsvidhya.com/blog/2021/06/bootstrap-the-source-of-its-power/,"ArticleVideo Book This article was published as a part of the Data Science Blogathon From where we stand the rain seems random. If we could ... 
The post Bootstrap: the source of its power appeared first on Analytics Vidhya."
2021,6,25,"Data privacy isn&#8217;t a compliance checkbox, but a competitive advantage",https://www.ibm.com/blogs/journey-to-ai/2021/06/data-privacy-isnt-a-compliance-checkbox-but-a-competitive-advantage/,"In the post-GDPR era data privacy has taken center stage yet again due to digital transformation across the globe.  Governments everywhere are enforcing more robust data protection guidelines to address new digital interactions between enterprises and consumers as well as to increase accountability from enterprises in the use and protection of data. Accordingly there has [&#8230;]
The post Data privacy isn&#8217;t a compliance checkbox but a competitive advantage appeared first on Journey to AI Blog."
2021,6,24,Reducing neonate mortality rates with AI and Edge computing,https://www.ibm.com/blogs/journey-to-ai/2021/06/reducing-neonate-mortality-rates-with-ai-and-edge-computing/,"She sees the concern in their eyes. She hears &#8220;we&#8217;ll take good care of you&#8211;both.&#8221; Then she awakes to learn her child is in intensive care 3 lb. 7 oz. and eight weeks early. Days into his stay he is jaundiced—a complication of sepsis. Thus begins the adventure into the unknown.  –Reflections from a preemie&#8217;s [&#8230;]
The post Reducing neonate mortality rates with AI and Edge computing appeared first on Journey to AI Blog."
2021,6,23,Infuse intelligent automation at scale with IBM Cloud Pak for Data 4.0,https://www.ibm.com/blogs/journey-to-ai/2021/06/infuse-intelligent-automation-at-scale-with-ibm-cloud-pak-for-data-4-0/,"When’s the last time you considered if you’re operating in a truly predictive enterprise furthermore if it’s easy for your data consumers models and apps to access the right data? More often than not the answer is a resounding “not very”. Between the proliferation of data types and sources and tightening regulations data is often [&#8230;]
The post Infuse intelligent automation at scale with IBM Cloud Pak for Data 4.0 appeared first on Journey to AI Blog."
2021,6,17,Trustworthy AI helps Regions Bank better serve customers,https://www.ibm.com/blogs/journey-to-ai/2021/06/trustworthy-ai-helps-regions-bank-better-serve-customers/,"Financial institutions worldwide are feeling the scrutiny from both customers and regulators alike. Perceptions of an institution’s governance practices including its commitment to ethics fairness explainability and transparency of decisions are critical to its standing. No wonder those poised to gain a competitive advantage today want to ensure their AI is fair trustworthy and explainable. A member of the S&#38;P 500 Index Regions Financial Corporation is one of the United States’ largest full-service [&#8230;]
The post Trustworthy AI helps Regions Bank better serve customers appeared first on Journey to AI Blog."
2021,6,16,"Operationalize AI: You built an AI model, now what?",https://www.ibm.com/blogs/journey-to-ai/2021/06/operationalize-ai-you-built-an-ai-model-now-what/,"Global AI Adoption Index 2021 reports the top drivers of AI adoption in organizations are: 1. Advances in AI that make it more accessible (46%); 2. Business needs (46%); and 3. Changing business needs due to COVID-19 (44%). To bring AI models into production businesses are also mitigating the following AI modeling and management issues: [&#8230;]
The post Operationalize AI: You built an AI model now what? appeared first on Journey to AI Blog."
2021,6,14,How bakery company Vaasan used AI to upgrade their planning,https://www.ibm.com/blogs/journey-to-ai/2021/06/how-bakery-company-vaasan-used-ai-to-upgrade-their-planning/,"The Finnish baker Vaasan knows a thing or two about fast delivery. After all the company’s roots date back to year 1849 which makes Vaasan one of the oldest nationwide bakeries in Finland. Vaasan is best known as the producer of Finland’s most popular bread Vaasan Ruispalat. The company has to be fast because the [&#8230;]
The post How bakery company Vaasan used AI to upgrade their planning appeared first on Journey to AI Blog."
2021,6,4,How do you drive exponential growth in the healthcare industry?,https://www.ibm.com/blogs/journey-to-ai/2021/06/how-do-you-drive-exponential-growth-in-the-healthcare-industry/,"The healthcare industry is adapting to changes resulting from the coronavirus pandemic but many complex challenges prevail. How do we anticipate and prevent hospitalization of high-risk patients? How can we reduce the length of stay without compromising quality of care? How do we improve patient experience? How do we obtain the insights needed to drive [&#8230;]
The post How do you drive exponential growth in the healthcare industry? appeared first on Journey to AI Blog."
2021,6,3,IBM Planning Analytics delivers continuous integration with Watson,https://www.ibm.com/blogs/journey-to-ai/2021/06/ibm-planning-analytics-delivers-continuous-integration-with-watson/,"IBM’s Global C-suite study recently validated that data-driven organizations are 178% more likely to outperform their peers in terms of revenue and profitability. It’s no surprise that more and more companies are moving beyond basic Financial Planning and Analysis (FP&#38;A) and toward adopting a mindset of Gartner&#8217;s newly-dubbed &#8220;Extended Planning &#38; Analysis&#8221; (xP&#38;A)—or what we [&#8230;]
The post IBM Planning Analytics delivers continuous integration with Watson appeared first on Journey to AI Blog."
2021,5,28,You could be paying less for software licensing,https://www.ibm.com/blogs/journey-to-ai/2021/05/you-could-be-paying-less-for-software-licensing/,"High licensing and maintenance fees Gartner Inc. defines the TCO for enterprise software as the total cost an organization incurs to use and maintain software technology over time. To calculate TCO companies consider direct costs such as hardware software and administration and indirect costs including human resources project management and downtime. But there’s one key [&#8230;]
The post You could be paying less for software licensing appeared first on Journey to AI Blog."
2021,5,27,Making Data Simple: What does Legacy Powers Legendary mean?,https://www.ibm.com/blogs/journey-to-ai/2021/05/making-data-simple-what-does-legacy-powers-legendary-mean/,The post Making Data Simple: What does Legacy Powers Legendary mean? appeared first on Journey to AI Blog.
2021,6,25,Applied Language Technology: A No-Nonsense Approach,https://www.kdnuggets.com/2021/06/applied-language-technology.html,Here is a free entry-level applied natural language processing course that can fit into any beginner's roadmap to understanding NLP. Check it out.
2021,6,25,"High-Performance Deep Learning: How to train smaller, faster, and better models – Part 2",https://www.kdnuggets.com/2021/06/high-performance-deep-learning-part2.html,As your organization begins to consider building advanced deep learning models with efficiency in mind to improve the power delivered through your solutions the software and hardware tools required for these implementations are foundational to achieving high-performance.
2021,6,25,How to create an interactive 3D chart and share it easily with anyone,https://www.kdnuggets.com/2021/06/create-interactive-3d-chart-share.html,This is a short tutorial on a great Plotly feature.
2021,6,24,Season 1 Of Data Science Perspectives Webcast Released,https://www.kdnuggets.com/2021/06/bill-frinks-season-1-data-science-perspectives-webcast.html,Season 1 of Data Science Perspectives is now live and ready for viewing where I interview many of the executives and professionals I’ve met to enable viewers to learn about how their careers unfolded what skills they look for when hiring what trends they think are coming next and more.
2021,6,24,What will the demand for Data Scientists be in 10 years? Will Data Scientists be extinct?,https://www.kdnuggets.com/2021/06/poll-demand-data-scientists-10-years.html,Participate in the latest KDnuggets survey and share your opinion: what does the next decade have in  store for data scientist demand?
2021,6,24,In-Warehouse Machine Learning and the Modern Data Science Stack,https://www.kdnuggets.com/2021/06/in-warehouse-machine-learning-modern-data-science-stack.html,As your organization matures its data science portfolio and capabilities establishing a modern data stack is vital to enabling such growth. Here we overview various in-data warehouse machine learning services and discuss each of their benefits and requirements.
2021,6,24,10 Python Code Snippets We Should All Know,https://www.kdnuggets.com/2021/06/10-python-code-snippets.html,Check out these Python code snippets and start using them to solve everyday problems.
2021,6,23,Workflow Orchestration with Prefect and Coiled,https://www.kdnuggets.com/2021/06/coiled-workflow-orchestration-prefect.html,Coiled helps data scientists use Python for ambitious problems scaling to the cloud for computing power ease and speed—all tuned for the needs of teams and enterprises.  In this demo example see how to spin up a Coiled cluster to execute Prefect jobs during runtime.
2021,6,23,Create and Deploy Dashboards using Voila and Saturn Cloud,https://www.kdnuggets.com/2021/06/create-deploy-dashboards-voila-saturn-cloud.html,Working with and training large datasets maintaining them all in one place and deploying them to production is a challenging job. In this article we covered what Saturn Cloud is and how it can speed up your end-to-end pipeline how to create dashboards using Voila and Python and publish them to production in just a few easy steps.
2021,6,23,Data Careers in Demand: Crowd Solutions Architect Explained,https://www.kdnuggets.com/2021/06/data-careers-crowd-solutions-architect.html,How can crowdsourcing support the applications of data teams at an organization? With an ever-increasing demand for more and higher quality data a new role of the Crowd Solutions Architect (CSA) can leverage the potential of the masses to bring an advantage to a business's capability to deliver effective AI-driven solutions.
2021,6,23,Fine-Tuning Transformer Model for Invoice Recognition,https://www.kdnuggets.com/2021/06/fine-tuning-transformer-model-invoice-recognition.html,The author presents a step-by-step guide from annotation to training.
2021,6,23,"KDnuggets™ News 21:n23, Jun 23: Pandas vs SQL: When Data Scientists Should Use Each Tool; How to Land a Data Analytics Job in 6 Months",https://www.kdnuggets.com/2021/n23.html,Pandas vs SQL: When Data Scientists Should Use Each Tool; How to Land a Data Analytics Job in 6 Months; A Graph-based Text Similarity Method with Named Entity Information in NLP; The Best Way to Learn Practical NLP?; An introduction to Explainable AI (XAI) and Explainable Boosting Machines (EBM)
2021,6,22,The Word “WORD” Has 13 Meanings,https://www.kdnuggets.com/2021/06/expert-word-has-13-meanings.html,Thoughts around Knowledge Graphs the semantic nature of language and the two main types of word ambiguity.
2021,6,22,Amazing Low-Code Machine Learning Capabilities with New Ludwig Update,https://www.kdnuggets.com/2021/06/ludwig-update-includes-low-code-machine-learning-capabilities.html,Integration with Ray MLflow and TabNet are among the top features of this release.
2021,6,22,Analytics Engineering Everywhere,https://www.kdnuggets.com/2021/06/analytics-engineering-everywhere.html,Many new roles have appeared in the data world ever since the rise of the Data Scientist took the spotlight several years ago. Now there is a new core player ready to take center stage and we may see in five years nearly every organization will have an Analytics Engineering team.
2021,6,22,What is Segmentation?,https://www.kdnuggets.com/2021/06/what-segmentation.html,Segmentation refers to many things and is one of the most frequently used words in marketing This article looks at segmentation from a somewhat different-than-usual perspective.
2021,6,21,"Top Stories, Jun 14-20: Data Scientists Will be Extinct in 10 Years",https://www.kdnuggets.com/2021/06/top-news-week-0614-0620.html,Also: Get Interactive Plots Directly With Pandas; How to Generate Automated PDF Documents with Python; Top 10 Data Science Projects for Beginners; Five types of thinking for a high performing data scientist
2021,6,21,Using External Data to Accelerate Business in a Post-Vaccinated World,https://www.kdnuggets.com/2021/06/roidna-external-data-accelerate-business-webinar.html,Join this webinar Jun 24 2021 to learn how companies are developing insights to better prepare for growth opportunities improve business performance and mitigate risk in a post-pandemic economy.
2021,6,21,Overview of AutoNLP from Hugging Face with Example Project,https://www.kdnuggets.com/2021/06/overview-autonlp-hugging-face-example-project.html,AutoNLP is a beta project from Hugging Face that builds on the company’s work with its Transformer project. With AutoNLP you can get a working model with just a few simple terminal commands.
2021,6,21,Pandas vs SQL: When Data Scientists Should Use Each Tool,https://www.kdnuggets.com/2021/06/pandas-vs-sql.html,Exploring data sets and understanding its structure content and relationships is a routine and core process for any Data Scientist. Multiple tools exist for performing such analysis and we take a deep dive into the benefits and different approaches of two important tools SQL and Pandas.
2021,6,21,How to troubleshoot memory problems in Python,https://www.kdnuggets.com/2021/06/troubleshoot-memory-problems-python.html,Memory problems are hard to diagnose and fix in Python. This post goes through a step-by-step process for how to pinpoint and fix memory leaks using popular open source python packages.
2021,6,18,"Major changes: Where Analytics, Data Science, Machine Learning were applied in 2020/21",https://www.kdnuggets.com/2021/06/poll-where-analytics-data-science-ml-applied.html,Our latest poll shows major change in where AI Data Science Machine Learning are being applied with decline in interest in traditional fields like CRM/Consumer Analytics and growth in applications to Computer Vision COVID Agriculture and Education.
2021,6,18,"High Performance Deep Learning, Part 1",https://www.kdnuggets.com/2021/06/efficiency-deep-learning-part1.html,Advancing deep learning techniques continue to demonstrate incredible potential to deliver exciting new AI-enhanced software and systems. But training the most powerful models is expensive--financially computationally and environmentally. Increasing the efficiency of such models will have profound impacts in many ways so developing future models with this intension in mind will only help to further expand the reach applicability and value of what deep learning has to offer.
2021,6,18,"Data Science is Not Becoming Extinct in 10 Years, Your Skills Might",https://www.kdnuggets.com/2021/06/data-science-not-becoming-extinct-10-years.html,4 reasons why data science is here to stay and what you need to do to ensure that your skillset stays in demand.
2021,6,17,"Submit Your Algorithm for a Chance to Win Prizes Totaling $700,000+",https://www.kdnuggets.com/2021/06/nij-recidivism-forecasting-challenge.html,Can your algorithm make fair and accurate #recidivism forecasts? Take part in US National Institute of Justice “Recidivism Forecasting Challenge” with prize money totaling over $700K.
2021,6,17,How to Land a Data Analytics Job in 6 Months,https://www.kdnuggets.com/2021/06/land-data-analytics-job-6-months.html,Go from zero to hero in under six months.
2021,6,17,"Data storytelling: brains are built for visuals, but hearts turn on stories",https://www.kdnuggets.com/2021/06/data-storytelling.html,Today we need much more than just numbers about our organization to understand gain insights and take relevant actions. While visualizations of the data are important making an emotional connection with the stories behind the data is key. If you want to sell a story send a missile to the heart.
2021,6,17,Dashboards for Interpreting & Comparing Machine Learning Models,https://www.kdnuggets.com/2021/06/dashboards-interpreting-comparing-machine-learning-models.html,This article discusses using Interpret to create dashboards for machine learning models.
2021,6,16,How a Polytechnic Helps You Make the Tech-Business Connection,https://www.kdnuggets.com/2021/06/wpi-polytechnic-make-tech-business-connection.html,WPI welcomes professionals of all levels to its 100% online MS in Business Analytics — no GRE or GMAT required. Get started here.
2021,6,16,The Best Way to Learn Practical NLP?,https://www.kdnuggets.com/2021/06/best-way-learn-practical-nlp.html,Hugging Face has just released a course on using its libraries and ecosystem for practical NLP and it appears to be very comprehensive. Have a look for yourself.
2021,6,16,An introduction to Explainable AI (XAI) and Explainable Boosting Machines (EBM),https://www.kdnuggets.com/2021/06/explainable-ai-xai-explainable-boosting-machines-ebm.html,Understanding why your AI-based models make the decisions they do is crucial for deploying practical solutions in the real-world. Here we review some techniques in the field of Explainable AI (XAI) why explainability is important example models of explainable AI using LIME and SHAP and demonstrate how Explainable Boosting Machines (EBMs) can make explainability even easier.
2021,6,16,A Graph-based Text Similarity Method with Named Entity Information in NLP,https://www.kdnuggets.com/2021/06/graph-based-text-similarity-method-named-entity-information-nlp.html,"In this article the author summarizes the 2017 paper ""A Graph-based Text Similarity Measure That Employs Named Entity Information"" as per their understanding. Better understand the concepts by reading along."
2021,6,16,"KDnuggets™ News 21:n22, Jun 16: Data Scientists Extinct in 10 Years? Generate Automated PDF Documents with Python",https://www.kdnuggets.com/2021/n22.html,Data Scientists be extinct in 10 years? How to generate PDF Documents with Python; Top 10 Data Science Projects for Beginners; Five types of thinking for a high performing data scientist; and how to get interactive plots directly with Pandas.
2021,6,15,KDnuggets Top Blogs Rewards for May 2021,https://www.kdnuggets.com/2021/06/top-blogs-rewards-may.html,We announce the winners of the first KDnuggets Top Blog Rewards Program.
2021,6,15,The Data Matters: Choosing the right data to analyze can make or break your analysis,https://www.kdnuggets.com/2021/06/nomad-data-matters.html,We started Nomad Data to help data scientists and business analysts quickly find the right commercial datasets to match their specific use case. We catalog use cases of data and use machine learning and AI to match analysis goals with datasets.
2021,6,15,7 Data Security Best Practices for 2021,https://www.kdnuggets.com/2021/06/7-data-security-best-practices-2021.html,Here are seven data security best practices to adopt this year.
2021,6,15,Beginners Guide to Debugging TensorFlow Models,https://www.kdnuggets.com/2021/06/beginners-guide-debugging-tensorflow-models.html,If you are new to working with a deep learning framework such as TensorFlow there are a variety of typical errors beginners face when building and training models. Here we explore and solve some of the most common errors to help you develop a better intuition for debugging in TensorFlow.
2021,6,15,Facebook Launches One of the Toughest Reinforcement Learning Challenges in History,https://www.kdnuggets.com/2021/06/facebook-launches-toughest-reinforcement-learning-challenges.html,The FAIR team just launched the NetHack Challenge as part of the upcoming NeurIPS 2021 competition. The objective is to test new RL ideas using a one of the toughest game environments in the world.
2021,6,14,"Top Stories, Jun 7-13: 5 Tasks To Automate With Python; Five types of thinking for a high performing data scientist",https://www.kdnuggets.com/2021/06/top-news-week-0607-0613.html,Also: How to Generate Automated PDF Documents with Python; Five types of thinking for a high performing data scientist; How I Doubled My Income with Data Science and Machine Learning; Top 10 Data Science Projects for Beginners
2021,6,14,Data Scientists Will be Extinct in 10 Years,https://www.kdnuggets.com/2021/06/data-scientists-extinct-10-years.html,And why it’s not a bad thing.
2021,6,14,Get Interactive Plots Directly With Pandas,https://www.kdnuggets.com/2021/06/interactive-plots-directly-pandas.html,Telling a story with data is a core function for any Data Scientist and creating data visualizations that are simultaneously illuminating and appealing can be challenging. This tutorial reviews how to create Plotly and Bokeh plots directly through Pandas plotting syntax which will help you convert static visualizations into interactive counterparts -- and take your analysis to the next level.
2021,6,14,Building a Knowledge Graph for Job Search Using BERT,https://www.kdnuggets.com/2021/06/knowledge-graph-job-search-bert.html,A guide on how to create knowledge graphs using NER and Relation Extraction.
2021,6,11,Top 10 Data Science Projects for Beginners,https://www.kdnuggets.com/2021/06/top-10-data-science-projects-beginners.html,Check out these projects for ideas to strengthen your skills and build a portfolio that stands out.
2021,6,11,Five types of thinking for a high performing data scientist,https://www.kdnuggets.com/2021/06/five-types-thinking-data-scientist.html,The way you think about a problem and the conceptual process you go through to find a solution may be guided by your personal skills or the type of problem at hand. Many mental models exist representing a variety of thinking patterns -- and as a Data Scientist appreciating different approaches can help you more effectively model data in the business world and communicate your results to the decision-makers.
2021,6,11,9 Deadly Sins of Machine Learning Dataset Selection,https://www.kdnuggets.com/2021/06/9-deadly-sins-ml-dataset-selection.html,Avoid endless pain in model debugging by focusing on datasets upfront.
2021,6,10,"Top May Stories: A Guide On How To Become A Data Scientist; Data Scientist, Data Engineer & Other Data Careers, Explained",https://www.kdnuggets.com/2021/06/top-stories-2021-may.html,A Guide On How To Become A Data Scientist; Data Scientist Data Engineer &#038; Other Data Careers Explained; Vaex: Pandas but 1000x faster; Data Preparation in SQL with Cheat Sheet
2021,6,10,Numerics V: Integrality – When Being Close Enough is not Always Good Enough,https://www.kdnuggets.com/2021/06/fico-numerics-vs-integrality-close-enough.html,Wow already the fifth blog in this series…What is left to tell about numerics? There is another place where a MIP solver can sneak in minor violations that we have not yet discussed: The integrality conditions.
2021,6,10,"The Essential Guide to Transformers, the Key to Modern SOTA AI",https://www.kdnuggets.com/2021/06/essential-guide-transformers-key-modern-sota-ai.html,You likely know Transformers from their recent spate of success stories in natural language processing computer vision and other areas of artificial intelligence but are familiar with all of the X-formers? More importantly do you know the differences and why you might use one over another?
2021,6,10,Feature Selection – All You Ever Wanted To Know,https://www.kdnuggets.com/2021/06/feature-selection-overview.html,"Although your data set may contain a lot of information about many different features selecting only the ""best"" of these to be considered by a machine learning model can mean the difference between a model that performs well--with better performance higher accuracy and more computational efficiency--and one that falls flat. The process of feature selection guides you toward working with only the data that may be the most meaningful and to accomplish this a variety of feature selection types methodologies and techniques exist for you to explore."
2021,6,10,How to Generate Automated PDF Documents with Python,https://www.kdnuggets.com/2021/06/generate-automated-pdf-documents-python.html,Discover how to leverage automation to create dazzling PDF documents effortlessly.
2021,6,9,How to speed up a Deep Learning Language model by almost 50X at half the cost,https://www.kdnuggets.com/2021/06/determined-ai-speed-up-deep-learning-language-model.html,In this blog post we show how to accelerate fine-tuning the ALBERT language model while also reducing costs by using Determined’s built-in support for distributed training with AWS spot instances.
2021,6,9,"Data Scientists, You Need to Know How to Code",https://www.kdnuggets.com/2021/06/data-scientists-need-know-code.html,You need to know how to code &#8212; and not just code but write good code.
2021,6,9,The 7 Best Open Source AI Libraries You May Not Have Heard Of,https://www.kdnuggets.com/2021/06/7-open-source-ai-libraries.html,AI researchers today have many exciting options for working with specialized tools. Although starting original projects from scratch is often not necessary knowing which existing library to leverage remains a challenge. This list of generally unknown yet awesome open-source libraries offers an interesting collection to consider for state-of-the-art research that spans from automatic machine learning to differentiable quantum circuits.
2021,6,9,How a Single Mistake Wasted 3 Years of My Data Science Journey,https://www.kdnuggets.com/2021/06/single-mistake-wasted-3-years-data-science.html,Self-paced courses are just sleeping pills; Industry experts are the right choice.
2021,6,9,"KDnuggets™ News 21:n21, Jun 9: 5 Tasks To Automate With Python; How I Doubled My Income with Data Science and Machine Learning",https://www.kdnuggets.com/2021/n21.html,5 Tasks To Automate With Python; How I Doubled My Income with Data Science and Machine Learning; Will There Be a Shortage of Data Science Jobs in the Next 5 Years?; How to Make Python Code Run Incredibly Fast; Stop (and Start) Hiring Data Scientists
2021,6,8,SAS® Visual Data Science Decisioning powered by SAS® Viya®:  Free Trial,https://www.kdnuggets.com/2021/06/sas-viya-visual-data-science-free-trial.html,SAS® Visual Data Science Decisioning provides the ultimate analytics experience. Start your free trial and get access to the latest in data visualization machine learning forecasting model deployment and more.
2021,6,8,This Data Visualization is the First Step for Effective Feature Selection,https://www.kdnuggets.com/2021/06/data-visualization-feature-selection.html,Understanding the most important features to use is crucial for developing a model that performs well. Knowing which features to consider requires experimentation and proper visualization of your data can help clarify your initial selections. The scatter pairplot is a great place to start.
2021,6,8,The only Jupyter Notebooks extension you truly need,https://www.kdnuggets.com/2021/06/only-jupyter-notebooks-extension-truly-need.html,Now you don’t need to restart the kernel after editing the code in your custom imports.
2021,6,8,5 Tips for Picking an Edge AI Platform,https://www.kdnuggets.com/2021/06/5-tips-edge-ai-platform.html,Edge Analytics isn’t just coding and tools.  The different environment outside the datacenter or cloud means a purpose built platform is the best way to deliver consistent results.  We discuss 5 different considerations for an edge platform to support your training and deployment.
2021,6,7,5 Data Science Open-source Projects You Should Consider Contributing to,https://www.kdnuggets.com/2021/06/5-data-science-open-source-projects-contribute.html,As you prepare to interview for a position in data science or are looking to jump to the next level now is the time to enhance your skills and your resume with by working on rea open-source projects. Here we suggest a great selection of projects you can contribute to and help build something awesome so all you need to do choose one and tackle it head on.
2021,6,15,Communal Computing,https://www.oreilly.com/radar/communal-computing/,Home assistants and smart displays are being sold in record numbers but they are built wrong. They are designed with one person in mind: the owner. These technologies need to fit into the communal spaces where they are placed like homes and offices. If they don’t fit they will be unplugged and put away due [&#8230;]
2021,5,25,AI Powered Misinformation and Manipulation at Scale #GPT-3,https://www.oreilly.com/radar/ai-powered-misinformation-and-manipulation-at-scale-gpt-3/,OpenAI’s text generating system GPT-3 has captured mainstream attention. GPT-3 is essentially an auto-complete bot whose underlying Machine Learning (ML) model has been trained on vast quantities of text available on the Internet. The output produced from this autocomplete bot can be used to manipulate people on social media and spew political propaganda argue about [&#8230;]
2021,4,19,AI Adoption in the Enterprise 2021,https://www.oreilly.com/radar/ai-adoption-in-the-enterprise-2021/,During the first weeks of February we asked recipients of our Data and AI Newsletters to participate in a survey on AI adoption in the enterprise. We were interested in answering two questions. First we wanted to understand how the use of AI grew in the past year. We were also interested in the practice [&#8230;]
2021,3,23,"InfoTribes, Reality Brokers",https://www.oreilly.com/radar/infotribes-reality-brokers/,It seems harder than ever to agree with others on basic facts let alone to develop shared values and goals: we even claim to live in a post-truth era1. With anti-vaxxers QAnon Bernie Bros flat earthers the intellectual dark web and disagreement worldwide as to the seriousness of COVID-19 and the effectiveness of masks have [&#8230;]
2021,3,9,The Next Generation of AI,https://www.oreilly.com/radar/the-next-generation-of-ai/,Programs like AlphaZero and GPT-3 are massive accomplishments: they represent years of sustained work solving a difficult problem. But these problems are squarely within the domain of traditional AI. Playing Chess and Go or building ever-better language models have been AI projects for decades. The following projects have a different flavor: In February PLOS Genetics [&#8230;]
2021,2,26,Product Management for AI,https://www.oreilly.com/radar/product-management-for-ai/,A couple of years ago Pete Skomoroch Roger Magoulas and I talked about the problems of being a product manager for an AI product. We decided that would be a good topic for an article and possibly more. After Pete and I wrote the first article for O’Reilly Radar it was clear that there was [&#8230;]
2021,2,19,5 things on our data and AI radar for 2021,https://www.oreilly.com/radar/5-things-on-our-data-and-ai-radar-for-2021/,Here are some of the most significant themes we see as we look toward 2021. Some of these are emerging topics and others are developments on existing concepts but all of them will inform our thinking in the coming year. MLOps FTW MLOps attempts to bridge the gap between Machine Learning (ML) applications and the [&#8230;]
2021,1,25,"Where Programming, Ops, AI, and the Cloud are Headed in 2021",https://www.oreilly.com/radar/where-programming-ops-ai-and-the-cloud-are-headed-in-2021/,In this report we look at the data generated by the O’Reilly online learning platform to discern trends in the technology industry—trends technology leaders need to follow. But what are “trends”? All too often trends degenerate into horse races over languages and platforms. Look at all the angst heating up social media when TIOBE or [&#8230;]
2020,10,22,Our Favorite Questions,https://www.oreilly.com/radar/our-favorite-questions/,&#8220;On peut interroger n&#8217;importe qui dans n&#8217;importe quel état; ce sont rarement les réponses qui apportent la vérité mais l&#8217;enchaînement des questions.&#8220; &#8220;You can interrogate anyone no matter what their state of being.&#160; It&#8217;s rarely their answers that unveil the truth but the sequence of questions that you have to ask.&#8220;–&#160; Inspector Pastor in La [&#8230;]
2020,10,13,AI Product Management After Deployment,https://www.oreilly.com/radar/ai-product-management-after-deployment/,The field of AI product management continues to gain momentum. As the AI product management role advances in maturity more and more information and advice has become available. Our previous articles in this series introduce our own take on AI product management discuss the skills that AI product managers need and detail how to bring [&#8230;]
2020,9,15,How to Set AI Goals,https://www.oreilly.com/radar/how-to-set-ai-goals/,AI Benefits and Stakeholders AI is a field where value in the form of outcomes and their resulting benefits is created by machines exhibiting the ability to learn and “understand” and to use the knowledge learned to carry out tasks or achieve goals. AI-generated benefits can be realized by defining and achieving appropriate goals. These [&#8230;]
2020,9,8,Pair Programming with AI,https://www.oreilly.com/radar/pair-programming-with-ai/,In a conversation with Kevlin Henney we started talking about the kinds of user interfaces that might work for AI-assisted programming. This is a significant problem: neither of us were aware of any significant work on user interfaces that support collaboration. However as software developers many of us have been practicing effective collaboration for years. [&#8230;]
2020,8,18,Why Best-of-Breed is a Better Choice than All-in-One Platforms for Data Science,https://www.oreilly.com/radar/why-best-of-breed-is-a-better-choice-than-all-in-one-platforms-for-data-science/,So you need to redesign your company’s data infrastructure. Do you buy a solution from a big integration company like IBM Cloudera or Amazon?&#160; Do you engage many small startups each focused on one part of the problem?&#160; A little of both?&#160; We see trends shifting towards focused best-of-breed platforms. That is products that are [&#8230;]
2020,7,28,Bringing an AI Product to Market,https://www.oreilly.com/radar/bringing-an-ai-product-to-market/,The Core Responsibilities of the AI Product Manager Product Managers are responsible for the successful development testing release and adoption of a product and for leading the team that implements those milestones. Product managers for AI must satisfy these same responsibilities tuned for the AI lifecycle. In the first two articles in this series we [&#8230;]
2020,7,28,"Power, Harms, and Data",https://www.oreilly.com/radar/power-harms-and-data/,A recent article in The Verge discussed PULSE an algorithm for “upsampling” digital images. PULSE when applied to a low-resolution image of Barack Obama recreated a White man&#8217;s face; applied to Alexandria Ocasio-Cortez it built a White woman&#8217;s face.  It had similar problems with other images of Black and Hispanic people frequently giving them White [&#8230;]
2020,7,21,"AI, Protests, and Justice",https://www.oreilly.com/radar/ai-protests-and-justice/,Largely on the impetus of the Black Lives Matter movement the public&#8217;s response to the murder of George Floyd and the subsequent demonstrations we&#8217;ve seen increased concern about the use of facial identification in policing. First in a highly publicized wave of announcements IBM Microsoft and Amazon have announced that they will not sell face [&#8230;]
2020,6,24,COVID-19 and Complex Systems,https://www.oreilly.com/radar/covid-19-and-complex-systems/,In various mailing lists about the COVID-19 pandemic I&#8217;ve seen several discussions of &#8220;complex systems theory&#8221; as possibly a way to understand how the pandemic is playing out in different locations. Specifically: why have Japan and Hong Kong not experienced an explosion in cases even though their governments responded poorly to the crisis? The argument [&#8230;]
2020,6,16,Decision-Making in a Time of Crisis,https://www.oreilly.com/radar/decision-making-in-a-time-of-crisis/,In the 1996 cult classic film Swingers two friends Trent and Mike (played by Vince Vaughan and Jon Favreau respectively) make an impromptu trip to Las Vegas. At the blackjack table Mike gets dealt an 11 and Trent tells him to double down. Mike responds “What?!” and Trent replies “Double down baby. You gotta double [&#8230;]
2020,6,9,Machine Learning and the Production Gap,https://www.oreilly.com/radar/machine-learning-and-the-production-gap/,The biggest problem facing machine learning today isn&#8217;t the need for better algorithms; it isn&#8217;t the need for more computing power to train models; it isn&#8217;t even the need for more skilled practitioners. It&#8217;s getting machine learning from the researcher&#8217;s laptop to production. That&#8217;s the real gap. It&#8217;s one thing to build a model; it&#8217;s [&#8230;]
2020,5,27,Reclaiming the stories that algorithms tell,https://www.oreilly.com/radar/reclaiming-the-stories-that-algorithms-tell/,Algorithms tell stories about who people are. The first story an algorithm told about me was that my life was in danger. It was 7:53 pm on a clear Monday evening in September of 1981 at the Columbia Hospital for Women in Washington DC. I was exactly one minute old. The medical team scored me—as [&#8230;]
2020,5,18,What to Do When AI Fails,https://www.oreilly.com/radar/what-to-do-when-ai-fails/,These are unprecedented times at least by information age standards. Much of the U.S. economy has ground to a halt and social norms about our data and our privacy have been thrown out the window throughout much of the world. Moreover things seem likely to keep changing until a vaccine or effective treatment for COVID-19 [&#8230;]
2020,5,14,Practical Skills for The AI Product Manager,https://www.oreilly.com/radar/practical-skills-for-the-ai-product-manager/,In our previous article What You Need to Know About Product Management for AI we discussed the need for an AI Product Manager.&#160; This role includes everything a traditional PM does but also requires an operational understanding of machine learning software development along with a realistic view of its capabilities and limitations. In this article [&#8230;]
2020,5,11,When models are everywhere,https://www.oreilly.com/radar/when-models-are-everywhere/,You probably interact with fifty to a hundred machine learning products every day from your social media feeds and YouTube recommendations to your email spam filter and the updates that the New York Times CNN or Fox News decide to push not to mention the hidden models that place ads on the websites you visit [&#8230;]
2020,4,22,How data privacy leader Apple found itself in a data ethics catastrophe,https://www.oreilly.com/radar/how-data-privacy-leader-apple-found-itself-in-a-data-ethics-catastrophe/,Three months ago Apple released a new credit card in partnership with Goldman Sachs that aimed to disrupt the highly regulated world of consumer finance. However a well-known software developer tweeted that he was given 20x the credit line offered to his wife despite the fact that they have been filing joint tax returns and [&#8230;]
2020,3,31,What you need to know about product management for AI,https://www.oreilly.com/radar/what-you-need-to-know-about-product-management-for-ai/,If you’re already a software product manager (PM) you have a head start on becoming a PM for artificial intelligence (AI) or machine learning (ML). You already know the game and how it is played: you’re the coordinator who ties everything together from the developers and designers to the executives. You’re responsible for the design [&#8230;]
2020,3,24,The unreasonable importance of data preparation,https://www.oreilly.com/radar/the-unreasonable-importance-of-data-preparation/,In a world focused on buzzword-driven models and algorithms you’d be forgiven for forgetting about the unreasonable importance of data preparation and quality: your models are only as good as the data you feed them. This is the garbage in garbage out principle: flawed data going in leads to flawed results algorithms and business decisions. [&#8230;]
2020,3,19,6 trends framing the state of AI and ML,https://www.oreilly.com/radar/6-trends-framing-the-state-of-ai-and-ml/,O’Reilly online learning is a trove of information about the trends topics and issues tech leaders need to know about to do their jobs. We use it as a data source for our annual platform analysis and we’re using it as the basis for this report where we take a close look at the most-used [&#8230;]
2020,3,18,AI adoption in the enterprise 2020,https://www.oreilly.com/radar/ai-adoption-in-the-enterprise-2020/,Last year when we felt interest in artificial intelligence (AI) was approaching a fever pitch we created a survey to ask about AI adoption. When we analyzed the results we determined the AI space was in a state of rapid change so we eagerly commissioned a follow-up survey to help find out where AI stands [&#8230;]
2020,2,18,5 key areas for tech leaders to watch in 2020,https://www.oreilly.com/radar/oreilly-2020-platform-analysis/,O’Reilly online learning contains information about the trends topics and issues tech leaders need to watch and explore. It’s also the data source for our annual usage study which examines the most-used topics and the top search terms.[1] This combination of usage and search affords a contextual view that encompasses not only the tools techniques [&#8230;]
2020,2,12,The state of data quality in 2020,https://www.oreilly.com/radar/the-state-of-data-quality-in-2020/,We suspected that data quality was a topic brimming with interest. Those suspicions were confirmed when we quickly received more than 1900 responses to our mid-November survey request. The responses show a surfeit of concerns around data quality and some uncertainty about how best to address those concerns. Key survey results: The C-suite is engaged [&#8230;]
2020,1,15,Reinforcement learning for the real world,https://www.oreilly.com/radar/reinforcement-learning-for-the-real-world/,Roger Magoulas recently sat down with Edward Jezierski reinforcement learning AI principal program manager at Microsoft to talk about reinforcement learning (RL). They discuss why RL&#8217;s role in AI is so important challenges of applying RL in a business environment and how to approach ethical and responsible use questions. Here are some highlights from their [&#8230;]
2020,1,7,8 AI trends we’re watching in 2020,https://www.oreilly.com/radar/8-ai-trends-were-watching-in-2020/,We see the AI space poised for an acceleration in adoption driven by more sophisticated AI models being put in production specialized hardware that increases AI’s capacity to provide quicker results based on larger datasets simplified tools that democratize access to the entire AI stack small tools that enables AI on nearly any device and [&#8230;]
2019,12,18,AI is computer science disguised as hard work,https://www.oreilly.com/radar/ai-is-computer-science-disguised-as-hard-work/,Roger Magoulas recently sat down with Rob Thomas and Tim O’Reilly to discuss Thomas’s AI framework called the AI Ladder which according to his recent paper is a framework that describes “the increasing levels of analytic sophistication that lead to and buttress a thriving AI environment.” Thomas notes both in his paper and in a [&#8230;]
2019,12,12,Why you should care about debugging machine learning models,https://www.oreilly.com/radar/why-you-should-care-about-debugging-machine-learning-models/,For all the excitement about machine learning (ML) there are serious impediments to its widespread adoption. Not least is the broadening realization that ML models can fail. And that’s why model debugging the art and science of understanding and fixing problems in ML models is so critical to the future of ML. Without being able [&#8230;]
2019,12,10,The road to Software 2.0,https://www.oreilly.com/radar/the-road-to-software-2-0/,Roughly a year ago we wrote “What machine learning means for software development.” In that article we talked about Andrej Karpathy’s concept of Software 2.0. Karpathy argues that we’re at the beginning of a profound change in the way software is developed. Up until now we’ve built systems by carefully and painstakingly telling systems exactly [&#8230;]
2019,11,26,Moving AI and ML from research into production,https://www.oreilly.com/radar/moving-ai-and-ml-from-the-research-realm-into-production/,In this interview from O&#8217;Reilly Foo Camp 2019 Dean Wampler head of evangelism at Anyscale.io talks about moving AI and machine learning into real-time production environments. Highlights from the interview include: Facilitating the transition from research to production in a robust way introduces a number of complications Wampler says including governance GDPR and traceability rules. [&#8230;]
2019,11,19,There’s a path to an AI ROI,https://www.oreilly.com/radar/theres-a-path-to-an-ai-roi/,In this interview from O&#8217;Reilly Foo Camp 2019 Hands-On Unsupervised Learning Using Python author Ankur Patel discusses the challenges and opportunities in making machine learning and AI accessible and financially viable for enterprise applications. Highlights from the interview include: The biggest hurdle businesses face when implementing machine learning or AI solutions is cleaning and preparing [&#8230;]
2019,11,14,“AI is a lie”,https://www.oreilly.com/radar/ai-is-a-lie/,In this interview from O&#8217;Reilly Foo Camp 2019 Eric Jonas assistant professor at the University of Chicago pierces the hype around artificial intelligence. Highlights from the interview include: Jonas argues that &#8220;AI is a lie&#8221;—meaning that our expectations far outsize the reality of what&#8217;s currently possible. One of the issues arising from that disconnect is [&#8230;]
2019,11,7,A world of deepfakes,https://www.oreilly.com/radar/a-world-of-deepfakes/,Deepfakes have been very much in the news for the past two years. It’s time to think about what deepfakes are and what they mean. Where do they come from? Why now? Is this just a natural evolution in the history of technology? Deepfakes are media that are created by AI. They appear to be [&#8230;]
2019,11,1,"Highlights from TensorFlow World in Santa Clara, California 2019",https://www.oreilly.com/radar/highlights-from-tensorflow-world-2019/,People from across the TensorFlow community came together in Santa Clara California for TensorFlow World. Below you&#8217;ll find links to highlights from the event. Opening keynote Jeff Dean explains why Google open-sourced TensorFlow and discusses its progress. Watch &#8220;Opening keynote&#8220; Accelerating ML at Twitter Theodore Summe offers a glimpse into how Twitter employs machine learning [&#8230;]
2019,11,1,Sticker recommendations and AI-driven innovations on the Hike messaging platform,https://www.oreilly.com/radar/sticker-recommendations-and-ai-driven-innovations-on-the-hike-messaging-platform/,This is a keynote from TensorFlow World in Santa Clara California 2019. See more keynotes from this event.
2019,11,1,“Human error”: How can we help people build models that do what they expect,https://www.oreilly.com/radar/human-error-how-can-we-help-people-build-models-that-do-what-they-expect/,This is a keynote from TensorFlow World in Santa Clara California 2019. See more keynotes from this event.
2019,11,1,Personalization of Spotify Home and TensorFlow,https://www.oreilly.com/radar/personalization-of-spotify-home-and-tensorflow/,This is a keynote from TensorFlow World in Santa Clara California 2019. See more keynotes from this event.
2019,11,1,TensorFlow.js: Bringing machine learning to JavaScript,https://www.oreilly.com/radar/tensorflow-js-bringing-machine-learning-to-javascript/,This is a keynote from TensorFlow World in Santa Clara California 2019. See more keynotes from this event.
2019,11,1,TFX: An end-to-end ML platform for everyone,https://www.oreilly.com/radar/tfx-an-end-to-end-ml-platform-for-everyone/,This is a keynote from TensorFlow World in Santa Clara California 2019. See more keynotes from this event.
2019,11,1,MLIR: Accelerating AI,https://www.oreilly.com/radar/mlir-accelerating-ai/,This is a keynote from TensorFlow World in Santa Clara California 2019. See more keynotes from this event.
2019,11,1,TensorFlow Hub: The platform to share and discover pretrained models for TensorFlow,https://www.oreilly.com/radar/tensorflow-hub-the-platform-to-share-and-discover-pretrained-models-for-tensorflow/,This is a keynote from TensorFlow World in Santa Clara California 2019. See more keynotes from this event.
2019,11,1,TensorFlow Lite: ML for mobile and IoT devices,https://www.oreilly.com/radar/tensorflow-lite-ml-for-mobile-and-iot-devices/,This is a keynote from TensorFlow World in Santa Clara California 2019. See more keynotes from this event.
2019,10,31,Accelerating ML at Twitter,https://www.oreilly.com/radar/accelerating-ml-at-twitter/,This is a keynote from TensorFlow World in Santa Clara California 2019. See more keynotes from this event.
2019,10,31,The latest from TensorFlow,https://www.oreilly.com/radar/the-latest-from-tensorflow/,This is a keynote from TensorFlow World in Santa Clara California 2019. See more keynotes from this event.
2019,10,31,TensorFlow World 2019 opening keynote,https://www.oreilly.com/radar/tensorflow-world-2019-opening-keynote/,This is a keynote from TensorFlow World in Santa Clara California 2019. See more keynotes from this event.
2019,10,31,TensorFlow community announcements,https://www.oreilly.com/radar/tensorflow-community-announcements/,This is a keynote from TensorFlow World in Santa Clara California 2019. See more keynotes from this event.
2019,10,17,Highlights from the O’Reilly Artificial Intelligence Conference in London 2019,https://www.oreilly.com/radar/highlights-from-ai-london-2019/,People from across the AI world came together in London for the Artificial Intelligence Conference. Below you&#8217;ll find links to highlights from the event. When flying is cheaper than standing still Raffaello D’Andrea presents his vision of how autonomous indoor drones will drive the next wave of robotics development. Watch &#8220;When flying is cheaper than [&#8230;]
2019,10,17,When to trust AI,https://www.oreilly.com/radar/when-to-trust-ai/,This is a keynote highlight from the O&#8217;Reilly Artificial Intelligence Conference in London 2019. Watch the full version of this keynote on the O&#8217;Reilly online learning platform. You can also see other highlights from the event.
2019,10,17,When flying is cheaper than standing still,https://www.oreilly.com/radar/when-flying-is-cheaper-than-standing-still/,This is a keynote highlight from the O&#8217;Reilly Artificial Intelligence Conference in London 2019. Watch the full version of this keynote on the O&#8217;Reilly online learning platform. You can also see other highlights from the event.
2019,10,17,"Machine learning challenges at LinkedIn: Spark, TensorFlow, and beyond",https://www.oreilly.com/radar/machine-learning-challenges-at-linkedin/,This is a keynote highlight from the O&#8217;Reilly Artificial Intelligence Conference in London 2019. Watch the full version of this keynote on the O&#8217;Reilly online learning platform. You can also see other highlights from the event.
2019,10,17,Accelerate with purpose,https://www.oreilly.com/radar/accelerate-with-purpose-ai-uk-19/,This is a keynote highlight from the O&#8217;Reilly Artificial Intelligence Conference in London 2019. Watch the full version of this keynote on the O&#8217;Reilly online learning platform. You can also see other highlights from the event.
2019,10,17,The quest for high-quality data,https://www.oreilly.com/radar/the-quest-for-high-quality-data-ai-uk-19/,This is a keynote highlight from the O&#8217;Reilly Artificial Intelligence Conference in London 2019. Watch the full version of this keynote on the O&#8217;Reilly online learning platform. You can also see other highlights from the event.
2019,10,16,Real-time AI for entity resolution,https://www.oreilly.com/radar/real-time-ai-for-entity-resolution/,This is a keynote highlight from the O&#8217;Reilly Artificial Intelligence Conference in London 2019. Watch the full version of this keynote on the O&#8217;Reilly online learning platform. You can also see other highlights from the event.
2019,10,16,The power of knowledge at scale,https://www.oreilly.com/radar/the-power-of-knowledge-at-scale/,This is a keynote highlight from the O&#8217;Reilly Artificial Intelligence Conference in London 2019. Watch the full version of this keynote on the O&#8217;Reilly online learning platform. You can also see other highlights from the event.
2021,6,25,Create and Preview RMarkdown Documents with QBit Workspace,https://www.r-bloggers.com/2021/06/create-and-preview-rmarkdown-documents-with-qbit-workspace/," Create and Preview RMarkdown Documents with QBit Workspace
RMarkdown is an excellent format to create documents which combine code outputs with text—a programming paradigm called Literate Programming first introduced by Donald Knuth. Although RMarkdow...


The post Create and Preview RMarkdown Documents with QBit Workspace first appeared on R-bloggers."
2021,6,25,Techguides Update: Shiny CI/CD in Actions!,https://www.r-bloggers.com/2021/06/techguides-update-shiny-ci-cd-in-actions/,"GitHub Actions has been maturing as a CI / CD tool therefore we updated our techguides to focus more on it.
There are many CI / CD tools available which people most often use together with GitHub. Since GitHub Actions is the first such tool that in...
The post Techguides Update: Shiny CI/CD in Actions! first appeared on R-bloggers."
2021,6,25,rJava with User-defined R Functions in Eclipse,https://www.r-bloggers.com/2021/06/rjava-with-user-defined-r-functions-in-eclipse/,"      This post shows how to call user-defined functions in R script from Eclipse Java with rJava package. This work will improve code readability and minimize the likelihood of errors in such a way that it reduces multiples lines of...


The post rJava with User-defined R Functions in Eclipse first appeared on R-bloggers."
2021,6,24,Updated forecasts for the UEFA Euro 2020 knockout stage,https://www.r-bloggers.com/2021/06/updated-forecasts-for-the-uefa-euro-2020-knockout-stage/,"
    After all group stage matches at the UEFA Euro 2020 we have updated the knockout stage forecasts by re-training our hybrid random forest model on the extended data. This shows that England profits most from the realized tournament draw.
...


The post Updated forecasts for the UEFA Euro 2020 knockout stage first appeared on R-bloggers."
2021,6,24,Working with Databases in R – Video Presentation from NairobiR and R-Ladies Nairobi,https://www.r-bloggers.com/2021/06/working-with-databases-in-r-video-presentation-from-nairobir-and-r-ladies-nairobi/," During this Working with Databases in R online presentation Christopher Maronga shares his years of practical experience in accessing and working with Databases in R. R Consortium assisted by providing...
The post Working with Databases in R – Video Presentation from NairobiR and R-Ladies Nairobi appeared first on R Consortium.


The post Working with Databases in R – Video Presentation from NairobiR and R-Ladies Nairobi first appeared on R-bloggers."
2021,6,24,Sentiment Analysis on Reddit using R,https://www.r-bloggers.com/2021/06/sentiment-analysis-on-reddit-using-r/,"According to Wikipedia Reddit is an American social news aggregation web content rating and discussion website. Registered members submit content to the site such as links text posts images and videos which are then voted up or down by other members. Posts are organized by subject into user-created boards called “...
The post Sentiment Analysis on Reddit using R first appeared on R-bloggers."
2021,6,24,"useR! 2021 Preview: Scaling Shiny, User Tests, and shiny.fluent",https://www.r-bloggers.com/2021/06/user-2021-preview-scaling-shiny-user-tests-and-shiny-fluent/,"
useR! – The Global Virtual R Conference Do you have a dashboard that’s not quite where you want it to be? Or are you curious about how to streamline your beautiful Shiny app projects? If you’re working with Shiny or want to enhance your data storytelling abilities with R ...


The post useR! 2021 Preview: Scaling Shiny User Tests and shiny.fluent first appeared on R-bloggers."
2021,6,24,Why Shiny? Opinions from a Shiny developer,https://www.r-bloggers.com/2021/06/why-shiny-opinions-from-a-shiny-developer/,"EARL 2021 will start with a week of afternoon workshops hosted by our expert Mango Solutions Data Scientists. This morning...
The post Why Shiny? Opinions from a Shiny developer appeared first on Mango Solutions.
The post Why Shiny? Opinions from a Shiny developer first appeared on R-bloggers."
2021,6,24,Who is going to Win the Euro 2020,https://www.r-bloggers.com/2021/06/who-is-going-to-win-the-euro-2020/," We have reached the knock-out phase of Euro 2020 (or 2021) where the final-16 teams and the games can be ... Read moreWho is going to Win the Euro 2020


The post Who is going to Win the Euro 2020 first appeared on R-bloggers."
2021,6,24,ggpairs in R- A Brief Introduction to ggpairs,https://www.r-bloggers.com/2021/06/ggpairs-in-r-a-brief-introduction-to-ggpairs/," In this article we are going to compare pairs and ggpairs functions in R. 1. pairs() in R pairs() function mainly used to plot...
The post ggpairs in R- A Brief Introduction to ggpairs appeared first on finnstats.


The post ggpairs in R- A Brief Introduction to ggpairs first appeared on R-bloggers."
2021,6,24,May 2021: “Top 40” New CRAN Packages,https://www.r-bloggers.com/2021/06/may-2021-top-40-new-cran-packages/,"Two hundred five packages made it to CRAN in May but seven were removed before this post went to print. Here are my “Top40” picks in ten categories: Computational Methods Data Genomics Machine Learning Medicine Science Statistics Time Series Utilities and Visualization.
Computational Methods
madgrad v0.1.0: Implements MADGRAD a Momentumized ...
The post May 2021: “Top 40” New CRAN Packages first appeared on R-bloggers."
2021,6,24,Strategic Analytics at Monash University: How RStudio Accelerated the Transformation,https://www.r-bloggers.com/2021/06/strategic-analytics-at-monash-university-how-rstudio-accelerated-the-transformation/,"
This is a guest post from Dr. Behrooz Hassani-Mahmooei Director of Strategic Intelligence and Insights Unit and his team at Monash University Australia
Photo credit: Monash University
Similar to any other large and complex organisatio...


The post Strategic Analytics at Monash University: How RStudio Accelerated the Transformation first appeared on R-bloggers."
2021,6,24,Winners of the 3rd annual Shiny Contest,https://www.r-bloggers.com/2021/06/winners-of-the-3rd-annual-shiny-contest/,"
Once again the Shiny community has wowed us with their contributions to the 3rd annual Shiny Contest that we announced back in March 2021.
We had 179 submissions from 164 unique Shiny developers to the contest this year over the two-mon...


The post Winners of the 3rd annual Shiny Contest first appeared on R-bloggers."
2021,6,24,How We Curate Our Monthly Newsletter,https://www.r-bloggers.com/2021/06/how-we-curate-our-monthly-newsletter/,"How to keep up with rOpenSci?
We agree that we’re doing so much good work that it’s hard. 😉
More seriously we’ve been curating and sharing a news digest with our community for years because we believe it to be useful.
Over time its ...
The post How We Curate Our Monthly Newsletter first appeared on R-bloggers."
2021,6,24,Shiny on ECS,https://www.r-bloggers.com/2021/06/shiny-on-ecs/,"
A recipe for setting up a simple Shiny app on ECS.
Docker Image
📢 If you want to simply use my Docker image for testing then you can skip this section and go straight to deploying.
Since this is for illustration purposes I’m going to keep the a...


The post Shiny on ECS first appeared on R-bloggers."
2021,6,23,Evaluation of the UEFA Euro 2020 group stage forecast,https://www.r-bloggers.com/2021/06/evaluation-of-the-uefa-euro-2020-group-stage-forecast/,"
    A look back on the group stage of the UEFA Euro 2020 to check whether our hybrid machine learning forecasts based were any good...
    How surprising was the group stage?
Yesterday the group stage of the UEFA Euro 2020 was conclude...


The post Evaluation of the UEFA Euro 2020 group stage forecast first appeared on R-bloggers."
2021,6,23,SHAP Analysis in 9 Lines,https://www.r-bloggers.com/2021/06/shap-analysis-in-9-lines/," This post shows how to make very generic and quick SHAP interpretations of XGBoost and LightGBM models.


The post SHAP Analysis in 9 Lines first appeared on R-bloggers."
2021,6,23,Using R code in Java Eclipse with rJava,https://www.r-bloggers.com/2021/06/using-r-code-in-java-eclipse-with-rjava/,"    This post shows how to use R code in Eclipse IDE for Java with rJava package. But this work requires several environment setting which is a little bit confusing. Although there are some good posts regarding this issue we use a step by step guide wi...


The post Using R code in Java Eclipse with rJava first appeared on R-bloggers."
2021,6,23,Compare data frames in R-Quick Guide,https://www.r-bloggers.com/2021/06/compare-data-frames-in-r-quick-guide/,"Compare data frames in R In this tutorial we are going to describe how to compare data frames in R. Let’s create a data frame...
The post Compare data frames in R-Quick Guide appeared first on finnstats.
The post Compare data frames in R-Quick Guide first appeared on R-bloggers."
2021,6,23,EARL online – presentation day highlights,https://www.r-bloggers.com/2021/06/earl-online-presentation-day-highlights/,"The final day of the Enterprise Applications of the R Language Conference will be a day full of presentations from...
The post EARL online – presentation day highlights appeared first on Mango Solutions.
The post EARL online – presentation day highlights first appeared on R-bloggers."
2021,6,23,MI2 talks at useR! 2021,https://www.r-bloggers.com/2021/06/mi2-talks-at-user-2021/," The useR! 2021 Conference is starting soon on July 5. This year MI2DataLab will showcase our recent R packages and applications within a 3h workshop on Responsible Machine Learning and five talks. Feel free to hook us up during the conference especia...


The post MI2 talks at useR! 2021 first appeared on R-bloggers."
2021,6,23,R Coding Challenge: 7 (+1) Ways to Solve a Simple Puzzle,https://www.r-bloggers.com/2021/06/r-coding-challenge-7-1-ways-to-solve-a-simple-puzzle/,"This time we want to solve the following simple task with R: Take the numbers 1 to 100 square them and add all the even numbers while subtracting the odd ones! If you want to see how to do that in at least seven different ways in R read on! There are ...
The post R Coding Challenge: 7 (+1) Ways to Solve a Simple Puzzle first appeared on R-bloggers."
2021,6,23,Notes from the 3rd Insurance Data Science event,https://www.r-bloggers.com/2021/06/notes-from-the-3rd-insurance-data-science-event/," Finally the Insurance Data Science conference was back last week. After last year’s cancellation due to Covid-19 over 250 delegates from around the world came together on-line for the third instalment of the conference.
The event kicked-off or shou...


The post Notes from the 3rd Insurance Data Science event first appeared on R-bloggers."
2021,6,22,10 Tips and Tricks for Data Scientists Vol.9,https://www.r-bloggers.com/2021/06/10-tips-and-tricks-for-data-scientists-vol-9/," We have started a series of articles on tips and tricks for data scientists (mainly in Python and R). In case you have ... Read more10 Tips and Tricks for Data Scientists Vol.9


The post 10 Tips and Tricks for Data Scientists Vol.9 first appeared on R-bloggers."
2021,6,22,5 Great Shiny Dashboards from Appsilon,https://www.r-bloggers.com/2021/06/5-great-shiny-dashboards-from-appsilon/,"
5 Great Shiny Dashboards – Appsilon’s Top Picks This article will cover a few of the best Shiny Dashboards created by the Appsilon team. R/Shiny is an R package that makes it easy to build interactive dashboards that look great in R. Additionally we can build Machine Learning or AI ...


The post 5 Great Shiny Dashboards from Appsilon first appeared on R-bloggers."
2021,5,10,W. Edwards Deming’s Contributions to the Practice of Analytics,https://analyticstrategy.com/demings-contributions-to-the-practice-of-analytics/?utm_source=rss&utm_medium=rss&utm_campaign=demings-contributions-to-the-practice-of-analytics,"How Deming integrated deep technical knowledge about statistical quality control good practical knowledge of how to implement statistical quality control and a great ability to teach.
The post W. Edwards Deming&#8217;s Contributions to the Practice of Analytics appeared first on Analytic Strategy Partners."
2021,4,9,Lessons from the Ever Given and Archegos: Four Ways Predictive Models Fail,https://analyticstrategy.com/lessons-from-the-ever-given-and-archegos-four-ways-predictive-models-fail/?utm_source=rss&utm_medium=rss&utm_campaign=lessons-from-the-ever-given-and-archegos-four-ways-predictive-models-fail,"Understanding the hierarchy of uncertainty and four ways predictive models fail - from false positives to black swans and deep uncertainty.
The post Lessons from the Ever Given and Archegos: Four Ways Predictive Models Fail appeared first on Analytic Strategy Partners."
2021,3,15,Profiles in Analytics: Frank Knight,https://analyticstrategy.com/profiles-in-analytics-frank-knight/?utm_source=rss&utm_medium=rss&utm_campaign=profiles-in-analytics-frank-knight,"Frank Knight's important distinction between risk and uncertainty and its relevance to analytics
today.
The post Profiles in Analytics: Frank Knight appeared first on Analytic Strategy Partners."
2021,2,15,How to Navigate the Challenging Journey from an AI Algorithm to an AI Product,https://analyticstrategy.com/the-journey-from-an-ai-algorithm-to-an-ai-product/?utm_source=rss&utm_medium=rss&utm_campaign=the-journey-from-an-ai-algorithm-to-an-ai-product,"It's a long challenging journey from an AI algorithm to an AI product. 
The post How to Navigate the Challenging Journey from an AI Algorithm to an AI Product appeared first on Analytic Strategy Partners."
2021,1,14,Three Reasons All Corporate Boards Need Someone Who Understands Both Analytic Innovation and Analytic Strategy,https://analyticstrategy.com/three-reasons-corporate-boards-need-expertise-in-ai/?utm_source=rss&utm_medium=rss&utm_campaign=three-reasons-corporate-boards-need-expertise-in-ai,"According to a 2019 report from CB Insights [1] between 2010 and 2019 there were 635 AI acquisitions. The acquisitions break into three groups as can be seen in the visualization below (Figure 1) from CB Insights. Facebook Apple Google Microsoft Amazon (FAGMA) and Intel accounted for 67 acquisitions each making 7 or more acquisitions [&#8230;]
The post Three Reasons All Corporate Boards Need Someone Who Understands Both Analytic Innovation and Analytic Strategy appeared first on Analytic Strategy Partners."
2020,12,14,Five Steps to Improve the Analytic Maturity of Your Company – 2021 Edition,https://analyticstrategy.com/five-steps-to-improve-analytic-maturity-of-your-company/?utm_source=rss&utm_medium=rss&utm_campaign=five-steps-to-improve-analytic-maturity-of-your-company,"Half of AI Projects Fail A good rule of thumb is that about half of all AI and analytic projects fail to bring business value. Here are some recent articles that remind us of this: Gil Press writing in Forbes [Press2019] summarized some of the statistics around the failure rate of AI projects. One of [&#8230;]
The post Five Steps to Improve the Analytic Maturity of Your Company &#8211; 2021 Edition appeared first on Analytic Strategy Partners."
2020,11,12,Machine Learning vs AI Business Models – What’s New with the Economics of AI?,https://analyticstrategy.com/machine-learning-vs-ai-business-models/?utm_source=rss&utm_medium=rss&utm_campaign=machine-learning-vs-ai-business-models,"some of the barriers in building a successful AI company
The post Machine Learning vs AI Business Models &#8211; What&#8217;s New with the Economics of AI? appeared first on Analytic Strategy Partners."
2020,10,12,Why Great Machine Learning Models are Never Enough: Three Lessons About Data Science from Dr. Foege’s Letter,https://analyticstrategy.com/data-science-lessons-from-the-foege-letter/?utm_source=rss&utm_medium=rss&utm_campaign=data-science-lessons-from-the-foege-letter,"Foege&#8217;s Letter In September William H. Foege MD MPH sent a private letter to Robert Redfield the Director of the CDC reminding him that the &#8220;best decisions come from the best science&#8221; and the &#8220;best results come from the best management.&#8221; The letter became public on October 6 2020 in a USA today article written [&#8230;]
The post Why Great Machine Learning Models are Never Enough: Three Lessons About Data Science from Dr. Foege&#8217;s Letter appeared first on Analytic Strategy Partners."
2020,9,10,Do You Need a Grand Strategy in Analytics?,https://analyticstrategy.com/do-you-need-a-grand-strategy-in-analytics/?utm_source=rss&utm_medium=rss&utm_campaign=do-you-need-a-grand-strategy-in-analytics,"In foreign affairs and national defense especially among academics it has becoming more common to talk about grand strategies. There is a very popular course at Yale University by John Lewis Gaddis called On Grand Strategy and in 2018 he published a book worth reading with the same name. An emerging definition for grand strategy [&#8230;]
The post Do You Need a Grand Strategy in Analytics? appeared first on Analytic Strategy Partners."
2020,8,10,When You Need to Deploy Predictive Models Safely,https://analyticstrategy.com/deploying-analytic-models-safely/?utm_source=rss&utm_medium=rss&utm_campaign=deploying-analytic-models-safely,"Little languages. A key insight in the development of Unix was that there was an important role for what became known as little languages which are simple specialized languages for executing important types of tasks. The insight was that it is much easier to design a little language that can be implemented efficiently for a [&#8230;]
The post When You Need to Deploy Predictive Models Safely appeared first on Analytic Strategy Partners."
2021,3,28,Random Forest Tutorial: Predicting Goals in Football,https://algobeans.com/2021/03/29/random-forest-tutorial-predicting-goals-in-football/,Learn how a random forest model can help us to predict the probability of a goal with applications ranging from performance appraisal to match-fixing detection.
2020,11,30,Kernel Density Plots,https://algobeans.com/2020/11/30/kernel-density-plots/,Visualizing soccer data we identify hot spots where most shots occur shot preferences across players and comparisons between different teams like Liverpool and Manchester United.
2017,11,2,Self-Organizing Maps Tutorial,https://algobeans.com/2017/11/02/self-organizing-map/,Visualize large datasets and identify potential clusters with this special breed of neural networks that uses neurons to learn the intrinsic shape of your data.
2017,7,18,Layman’s Guide to A/B Testing,https://algobeans.com/2017/07/19/laymans-guide-to-ab-testing/,A/B tests help you decide between two options A and B. Read this step-by-step guide on conducting your own A/B test to make the right decisions.
2017,4,4,Time Series Analysis with Generalized Additive Models,https://algobeans.com/2017/04/04/laymans-tutorial-time-series-analysis/,Whenever you spot a trend plotted against time you would be looking at a time series. The de facto choice for studying financial market performance and weather forecasts time series are one of the most pervasive analysis techniques because of its inextricable relation to time - we are always interested to foretell the future.
2016,11,3,Artificial Neural Networks Introduction (Part II),https://algobeans.com/2016/11/03/artificial-neural-networks-intro2/,In the 2nd part of our tutorial on artificial neural networks we cover 3 techniques to improve prediction accuracy: distortion mini-batch gradient descent and dropout.
2016,9,14,k-Nearest Neighbors & Anomaly Detection Tutorial,https://algobeans.com/2016/09/14/k-nearest-neighbors-anomaly-detection-tutorial/,Do you know what gives red and white wine their colors? Use k-NN to discover the chemical make-up that defines typical types of wines as well as to detect atypical ones.
2016,8,25,Random Forest Tutorial: Predicting Crime in San Francisco,https://algobeans.com/2016/08/25/random-forest-tutorial/,Learn how random forests an ensemble of decision trees can help predict where and when a crime will happen in San Francisco California.
2016,7,27,Decision Trees Tutorial,https://algobeans.com/2016/07/27/decision-trees-tutorial/,Decision trees can be used to identify customer profiles or to predict who will resign. Using the Titanic dataset learn about its advantages and pitfalls as well as better alternatives.
2016,6,14,Principal Component Analysis Tutorial,https://algobeans.com/2016/06/15/principal-component-analysis-tutorial/,You are exploring the nutritional content of food. How can food items be differentiated? How might they be classified? PCA derives underlying variables that help you slice your data for these insights.
2021,6,24,Cloud data warehouse startup Firebolt closes $127M Series B funding round,https://www.zdnet.com/article/cloud-data-warehouse-startup-firebolt-closes-127m-series-b-funding-round/#ftag=RSSbaffb68,The cloud data warehouse startup which is focused on application-oriented analytics over big data will use the new funds for expansion of its product engineering and go-to-market teams.
2021,6,24,HPE and Nutanix partner to add Database-as-a-Service on GreenLake cloud service,https://www.zdnet.com/article/hpe-and-nutanix-partner-to-add-database-as-a-service-on-greenlake-cloud-service/#ftag=RSSbaffb68,HPE is partnering with Nutanix to expand HPE GreenLake’s DBaaS portfolio. The joint offering will make legacy databases first-class citizens in a hybrid environment that operates and soon will be billed as a cloud service.
2021,6,22,More than words: Shedding light on the data terminology mess,https://www.zdnet.com/article/more-than-words-shedding-light-on-the-data-terminology-mess/#ftag=RSSbaffb68,Data management data governance data observability data fabric data mesh DataOps MLOps AIOps. It's a data terminology mess out there. Let's try and untangle it because there's more to words than lingo.
2021,6,22,Databricks cofounder’s next act: Shining a Ray on serverless autoscaling,https://www.zdnet.com/article/databricks-cofounders-next-act-shining-a-ray-on-compute-autoscaling/#ftag=RSSbaffb68,After helping shepherd Spark to surmount the data bottleneck UC Berkeley’s Ion Stoica is helping unleash Ray an emerging open source project to get over the compute bottleneck for scaling machine learning models into production. It will allow any developer to launch their own serverless cluster through a universal API that works anywhere.
2021,6,17,"The biggest investment in database history, the biggest social network ever, and other graph stories from Neo4j",https://www.zdnet.com/article/the-biggest-investment-in-database-history-the-biggest-social-network-ever-and-other-graph-stories-from-neo4j/#ftag=RSSbaffb68,A $325 million Series F funding round bringing Neo4j's valuation to over $2 billion. A social network of 3 billion people distributed across 1000 servers. The latter is a demo; the former is not. But both are real signs that the graph market and Neo4j are getting huge.
2021,6,17,Timescale scales out and sets its sights on analytics,https://www.zdnet.com/article/timescale-scales-out-and-sets-its-sights-on-analytics/#ftag=RSSbaffb68,Yes there’s another time series database on the radar screen and unlike most of them it runs on PostgreSQL. In addition the latest release adds support for multi-node distributed deployment.
2021,6,15,DataStax launches beta of Astra Streaming service,https://www.zdnet.com/article/datastax-launches-beta-of-astra-streaming-service/#ftag=RSSbaffb68,DataStax is placing its streaming bet on Apache Pulsar an emerging rival to Kafka unveiling a new cloud-managed service in its Astra portfolio.
2021,6,14,Where is IBM’s hybrid cloud launchpad?,https://www.zdnet.com/article/where-is-ibms-hybrid-cloud-launchpad2020,2,5,Dis-aggregated hardware,http://themainstreamseer.blogspot.com/2020/02/dis-aggregated-hardware.html,y
2020,1,16,Open Source Networking: a hierarchical approach,http://themainstreamseer.blogspot.com/2020/01/open-source-networking-hierarchical.html, 
2020,1,13,Curious about 5G?,http://themainstreamseer.blogspot.com/2020/01/curious-about-5g.html,l
2019,7,16,"AI, ML, NN and DL: a visual explanation",http://themainstreamseer.blogspot.com/2019/07/ai-ml-nn-and-dl-visual-explanation.html,a
2019,6,23,The Research Process,http://themainstreamseer.blogspot.com/2019/06/the-research-process.html,t
2019,3,28,Operationalize Trusted AI with IBM Watson OpenScale,http://themainstreamseer.blogspot.com/2019/03/operationalize-trusted-ai-with-ibm.html,e
2019,2,4,Satellite imagery and remote sensing puzzles,http://themainstreamseer.blogspot.com/2019/02/satellite-imagery-and-remote-sensing.html,s
2019,1,9,The world of languages,http://themainstreamseer.blogspot.com/2019/01/the-world-of-languages.html,t
2018,7,22,Producing a map with 5 lines of code,http://themainstreamseer.blogspot.com/2018/07/producing-map-in-5-lines-of-code.html, 
2018,4,29,IBM SPSS and Entity Analytics at work,http://themainstreamseer.blogspot.com/2018/04/ibm-spss-and-entity-analytics-at-work.html,p
2018,4,21,Testing Senzing's Entity Resolution Workbench,http://themainstreamseer.blogspot.com/2018/04/testing-senzings-entity-resolution.html,u
2018,3,1,"Watson Analytics, SPSS Modeler and Esri ArcGIS",http://themainstreamseer.blogspot.com/2018/03/watson-analytics-spss-modeler-and-esri.html,b
2017,11,14,Visualization of the 1854 London Cholera Outbreak,http://themainstreamseer.blogspot.com/2017/11/visualization-of-1854-london-cholera.html,l
2017,10,27,What caused the Challenger disaster?,http://themainstreamseer.blogspot.com/2017/10/what-caused-challenger-disaster.html,i
2017,10,15,Regression in R,http://themainstreamseer.blogspot.com/2017/10/regression-in-r.html,c
2017,10,7,Coefficient of Alienation,http://themainstreamseer.blogspot.com/2017/10/coefficient-of-alienation.html,l
2017,9,30,Homoscedasticity and heteroscedasticity,http://themainstreamseer.blogspot.com/2017/09/homoscedasticity-and-heteroscedasticity.html,y
2017,9,27,Standard Deviation versus Absolute Mean Deviation,http://themainstreamseer.blogspot.com/2017/09/standard-deviation-versus-absolute-mean.html, 
2017,9,13,Basic Statistics in R,http://themainstreamseer.blogspot.com/2017/09/basic-statistics-in-r.html,a
2017,9,13,Adding a .RData file to DSX in 5 easy steps,http://themainstreamseer.blogspot.com/2017/09/adding-rdata-file-in-5-easy-steps.html,v
2017,9,4,Basic graphs in R,http://themainstreamseer.blogspot.com/2017/09/basic-graphs-in-r.html,a
2017,9,2,Advanced Data Preparation in R,http://themainstreamseer.blogspot.com/2017/09/advanced-data-preparation-in-r.html,i
2017,8,28,Engine bleed air: a primer,http://themainstreamseer.blogspot.com/2017/08/engine-bleed-air-primer.html,l
2017,8,24,Data Preparation in R,http://themainstreamseer.blogspot.com/2017/08/data-preparation-in-r.html,a
2021,6,25,Gradient Flow Snapshot #60: Self-supervised Learning; SaaS CTO Security Checklist; 2021 NLP Survey,http://practicalquant.blogspot.com/2021/06/gradient-flow-snapshot-60-self.html, 
2021,6,24,Training and Sharing Large Language Models,http://practicalquant.blogspot.com/2021/06/training-and-sharing-large-language.html,S
2021,6,18,Gradient Flow Snapshot #59: From Cloud → Sky computing; Automation in DataOps; Top Technology Trends,http://practicalquant.blogspot.com/2021/06/gradient-flow-snapshot-59-from-cloud.html,u
2021,6,17,Questioning the Efficacy of Neural Recommendation Systems,http://practicalquant.blogspot.com/2021/06/questioning-efficacy-of-neural.html,b
2021,6,11,Gradient Flow Snapshot #58: Delta Live Tables; Knowledge Graphs; Data Portability,http://practicalquant.blogspot.com/2021/06/gradient-flow-snapshot-58-delta-live.html,s
2021,6,10,Automation in Data Management and Data Labeling,http://practicalquant.blogspot.com/2021/06/automation-in-data-management-and-data.html,c
2021,6,4,Gradient Flow Snapshot #57: Monitoring Machine Learning Models; Greykite for Time-series Forecasting,http://practicalquant.blogspot.com/2021/06/gradient-flow-snapshot-57-monitoring.html,r
2021,6,3,Reinforcement Learning For the Win,http://practicalquant.blogspot.com/2021/06/reinforcement-learning-for-win.html,i
2021,5,28,Gradient Flow Snapshot #56: Airflow + Ray; Data Warehouse → Lakehouse; CSV file → Knowledge Graph,http://practicalquant.blogspot.com/2021/05/gradient-flow-snapshot-56-airflow-ray.html,b
2021,5,27,How Companies Are Investing in AI Risk and Liability Minimization,http://practicalquant.blogspot.com/2021/05/how-companies-are-investing-in-ai-risk.html,e
2021,5,21,"Gradient Flow Snapshot #55: Reinforcement Learning in the Enterprise, Knowledge Graphs in Finance",http://practicalquant.blogspot.com/2021/05/gradient-flow-snapshot-55-reinforcement.html, 
2021,5,20,The Future of Machine Learning Lies in Better Abstractions,http://practicalquant.blogspot.com/2021/05/the-future-of-machine-learning-lies-in.html,t
2021,5,14,Gradient Flow Snapshot #54: NLP Index and Getting Read for New AI Regulations,http://practicalquant.blogspot.com/2021/05/gradient-flow-snapshot-54-nlp-index-and.html,o
2021,5,13,Why You Should Optimize Your Deep Learning Inference Platform,http://practicalquant.blogspot.com/2021/05/why-you-should-optimize-your-deep.html, 
2021,5,7,Gradient Flow Snapshot #53: Data Validation for Machine Learning; Modernizing Data Governance,http://practicalquant.blogspot.com/2021/05/gradient-flow-snapshot-53-data.html,o
2021,5,6,AI Beyond Automation,http://practicalquant.blogspot.com/2021/05/ai-beyond-automation.html,u
2021,4,30,Gradient Flow Snapshot #52: Data Integration; Language Benchmarks; Online Resource Allocation with Ray,http://practicalquant.blogspot.com/2021/04/gradient-flow-snapshot-52-data.html,r
2021,4,29,Injecting Software Engineering Practices and Rigor into Data Governance,http://practicalquant.blogspot.com/2021/04/injecting-software-engineering.html, 
2021,4,23,Gradient Flow Snapshot #51: What is DataOps; 2021 Technology Radar,http://practicalquant.blogspot.com/2021/04/gradient-flow-snapshot-51-what-is.html,N
2021,4,22,Building a data store for unstructured data and deep learning applications,http://practicalquant.blogspot.com/2021/04/building-data-store-for-unstructured.html,e
2021,4,20,FREE Report: 2021 Business at the Speed of AI Report,http://practicalquant.blogspot.com/2021/04/free-report-2021-business-at-speed-of.html,w
2021,4,16,Gradient Flow Snapshot #50: Data Engineering jobs in the U.S; Algorithms That Make Instacart Roll,http://practicalquant.blogspot.com/2021/04/gradient-flow-snapshot-50-data.html,s
2021,4,15,How Technology Companies Are Using Ray,http://practicalquant.blogspot.com/2021/04/how-technology-companies-are-using-ray.html,l
2021,4,13,FREE Report: 2020 NLP Industry Survey Report,http://practicalquant.blogspot.com/2021/04/free-report-2020-nlp-industry-survey.html,e
2021,4,9,Gradient Flow Snapshot #49: Data Cascades; Exploiting ML Models; Prisma Migrate,http://practicalquant.blogspot.com/2021/04/gradient-flow-snapshot-49-data-cascades.html,t
 Management,https://practicalanalytics.wordpress.com/2016/03/03/4917/,New data driven FinTech business models built on Hadoop Spark and Machine Learning are rapidly emerging and disrupting wealth management.  Here is my recent posting from disruptivedigital.wordpress.com about one such use case&#8230; Robo-Advisors. We are in the early stages of a generational shift in wealth management especially &#8220;plain vanilla&#8221; investing for the mass affluent and millennial segment.  Until recently you had [&#8230;]
2016,2,18,Big Data is Entering the Trough of Disillusionment,https://practicalanalytics.wordpress.com/2016/02/18/big-data-is-entering-the-trough-of-disillusionment/,Big data data lakes and becoming data-driven is no longer news or novel. Hype is not enough anymore. The challenge today for leaders in every enterprises is (a) how to monetize data? (b) how to create enterprise class platforms instead of sandboxes? Basically how can data analytics and insight drive digital operations and digital transformation? The paradox [&#8230;]
2016,2,9,Big Data Evolving Landscape – 2016,https://practicalanalytics.wordpress.com/2016/02/09/big-data-evolving-landscape-2016/,In 2017 Big Data as a term is pretty much dead.   The market &#8211; VCs Startups and F500 companies &#8211; have stopped using Big Data as a term to describe their programs/projects and have moved to Machine Learning (ML) and AI. Everything Big Data is now AI. 2015 was a year of significant change [&#8230;]
2015,9,4,Customer Journey Analytics and Data Science,https://practicalanalytics.wordpress.com/2015/09/04/customer-journey-analytics-and-data-science/,Where do customers abandon the shopping process? Is it the same in every geography? Audience of One&#8230;. Who are your fans versus haters in the marketplace? How do customers feel about your products? How engaged are customers with your brand versus your competitors’ brands across social media and web channels? Fortune 500 companies are making large investments [&#8230;]
2015,7,21,Consumerization of BI: Data Visualization Competency Center,https://practicalanalytics.wordpress.com/2015/07/21/enabling-enterprise-data-mining-and-visualization/,What do users want? Self-service interactive analytics with all kinds of datasets with instant response times no waiting. Today there is a strong move towards &#8220;Consumerization of BI&#8221; as business users are demand the same speed and ease of use from their enterprise applications as their at-home software. Consumerization of BI (“data at your fingertips”) means: Help [&#8230;]
2020,12,3,Machine learning algorithms surprises at deployment? (article on Medium),http://www.bzst.com/2020/12/machine-learning-algorithms-surprises.html,"Machine
 learning (ML) algorithms are being used to generate predictions in 
every corner of our decision-making life. Methods range from “simple” 
algorithms such as trees forests naive Bayes..."
2018,12,10,Forecasting large collections of time series,http://www.bzst.com/2018/12/forecasting-large-collections-of-time.html,"With the recent launch of Amazon Forecast I can no longer procrastinate writing about forecasting ""at scale""!

Quantitative forecasting of time series has been used (and taught) for decades with..."
2018,2,4,Data Ethics Regulation: Two key updates in 2018,http://www.bzst.com/2018/02/data-ethics-regulation-two-key-updates.html,This year two important new regulations will be impacting research with human subjects: the EU's General Data Protection Regulation (GDPR) which kicks in May 2018 and the USA's updated Common...
2017,12,25,Election polls: description vs. prediction,http://www.bzst.com/2017/12/election-polls-description-vs-prediction.html,My papers To Explain or To Predict and Predictive Analytics in Information Systems Research contrast the process and uses of predictive modeling and causal-explanatory modeling. I briefly mentioned...
2017,11,6,"Statistical test for ""no difference""",http://www.bzst.com/2017/11/statistical-test-for-no-difference.html,"To most researchers and practitioners using statistical inference the popular hypothesis testing universe consists of two hypotheses:

H0 is the null hypothesis of ""zero effect""

H1 is the..."
2017,9,5,My videos for “Business Analytics using Data Mining” now publicly available!,http://www.bzst.com/2017/09/my-videos-for-business-analytics-using.html,Five years ago in 2012 I decided to experiment in improving my teaching by creating a flipped classroom (and semi-MOOC) for my course “Business Analytics Using Data Mining” (BADM) at the Indian...
2017,3,14,Data mining algorithms: how many dummies?,http://www.bzst.com/2017/03/data-mining-algorithms-how-many-dummies.html,"There's lots of posts on ""k-NN for Dummies"". This one is about ""Dummies for k-NN""

Categorical predictor variables are very common. Those who've taken a Statistics course covering linear (or..."
2016,12,22,Key challenges in online experiments: where are the statisticians?,http://www.bzst.com/2016/12/key-challenges-in-online-experiments.html,Randomized experiments (or randomized controlled trials RCT) are a powerful tool for testing causal relationships. Their main principle is random assignment where subjects or items are assigned...
2016,10,24,Experimenting with quantified self: two months hooked up to a fitness band,http://www.bzst.com/2016/10/experimenting-with-quantitative-self.html,It's one thing to collect and analyze behavioral big data (BBD) and another to understand what it means to be the subject of that data. To really understand. Yes we're all aware that our social...
2016,4,26,Statistical software should remove *** notation for statistical significance,http://www.bzst.com/2016/04/statistical-software-should-remove.html,Now that the emotional storm following the American Statistical Association's statement on p-values is slowing down (is it? was there even a storm outside of the statistics area?) let's think about...
2016,3,24,A non-traditional definition of Big Data: Big is Relative,http://www.bzst.com/2016/03/a-non-traditional-definition-of-big.html,"I've noticed that in almost every talk or discussion that involves the term Big Data one of the first slides by the presenter or the first questions to be asked by the audience is ""what is Big..."
2015,12,7,Predictive analytics in the long term,http://www.bzst.com/2015/12/predictive-analytics-in-long-term.html,Ten years ago micro-level prediction the way we know it today was nearly absent in companies. MBAs learned about data analysis mostly in a requires statistics course which covered mostly...
2015,8,19,Categorical predictors: how many dummies to use in regression vs. k-nearest neighbors,http://www.bzst.com/2015/08/categorical-predictors-how-many-dummies.html,Recently I've had discussions with several instructors of data mining courses about a fact that is often left out of many books but is quite important: different treatment of dummy variables in...
2015,3,2,Psychology journal bans statistical inference; knocks down server,http://www.bzst.com/2015/03/psychology-journal-bans-statistical.html,In its recent editorial the journal Basic and Applied Social Psychology announced that it will no longer accept papers that use classical statistical inference. No more p-values t-tests or even......
2015,2,7,"Teaching spaces: ""Analytics in a Studio""",http://www.bzst.com/2015/02/teaching-spaces-analytics-in-studio.html,My first semester at NTHU has been a great learning experience. I introduced and taught two new courses in our new Business Analytics concentration (data mining and forecasting). Both courses met...
2014,12,19,New curriculum design guidelines by American Statistical Association: Who will teach?,http://www.bzst.com/2014/12/new-curriculum-design-guidelines-by.html,"The American Statistical Association published new ""Curriculum Guidelines for Undergraduate Programs in Statistical Science"". This is the first update to the guidelines since 2000.
The executive..."
2014,10,16,"What's in a name? ""Data"" in Mandarin Chinese",http://www.bzst.com/2014/10/whats-in-name-data-in-mandarin-chinese.html,"The term ""data"" now popularly used in many languages is not as innocent as it seems. The biggest controversy that I've been aware of is whether the English term ""data"" is singular or plural. The..."
2014,9,26,Humane and Socially Responsible Analytics: A new concentration at National Tsing Hua University,http://www.bzst.com/2014/09/humane-and-socially-responsible.html,This Fall I'm introducing two new elective courses at NTHU's Institute of Service Science: Business Analytics using Data Mining and Business Analytics using Forecasting (if you're wondering about...
2014,9,19,"India redefines ""reciprocity""; Israeli professionals pay the price",http://www.bzst.com/2014/09/india-redefines-reciprocity-israeli.html,After a few years of employment at the Indian School of Business (in 2010 as a visitor and later as a tenured SRITNE Chaired Professor of Data Analytics) the time has come for me to get a new...
2014,4,2,Parallel coordinate plot in Tableau: a workaround,http://www.bzst.com/2014/04/parallel-coordinate-plot-in-tableau.html,The parallel coordinate plot is useful for visualizing multivariate data in a dis-aggregated way where we have multiple numerical measurements for each record. A scatter plot displays two...
2014,3,15,Can women be professors or doctors? Not according to Jet Airways,http://www.bzst.com/2014/03/can-women-be-professors-or-doctors-not.html,"I am already used to the comical scene at airports in Asia where a sign-holder with ""Professor Galit Shmueli"" sees us walk in his/her direction and right away rushes to my husband. Whether or not..."
2014,3,6,The use of dummy variables in predictive algorithms,http://www.bzst.com/2014/03/the-use-of-dummy-variables-in.html,Anyone who has taken a course in statistics that covers linear regression has heard some version of the rule regarding pre-processing categorical predictors with more than two categories and the need...
2013,11,27,Running a data mining contest on Kaggle,http://www.bzst.com/2013/11/running-data-mining-contest-on-kaggle.html,Following the success last year I've decided once again to introduce a data mining contest in my Business Analytics using Data Mining course at the Indian School of Business. Last year I used two...
2013,11,21,The Scientific Value of Testing Predictive Performance,http://www.bzst.com/2013/11/the-value-of-testing-predictive.html,This week's NY Times article Risk Calculator for Cholesterol Appears Flawed and CNN article Does calculator overstate heart attack risk? illustrate the power of evaluating the predictive performance...
2013,11,5,A Tale of Two (Business Analytics) Courses,http://www.bzst.com/2013/11/a-tale-of-two-business-analytics-courses.html,"I have been teaching two business analytics elective MBA-level courses at ISB. One is called ""Business Analytics Using Data Mining"" (BADM) and the other ""Forecasting Analytics"" (FCAS). Although we..."
2018,6,15,Data Science Book: Everybody Lies,http://www.dataminingblog.com/data-science-book-everybody-lies/,Seth Stephens-Davidowitz has written a very entertaining book on big data and how it can be used to understand Humankind. The main idea of Seth is that Google searches is the most powerful source of information to understand what people really think about. Seth argues that the main advantage of Big Data is our ability [&#8230;]
2018,5,28,Data Science Book: Profit Driven Business Analytics,http://www.dataminingblog.com/data-science-book-profit-driven-business-analytics/,Verbeke Baesens and Bravo have written a data science book focusing on profit. Instead of the typical statistical or programming point of view Profit Driven Business Analytics has a self-proclaimed value-centric perspective. This means the book approaches each topic with a focus on profit costs and ROI. Each data science subject is briefly explained and [&#8230;]
2018,5,11,"Data Science Workshop, EPFL, June 4-6th",http://www.dataminingblog.com/data-science-workshop-epfl-june-4-6th/,What do Swisscom Expedia Cisco Google Frontiers and Bühler have in common? They will present use cases at the Data Science Workshop EPFL June 4-6th. Industry Talks &#8220;The 2018 IEEE Data Science Workshop is a new workshop that aims to bring together researchers in academia and industry to share the most recent and exciting advances [&#8230;]
2018,3,19,Creating Value with Big Data Analytics (book review),http://www.dataminingblog.com/creating-value-with-big-data-analytics-book-review/,Verhoef Kooge and Walk have written a detailed and technical book on the application of data analytics to Marketing. While not stated in the title the subtitle makes it clear: the book is dedicated to people in Marketing and Sales. The strong academic background of the authors is transparent in the book which is full [&#8230;]
2017,11,12,Data Analytics for Internal Audit,http://www.dataminingblog.com/data-analytics-for-internal-audit/,This is a guest post from Marcel Baumgartner Data Analytics Expert at Nestlé S.A. Introduction Large publicly listed companies not only have external auditors who check the books but often also a large community of internal auditors. These collaborators provide the company with a sufficient level of assurance in terms of adherence to internal and [&#8230;]
2017,10,21,The academic tip: What is Deep Learning?,http://www.dataminingblog.com/the-academic-tip-what-is-deep-learning/,This is a guest post from Jacques Zuber Data Science Teacher at HEIG-VD. The commonly called deep learning or hierarchical learning is now a popular trend in machine learning. Recently during the Swiss Analytics Meeting Prof. Dr. Sven F. Crone presented how we can use deep learning in the industry in a forecasting perspective (beer [&#8230;]
2017,10,18,"Interview of Jerome Berthier, Head of BI and Big Data at ELCA",http://www.dataminingblog.com/interview-of-jerome-berthier-head-of-bi-and-big-data-at-elca/,Data Mining Research (DMR): Can you tell us who you are and how you came to the field of Data Science? Jerome Berthier (JB): My name is Jerome Berthier I am an engineer in Computer Science and I have an MBA in management. After 10 years working in different roles for an IT provider (developer [&#8230;]
2017,10,12,Will Data Scientists be Replaced by Machines?,http://www.dataminingblog.com/will-data-scientists-be-replaced-by-machines/,Data Science automation is a hot topic recently with several articles about it[1]. Most of them discuss the so-called &#8220;automation&#8221; tools[2]. Too often editors claim that their tools can automate the Data Science process. This provides the feeling that combining these tools with a Big Data architecture can solve any business problems. The misconception comes [&#8230;]
2017,9,1,Data Science Book Review: Statistics Done Wrong,http://www.dataminingblog.com/data-science-book-review-statistics-done-wrong/,If you read this blog you are very likely to be involved in any kind of data collection manipulation or analysis. When not performed wisely your analysis will lead you to incorrect conclusions. Alex Reinhart in his book Statistics Done Wrong has listed several concepts that are key when analysing data such as statistical power [&#8230;]
2017,6,18,Data Science Book Review: Superforecasting,http://www.dataminingblog.com/2437-2/,Superforecasting &#8211; by Tetlock and Gartner &#8211; explains the huge study performed by Tetlock about the ability of people to predict future events (mainly geo-political). The closed questions (i.e. choose between yes/no) are far from real numbers you will predict in business forecasting. Tetlock discusses skills that have been identified as driving accurate forecasts. The [&#8230;]
2021,6,19,Brief report about ICIVIS 2021,https://data-mining.philippe-fournier-viger.com/brief-report-about-icivis-2021/,This week-end I have attended the International Conference on Image Vision and Intelligent system from 18 to 20 June 2021 in Changsha city China. It is a medium-sized conference (about 100 participants) but It is well-organized and there was many &#8230; Continue reading &#8594;
2021,5,5,Approximate Algorithms for High Utility Itemset Mining,https://data-mining.philippe-fournier-viger.com/approximate-algorithms-for-high-utility-itemset-mining/,On this blog I have previously given an introduction to a popular data mining task called high utility itemset mining. Put simply this task aims at finding all the sets of values (items) that have a high importance in a &#8230; Continue reading &#8594;
2021,4,15,UDML 2021 @ ICDM 2021,https://data-mining.philippe-fournier-viger.com/udml-2021-icdm-2021/,Hi all This is to let you know that the UDML workshop on utility driven mining and learning is back again this year at ICDM for the fourth edition. The topic of this workshop is the concept of utility in &#8230; Continue reading &#8594;
2021,4,14,MLiSE 2021 @ PKDD 2021 – a new workshop!,https://data-mining.philippe-fournier-viger.com/mlise-2021-pkdd-2021-a-new-workshop/,I am glad to announce that I am co-organizing a new workshop called MLiSE 2021 (1st international workshop on Machine Learning in Software Engineering) held in conjunction with the ECML PKDD 2021 conference. Briefly the aim of this workshop is &#8230; Continue reading &#8594;
2021,4,11,Mining Episode Rules (video),https://data-mining.philippe-fournier-viger.com/mining-episode-rules-video/,In this blog post I will share the&#160;video&#160;of our most recent paper presented last week at ACIIDS 2021. It is about a new algorithm named POERM for about&#160;analyzing sequences of events or symbols. The algorithm will find rules called &#8220;episode &#8230; Continue reading &#8594;
2021,4,6,A Brief Report about ACIIDS 2021 (13th Asian Conference on Intelligent Information and Database Systems),https://data-mining.philippe-fournier-viger.com/a-brief-report-about-aciids-2021-13th-asian-conference-on-intelligent-information-and-database-systems/,In this blog post I will give a brief report about the ACIIDS 2021 conference that I am attending from April 7–10 2021. What is ACIIDS? ACIIDS is an international conference focusing on intelligent information and database systems. The conference &#8230; Continue reading &#8594;
2021,3,21,"Phrasebank, an interesting tool to improve your academic writing.",https://data-mining.philippe-fournier-viger.com/phrasebank-an-interesting-tool-to-improve-your-academic-writing/,Writing a research paper is not easy! It is a skill that takes time to master. Luckily most research papers follow more or less the same structure. In previous blog posts I have given an overview of how to write &#8230; Continue reading &#8594;
2021,3,10,Papers without code (and the problem of non-reproducible research),https://data-mining.philippe-fournier-viger.com/paper-without-code-non-reproducible-research/,Recently there has been some debate on the Machine Learning sub-Reddit about the reproducibility or I should say the lack of reproducibility of numerous machine learning papers. Several Reddit users complained that they spent much time (sometimes weeks) to try &#8230; Continue reading &#8594;
2021,3,9,An Overview of Pattern Mining Techniques,https://data-mining.philippe-fournier-viger.com/an-overview-of-pattern-mining-techniques-by-data-types/,In this blog post I will give an overview of some of the main pattern mining tasks to explain what kind of patterns can be found in different types of symbolic data. I will describe some main types of data &#8230; Continue reading &#8594;
2021,3,9,How to write a research grant proposal?,https://data-mining.philippe-fournier-viger.com/how-to-write-a-research-grant-proposal/,Today I will discuss how to write a good research grant proposal. This topic is important for researchers who are at the beginning of their careers and want to obtain funding for their research projects. A good research proposal can be career-changing as it &#8230; Continue reading &#8594;
2021,2,8,Data Science on Azure,https://ryanswanstrom.com/2021/02/08/data-science-azure/,"Learn about Building Data Science Solutions on Azure Join us for a discussion on data science on Azure. I was lucky enough to get Priyanshi Singh and Julian Soh to join me for a conversation about their newest book and their data science careers. This should be a fun conversation and highly informative. Please feel [...]
The post Data Science on Azure appeared first on Ryan Swanstrom."
2021,1,25,Data Management & Data Stories with Scott Taylor,https://ryanswanstrom.com/2021/01/25/data-management-data-stories-with-scott-taylor/,"Data Management with Scott Taylor Scott better known as The Data Whisperer has spent over two decades solving DATA problems. Scott is an expert in Data Management and storytelling. He recently wrote a book Telling Your Data Story to help companies do just that. He is also a live streamer video creator podcaster and entertaining [...]
The post Data Management &#038; Data Stories with Scott Taylor appeared first on Ryan Swanstrom."
2021,1,20,Starting a Data Career,https://ryanswanstrom.com/2021/01/20/starting-a-data-career/,"Starting a Data Career If you are just starting a career in data you will not want to miss this live interview. Join me as I speak with Henrique Senra from Simbiose Ventures to discuss the book Getting Started with Data. The book was written out of a desire to provide a better future for [...]
The post Starting a Data Career appeared first on Ryan Swanstrom."
2021,1,19,Stress Free Goal Setting with Deb Eckerling,https://ryanswanstrom.com/2021/01/19/stress-free-goal-setting-deb-eckerling/,"Stress-free Goal Setting with Deb Eckerling Debra Eckerling is a goal-setting expert. She offers coaching workshops and online support for people looking to achieve goals in any aspect of life. She has organized conferences served as an editor for numerous publications and authored 3 books. Deb joined to talk about goal setting and her latest [...]
The post Stress Free Goal Setting with Deb Eckerling appeared first on Ryan Swanstrom."
2021,1,15,Ryan Swanstrom a Guest on #PirateBroadcast,https://ryanswanstrom.com/2021/01/15/ryan-swanstrom-a-guest-on-piratebroadcast/,"I am super excited to have been invited as a guest on another live show. The #PirateBroadcast with Russ Johns. It will be happening January 15 2021 at 8:00am Central Time. I would love to have you stop by if you are available. I will be talking more about my latest project The Example Show [...]
The post Ryan Swanstrom a Guest on #PirateBroadcast appeared first on Ryan Swanstrom."
2021,1,8,Data Science 101 Blog gets a New Home,https://ryanswanstrom.com/2021/01/08/data-science-101-blog-gets-a-new-home/,"The Data Science 101 blog was started almost 9 years ago and the goal was to collect useful information for people looking to break into data science. It did that well but now there are many other resources for learning data science. Thus I have changed the URL and will be altering the focus a [...]
The post Data Science 101 Blog gets a New Home appeared first on Ryan Swanstrom."
2020,11,8,Being Creative with Video on LinkedIn,https://ryanswanstrom.com/2020/11/07/being-creative-with-video-on-linkedin/,"https://youtu.be/TSP5BJs8LsQ
The post Being Creative with Video on LinkedIn appeared first on Ryan Swanstrom."
2020,10,24,Nhung Ho – Data Science in a Cloud World,https://ryanswanstrom.com/2020/10/24/nhung-ho-data-science-in-a-cloud-world/,"This is a great talk for data scientists and managers of technology teams. If you do data science in 2020 or beyond there is a good chance the cloud will be involved. Topics covered: Lessons learned when migrating data science (or technology in general) to the cloudAI services available via different cloud providersWorkflows in the [...]
The post Nhung Ho &#8211; Data Science in a Cloud World appeared first on Ryan Swanstrom."
2020,9,4,Going Live to Grow Your Business – An Interview with Lindy Chapman,https://ryanswanstrom.com/2020/09/04/going-live-to-grow-your-business-an-interview-with-lindy-chapman/,"https://youtu.be/PQ54qHYxAXU
The post Going Live to Grow Your Business &#8211; An Interview with Lindy Chapman appeared first on Ryan Swanstrom."
2021,6,21,Data Visualization and Data Analysis in Python — using the OkCupid dataset (Part 2),https://data36.com/data-visualization-analysis-python/,"This article is about dating and data science! Please welcome our guest author Amy Birdee who has done multiple data science hobby projects recently and built a truly...
The post Data Visualization and Data Analysis in Python &#8212; using the OkCupid dataset (Part 2) appeared first on Data36."
2021,6,10,Negotiating the data science job salary. Should you do that? How do you do that?,https://data36.com/negotiating-the-data-science-job-salary-how-to/,"Should you negotiate your salary before you get into a junior data scientist role? To start: yes! You should always negotiate. Tech companies in particular EXPECT you to...
The post Negotiating the data science job salary. Should you do that? How do you do that? appeared first on Data36."
2021,5,31,Data Cleaning and Exploratory Data Analysis Using the OkCupid Dataset (Part 1),https://data36.com/data-cleaning-and-exploratory-data-analysis-project/,"This article is about dating and data science! Please welcome our guest author Amy Birdee who has done multiple data science hobby projects recently and built a truly...
The post Data Cleaning and Exploratory Data Analysis Using the OkCupid Dataset (Part 1) appeared first on Data36."
2021,5,10,Junior Data Scientist Job Interview Questions (and How to Answer Them),https://data36.com/junior-data-scientist-job-interview-questions-answers/,"In this article I&#8217;ll show you a few junior data scientist job interview questions… And also how to answer them. Before we get started&#8230; This article is part...
The post Junior Data Scientist Job Interview Questions (and How to Answer Them) appeared first on Data36."
2021,4,26,Beautiful Soup Tutorial 2. – How to Scrape Multiple Web Pages,https://data36.com/scrape-multiple-web-pages-beautiful-soup-tutorial/,"Scraping one web page is fun but scraping more web pages is more fun. In this tutorial you’ll learn how to do just that; along the way you’ll...
The post Beautiful Soup Tutorial 2. – How to Scrape Multiple Web Pages appeared first on Data36."
2021,4,19,18 Things You’ll Learn about Data Science Only on the Job,https://data36.com/data-science-on-the-job/,"Data Science on the job is different from the things you see about it in tutorials. I wish someone would&#8217;ve told me about these when I was an...
The post 18 Things You&#8217;ll Learn about Data Science Only on the Job appeared first on Data36."
2021,4,12,How to Apply for a Data Science Job and How to Prepare for Interviews,https://data36.com/apply-prepare-data-science-job-interviews/,"You have an awesome resume a jaw-dropping portfolio and a Pulitzer Prize winning cover letter. What’s next? Now we’re getting to the good stuff: applying for a data...
The post How to Apply for a Data Science Job and How to Prepare for Interviews appeared first on Data36."
2021,3,29,Beautiful Soup Tutorial 1. – An Introduction to Web Scraping with Python,https://data36.com/beautiful-soup-tutorial-web-scraping/,"As a data scientist or data analyst sooner or later you’ll come to a point where you have to collect large amounts of data. Be it a hobby...
The post Beautiful Soup Tutorial 1. – An Introduction to Web Scraping with Python appeared first on Data36."
2021,3,15,"Before you apply for a data science position… (resume, cover letter, website, GitHub help)",https://data36.com/data-science-cv-resume-cover-letter-github/,"This article is all about how to make your application stand out for data science and analytics positions. I&#8217;ll show you what will catch recruiters’ eyes when sifting...
The post Before you apply for a data science position&#8230; (resume cover letter website GitHub help) appeared first on Data36."
2021,3,15,How to Get a Job in Data Science and Analytics (episode #1),https://data36.com/get-job-data-science-analytics/,"Hello fellow job seekers! My name is Peter. About a year ago I was in your shoes. I wanted one of those high-paying jobs in data science and...
The post How to Get a Job in Data Science and Analytics (episode #1) appeared first on Data36."
2021,4,21,How Dataquest Helped an SEO Expert Save Tons of Time,https://www.dataquest.io/blog/data-skills-python-for-seo/,"Antoine Eripret the SEO Lead at Liligo.com decided to learn Python for SEO because he realized he was wasting time.He got interested in search engine optimization as a student and entered the industry full-time after getting his Masters in 2016.As he built experience in the field he says “I realized that I was sometimes doing [&#8230;]
The post How Dataquest Helped an SEO Expert Save Tons of Time appeared first on Dataquest."
2021,4,21,11 Reasons To Learn Bash (A.K.A. Command Line),https://www.dataquest.io/blog/why-learn-the-command-line/,"Learn the command line (also called terminal bash or shell) a skill that is critical for doing data science work and building data pipelines efficiently.
The post 11 Reasons To Learn Bash (A.K.A. Command Line) appeared first on Dataquest."
2021,4,18,Data Analyst Skills – 8 Skills You Need to Get a Job,https://www.dataquest.io/blog/data-analyst-skills/,"What are 5 real-world tasks that cover most of the skills someone needs to be hired as a data analyst?
The post Data Analyst Skills – 8 Skills You Need to Get a Job appeared first on Dataquest."
2021,4,14,Learn R the Right Way in 5 Steps,https://www.dataquest.io/blog/learn-r-for-data-science/,"R is in an increasingly popular language for data analysis and data science. Here's how you can learn R and be sure it sticks so you can get the career you want.
The post Learn R the Right Way in 5 Steps appeared first on Dataquest."
2021,4,12,11 Real World Applications for Python Skills,https://www.dataquest.io/blog/real-world-python-use-cases/,"Python is one of the most frequently-recommended programming languages. You’ve probably heard people say that’s because it’s relatively easy to learn — and that’s true! But is Python actually useful? What are some of the real-world applications for Python skills once you’ve got them?In this post we’ll look at some of the most common use-cases [&#8230;]
The post 11 Real World Applications for Python Skills appeared first on Dataquest."
2021,4,12,"Data Engineer, Data Analyst, Data Scientist — What’s the Difference?",https://www.dataquest.io/blog/data-analyst-data-scientist-data-engineer/,"In the fast-growing field of data the ""big three"" job roles are data engineer data analyst and data scientist. Figure out which is the best fit for you.
The post Data Engineer Data Analyst Data Scientist — What’s the Difference? appeared first on Dataquest."
2021,4,6,Python Practice: Free Ways To Improve Your Python Skills,https://www.dataquest.io/blog/python-practice/,"Getting good Python practice can help solidify your coding skills. Here are some of the best resources for practicing Python:
The post Python Practice: Free Ways To Improve Your Python Skills appeared first on Dataquest."
2021,4,5,You Need Data Skills to Future-Proof Your Career,https://www.dataquest.io/blog/data-skills-to-future-proof-your-career/,"No matter what industry you're in you need data skills to future-proof your career.&#160;&#160;You might be thinking: Vik is the CEO of a company that teaches data science - of course he'd say that! But stick with me for a few more paragraphs I'll walk you through how data was key to all of the [&#8230;]
The post You Need Data Skills to Future-Proof Your Career appeared first on Dataquest."
2021,3,30,Tutorial: Web Scraping with Python Using Beautiful Soup,https://www.dataquest.io/blog/web-scraping-python-using-beautiful-soup/,"Learn how to scrape the web with Python! The internet is an absolutely massive source of data — data that we can access using web scraping and Python!&#160;In fact web scraping is often the only way we can access data. There is a lot of information out there that isn't available in convenient CSV exports [&#8230;]
The post Tutorial: Web Scraping with Python Using Beautiful Soup appeared first on Dataquest."
2021,3,18,Data Analytics Certification: Do You Need a Certificate to Get a Job as a Data Analyst?,https://www.dataquest.io/blog/data-analytics-certification/,"If you’re interested in becoming a data analyst or even just interested in adding some data skills to your resume you’ve probably wondered: do I need some kind of data analytics certification?Finding the real answer to this question is tricky. There are a million data analytics certificate programs out there and they all have a [&#8230;]
The post Data Analytics Certification: Do You Need a Certificate to Get a Job as a Data Analyst? appeared first on Dataquest."
2021,3,16,SQL Operators: 6 Different Types (w/ Examples),https://www.dataquest.io/blog/sql-operators/,"We have previously covered why you need to learn SQL to get a data job in 2021 as well as publishing a full list of SQL commands to help you get started. Next we’re going to be looking at SQL operators.We’re going to cover what exactly SQL operators are before providing a comprehensive list of [&#8230;]
The post SQL Operators: 6 Different Types (w/ Examples) appeared first on Dataquest."
2021,3,15,Why Jorge Prefers Dataquest Over DataCamp for Learning Data Analysis,https://www.dataquest.io/blog/dataquest-datacamp-learn-data-analysis/,"When Jorge Varade decided he wanted to learn data analysis he tried both DataCamp and Dataquest and found he strongly preferred the latter. Here's why.
The post Why Jorge Prefers Dataquest Over DataCamp for Learning Data Analysis appeared first on Dataquest."
2021,3,10,How Long Does It Take to Learn SQL?,https://www.dataquest.io/blog/how-long-learn-sql/,"How long does it take to learn SQL? It depends on your goals and your background so we've broken down a variety of scenarios for you.
The post How Long Does It Take to Learn SQL? appeared first on Dataquest."
2021,3,4,SQL vs T-SQL: Understanding the Differences,https://www.dataquest.io/blog/sql-vs-t-sql/,"SQL or T-SQL — which one do you need to learn? SQL and T-SQL both see heavy use in the database and data science industries. But what exactly are they? These two query languages are very similar both in name and in what they can do so the distinction between them can be difficult to [&#8230;]
The post SQL vs T-SQL: Understanding the Differences appeared first on Dataquest."
2021,2,24,How to Learn Python (Step-by-Step) in 2021,https://www.dataquest.io/blog/learn-python-the-right-way/,"Learn Python the right way avoid the ""cliff of boring"" and give yourself the best chance to actually learn to code by following these steps.
The post How to Learn Python (Step-by-Step) in 2021 appeared first on Dataquest."
2021,2,18,The Best Way to Learn SQL (According to Seasoned Devs),https://www.dataquest.io/blog/best-way-to-learn-sql/,"What's the best way to learn SQL? With all of the resources available learning SQL the “right way” can be difficult. Finding the best way to learn SQL is tricky because everyone learns things differently. But after training tens of thousands of students — seeing what works and what doesn’t — we’ve come up with [&#8230;]
The post The Best Way to Learn SQL (According to Seasoned Devs) appeared first on Dataquest."
2021,2,17,SQL Commands: The Complete List (w/ Examples),https://www.dataquest.io/blog/sql-commands/,"What can you do with SQL? Here's a reference guide to the most commonly-used SQL commands with code examples.
The post SQL Commands: The Complete List (w/ Examples) appeared first on Dataquest."
2021,2,11,SQL vs MySQL: A Simple Guide to the Differences,https://www.dataquest.io/blog/sql-vs-mysql/,"SQL and MySQL are important tools for working with data. But what are they exactly and how are they different? Let's clear that up. 
The post SQL vs MySQL: A Simple Guide to the Differences appeared first on Dataquest."
2021,2,1,SQL Interview Questions — Real Questions to Prep for Your Job Interview,https://www.dataquest.io/blog/sql-interview-questions/,"A lot of the SQL interview questions you'll find on the web are generic: ""What is SQL?"" You'll never be asked that. We've got real questions to help you prep.
The post SQL Interview Questions — Real Questions to Prep for Your Job Interview appeared first on Dataquest."
2021,2,1,SQL Basics — Hands-On Beginner SQL Tutorial Analyzing Bike-Sharing,https://www.dataquest.io/blog/sql-basics/,"Learn the SQL basics and go hands-on querying databases as you analyze bike rental data in this free beginner SQL tutorial.
The post SQL Basics — Hands-On Beginner SQL Tutorial Analyzing Bike-Sharing appeared first on Dataquest."
2021,1,29,Want a Job in Data? Learn SQL.,https://www.dataquest.io/blog/why-sql-is-the-most-important-language-to-learn/,"Learning SQL might not be as ""sexy"" as learning Python or R but it's a fundamental skill for almost every data scientist and data analyst job. Here's why.
The post Want a Job in Data? Learn SQL. appeared first on Dataquest."
2021,1,20,SQL Cheat Sheet — SQL Reference Guide for Data Analysis,https://www.dataquest.io/blog/sql-cheat-sheet/,"Whether you’re learning SQL through one of our interactive SQL courses or by some other means it can be really helpful to have a SQL cheat sheet.Bookmark this article or download and print the PDF and keep it handy for quick reference the next time you’re writing an SQL query!Our SQL cheat sheet goes a [&#8230;]
The post SQL Cheat Sheet — SQL Reference Guide for Data Analysis appeared first on Dataquest."
2021,1,19,Do You Need a SQL Certification to Get a Data Job in 2021?,https://www.dataquest.io/blog/sql-certification/,"If you want to work in data do you need a SQL certification? That’s a question that can be difficult to answer especially with different organizations pushing to get you to spend money on their certificate programs. Table Of Contents (click to expand) 1Do you need to learn SQL? Yes.2Do you need a SQL certificate? [&#8230;]
The post Do You Need a SQL Certification to Get a Data Job in 2021? appeared first on Dataquest."
2021,1,18,SQL Joins Tutorial: Working with Databases,https://www.dataquest.io/blog/sql-joins-tutorial/,"Learn how to master joins in the SQL joins tutorial. Learn to use inner left right and outer joins while analyzing CIA factbook data.
The post SQL Joins Tutorial: Working with Databases appeared first on Dataquest."
2021,1,13,45 Fun (and Unique) Python Project Ideas for Easy Learning,https://www.dataquest.io/blog/python-projects-for-beginners/,"Building projects is an extremely succesful way to learn but building Python projects for beginners can be difficult. Learn how to build with success!
The post 45 Fun (and Unique) Python Project Ideas for Easy Learning appeared first on Dataquest."
2021,1,12,SQL Tutorial: Selecting Ungrouped Columns Without Aggregate Functions,https://www.dataquest.io/blog/sql-tutorial-selecting-ungrouped-columns-without-aggregate-functions/,"When is a SQL query that returns the correct answer actually wrong? In this tutorial we're going to take a close look at a very common mistake. It's one that will actually return the right answer but it's still a mistake that's important to avoid.That probably sounds rather mysterious so let's dive right in. We'll [&#8230;]
The post SQL Tutorial: Selecting Ungrouped Columns Without Aggregate Functions appeared first on Dataquest."
2020,12,7,Apply to Dataquest and AI Inclusive’s Under-Represented Genders 2021 Scholarship!,https://www.dataquest.io/blog/dataquest-scholarship-women-underrepresented-genders/,"Dataquest is launching another data science scholarship for women and anyone who identifies as an underrepresented gender in data science!
The post Apply to Dataquest and AI Inclusive&#8217;s Under-Represented Genders 2021 Scholarship! appeared first on Dataquest."
2020,11,5,Beginner Python Tutorial: Analyze Your Personal Netflix Data,https://www.dataquest.io/blog/python-tutorial-analyze-personal-netflix-data/,"How much time have you spent watching The Office on Netflix? Find out with this entry-level tutorial on analyzing your own Netflix usage data!
The post Beginner Python Tutorial: Analyze Your Personal Netflix Data appeared first on Dataquest."
2020,10,21,R vs Python for Data Analysis — An Objective Comparison,https://www.dataquest.io/blog/python-vs-r/,"Python vs. R — which is better for data science? We compare the two languages side by side and see how Python and R perform on the same analysis steps.
The post R vs Python for Data Analysis — An Objective Comparison appeared first on Dataquest."
2020,10,8,How to Learn Fast: 7 Science-Backed Study Tips for Learning New Skills,https://www.dataquest.io/blog/how-to-learn-fast-science-based-study-tips/,"Want to learn a new skill as fast as possible? These science-based tips will show you how you can learn more efficiently.
The post How to Learn Fast: 7 Science-Backed Study Tips for Learning New Skills appeared first on Dataquest."
2020,10,6,“Not Enough Memory” — How Data Skills Ended an Excel Nightmare,https://www.dataquest.io/blog/dataquest-changed-my-life/,"How learning data science and Python with Dataquest helped Curtly Critchlow solve a massive Excel problem at his job.
The post “Not Enough Memory” — How Data Skills Ended an Excel Nightmare appeared first on Dataquest."
2020,10,4,How to Write a Great Data Science Resume,https://www.dataquest.io/blog/how-data-science-resume-cv/,"How can you get a data science job? It all starts with a great resume: one that frames your data analysis and data science projects in the right way.
The post How to Write a Great Data Science Resume appeared first on Dataquest."
2020,10,1,How Long Does it Take to Learn Python?,https://www.dataquest.io/blog/how-long-does-it-take-to-learn-python/,"Are you ready to learn Python for Data Science? With the right program habits and structure you can master it more quickly than you might think.
The post How Long Does it Take to Learn Python? appeared first on Dataquest."
2020,9,22,Practical Data Ethics — How You Can Make Your Data Work More Ethical,https://www.dataquest.io/blog/practical-data-ethics-make-your-data-work-more-ethical/,"How can you as a junior data professional help ensure your company's data work is more ethical? Here are practical strategies and tactics.
The post Practical Data Ethics — How You Can Make Your Data Work More Ethical appeared first on Dataquest."
2020,9,16,Making Learning to Code Friendlier with Art — An Interview with Dr. Allison Horst,https://www.dataquest.io/blog/making-learning-to-code-friendlier-with-art-allison-horst-interview/,"Learning to code for the first time can be intimidating. One way to make it more approachable? Fun pictures of fuzzy monsters.
The post Making Learning to Code Friendlier with Art — An Interview with Dr. Allison Horst appeared first on Dataquest."
2020,9,16,21 Places to Find Free Datasets for Data Science Projects,https://www.dataquest.io/blog/free-datasets-for-projects/,"A collection of the best places to find free data sets for data visualization data cleaning machine learning and data processing projects.
The post 21 Places to Find Free Datasets for Data Science Projects appeared first on Dataquest."
2020,9,10,11 High-Paying Data Analytics Jobs in 2020,https://www.dataquest.io/blog/10-data-analytics-jobs/,"Thinking about kickstarting a career in data analytics? These 10 high-paying jobs may just be the motivation you need to learn more about the data science industry and gain the specific skills you need to succeed. 
The post 11 High-Paying Data Analytics Jobs in 2020 appeared first on Dataquest."
2020,9,2,Data Visualization in R with ggplot2: A Beginner Tutorial,https://www.dataquest.io/blog/data-visualization-in-r-with-ggplot2-a-beginner-tutorial/,"A famous general is thought to have said “A good sketch is better than a long speech.” That advice may have come from the battlefield but it's applicable in lots of other areas — including data science. ""Sketching"" out our data by visualizing it using ggplot2 in R is more impactful than simply describing the [&#8230;]
The post Data Visualization in R with ggplot2: A Beginner Tutorial appeared first on Dataquest."
2020,9,2,Tutorial: Web Scraping with Python Using Beautiful Soup,https://www.dataquest.io/blog/web-scraping-tutorial-python/,"Web scraping allows us to extract information from web pages. In this tutorial you'll learn how to perform web scraping with Python and BeautifulSoup.
The post Tutorial: Web Scraping with Python Using Beautiful Soup appeared first on Dataquest."
2020,9,1,How to Use If-Else Statements and Loops in R,https://www.dataquest.io/blog/control-structures-in-r-using-loops-and-if-else-statements/,"Learn to use if-else statements for loops and while loops to build complex conditional programs in R a valuable skill for aspiring data scientists and R programmers alike.
The post How to Use If-Else Statements and Loops in R appeared first on Dataquest."
2020,8,28,Do You Post Too Much? Analyze Your Personal Facebook Data with Python,https://www.dataquest.io/blog/analyze-facebook-data-python/,"As of Q2 2020 Facebook claims more than 2.7 billion active users. That means that if you're reading this article chances are you're a Facebook user. But just how much of a Facebook user are you? How much do you really post? We can find out using Python!&#160;Specifically we're going to use Python to create [&#8230;]
The post Do You Post Too Much? Analyze Your Personal Facebook Data with Python appeared first on Dataquest."
2020,8,26,Best Data Science Books in 2020 (Vetted by Experts),https://www.dataquest.io/blog/data-science-books/,"Learn Python R machine learning social media scraping and much more from these free data science books you can download today.
The post Best Data Science Books in 2020 (Vetted by Experts) appeared first on Dataquest."
2020,8,24,How to Use Jupyter Notebook in 2020: A Beginner’s Tutorial,https://www.dataquest.io/blog/jupyter-notebook-tutorial/,"Use this tutorial to learn how to create your first Jupyter Notebook important terminology and how easily notebooks can be shared and published online.
The post How to Use Jupyter Notebook in 2020: A Beginner’s Tutorial appeared first on Dataquest."
2020,8,20,Top Tips for Learning R from Africa R’s Shelmith Kariuki,https://www.dataquest.io/blog/top-tips-for-learning-r-from-africa-rs-shelmith-kariuki/,"If you’re just at the beginning of your journey learning R programming and you're looking for tips there’s a lot you can learn from Shelmith Kariuki.Shel has years of professional experience working in data science and years of experience teaching statistics and data skills to others. She’s a data analyst an RStudio certified Tidyverse instructor [&#8230;]
The post Top Tips for Learning R from Africa R’s Shelmith Kariuki appeared first on Dataquest."
2020,8,19,How to Become a Data Scientist (Step-By-Step) in 2020,https://www.dataquest.io/blog/how-to-become-a-data-scientist/,"Data science is one of the most buzzed about fields right now and&#160;data scientists are in extreme demand. And with good reason — data scientists are doing everything from&#160;creating self-driving cars&#160;to&#160;automatically captioning images. Given all the interesting applications it makes sense that data science is a very sought-after career.&#160;&#160;Data science is applied in many field [&#8230;]
The post How to Become a Data Scientist (Step-By-Step) in 2020 appeared first on Dataquest."
2020,8,15,Python API Tutorial: Getting Started with APIs,https://www.dataquest.io/blog/python-api-tutorial/,"In this data science tutorial learn about APIs by analyzing data from the international space station in this step-by-step Python API tutorial.
The post Python API Tutorial: Getting Started with APIs appeared first on Dataquest."
2020,8,14,Can Anyone Learn to Code? Yes! (It’s Science),https://www.dataquest.io/blog/can-anyone-learn-to-code-yes-its-science/,"Anyone can learn to code — and that's not just a nice-sounding platitude there's real science backing it up. Not a math person? That could actually help...
The post Can Anyone Learn to Code? Yes! (It’s Science) appeared first on Dataquest."
2020,8,11,How to Learn Python for Data Science In 5 Steps,https://www.dataquest.io/blog/how-to-learn-python-for-data-science-in-5-steps/,"Interested in how to learn Python for data science? It's not as hard as you think! Here's a clear roadmap for learning Python programming and other data science skills.
The post How to Learn Python for Data Science In 5 Steps appeared first on Dataquest."
2020,8,5,Tutorial: Getting Started with R and RStudio,https://www.dataquest.io/blog/tutorial-getting-started-with-r-and-rstudio/,"Get your R programming journey off on the right foot with this RStudio tutorial that walks through everything from installation to best practices.
The post Tutorial: Getting Started with R and RStudio appeared first on Dataquest."
2020,8,4,When Learning is Hard: 3 Ways to Make it Easier (Guest Post),https://www.dataquest.io/blog/3-ways-to-make-learning-easier/,"The following is a guest post by Darya Jandossova Troncoso and does not necessarily represent the views or opinions of Dataquest.Learning is a lifelong process. It starts when we're babies and follows us into old age. Education is essential to our development and to how we see the world. The desire for knowledge starts at [&#8230;]
The post When Learning is Hard: 3 Ways to Make it Easier (Guest Post) appeared first on Dataquest."
2021,6,3,Movie Clip- NSA,https://decisionstats.com/2021/06/03/movie-clip-nsa/,
2021,3,5,MLFlow on Azure Databricks,https://decisionstats.com/2021/03/05/mlflow-on-azure-databricks/,On Azure Databricks you can create experiments using MLFlow https://mlflow.org/ notebook_path = &#8216;/Users/Ajay/Folder&#8217; notebook_path = notebook_pathmlflow.set_experiment(notebook_path + &#8216;_experiments&#8217;) with mlflow.start_run(run_name=&#8221;ExperimentRun&#8221;+curr_ts):mlflow.log_params({&#8216;RSME&#8217;: RSME&#8216;AUC&#8217;: AUC })mlflow.end_run() https://docs.microsoft.com/en-us/azure/databricks/applications/mlflow/ https://docs.microsoft.com/en-us/azure/databricks/applications/mlflow/quick-start-python
2021,1,5,Extract date from datetime in Pandas column,https://decisionstats.com/2021/01/05/extract-date-from-datetime-in-pandas-column/,use .dt.date df[&#8216;column&#8217;] = pd.to_datetime(df[&#8216;column&#8217;] format=&#8217;%Y-%m-%d&#8217;).dt.date Source- https://stackoverflow.com/questions/16176996/keep-only-date-part-when-using-pandas-to-datetime
2021,5,28,Oracle Function Returns Two Values,https://www.deep-data-mining.com/2021/05/oracle-function-returns-two-values.html,pre{border: 2px solid #666; padding: 10px; background-color: lightgray;} pre.code {border: 2px solid #666; padding: 10px; background-color: lightyellow;} pre.out {border: 2px solid #666; padding: 10px; background-color: lightgray;} div.code {color:blue} There is a table in a schema that contains three columns p low and hi. In the table p is the primary key. I want to develop a function to return low and hi based on an input variable p. First I create a type.create or replace type t_low_hi as object ( low number hi number);   Then I create a function that finds low and hi based on p constructs a type object and returns it.   create or replace function f_prob (p_p number)  return t_low_hi is  p_Low number;  p_Hi number;  Str_sql varchar2(2000);  begin  Str_sql := 'Select low hi from t_lookup where p=:1';  Execute immediate str_sql into p_low p_hi using p_p;  return t_low_hi(p_low p_hi);  end;  /  I call the function and retrieve low and hi for p with a value of 0.99.  select x.v.low  x.v.hi from (select f_prob(0.99) v from dual) x;    
2021,1,13,Updated Online Chinese Document Analytics Tool,https://www.deep-data-mining.com/2021/01/updated-online-chinese-document.html,"pre{border: 2px solid #666; padding: 10px; background-color: lightgray;} pre.code {border: 2px solid #666; padding: 10px; background-color: lightyellow;} pre.out {border: 2px solid #666; padding: 10px; background-color: lightgray;} div.code {color:blue} We have updated our free online tool for analyzing Chinese documents: https://aistrike.us/text-analysis.html A user fills in a textbox with the content and click Submit button. The tool identifies words displays a word cloud picture and calculates a sentiment index for each sentence. Enjoy!  Identifying words in a sentence is necessary.  Chinese words in a sentence are next to each other without spaces separating them e.g.  Chinesewordsinasentencearenexttoeachotherwithoutspacesseparatingthem. And yes the division of words could be ambiguous. For example ""结婚和尚未结婚的"" could mean ""married and unmarried"" (""结婚 | 和 | 尚未结婚的"") or ""married monk unmarried"" (""结婚 | 和尚 | 未结婚的"") .    "
2020,12,29,Inventory Optimization - Calculate Safety Stock,https://www.deep-data-mining.com/2020/12/inventory-optimization-calculate-safety.html,"   h1h2h3h4 {   text-align: center; } pre{border: 2px solid #666; padding: 10px; background-color: lightgray;} pre.code {border: 2px solid #666; padding: 10px; background-color: lightyellow;} pre.out {border: 2px solid #666; padding: 10px; background-color: lightgray;} div.code {color:blue}  Safety stock provides a ""cushion"" in inventory to address the uncertainty in a customer's demand. It is important to maintain the ""right"" amount of safety stock. If it is too low we may not be able to fulfill a customers' orders in a timely fashion. On the other hand safety stock that is too high incurs significant financial and/or logistics burden to the business.   The calculation of safety stock is based on a number of factors include historical customer demands product lead time and fill rate (a.k.a. demand satisfaction). The team at safetystockcalc.com builds a website and describe a popular approach to calculate safety stock using an example.  On the website you may also find two free tools that are useful an online calculator and a downloadable spreadsheet with all the formulas. Enjoy! Online Safety Stock Calculator Screenshot     Spreadsheet Safety Stock Calculator Screenshot     "
2020,12,1,An Online Tool for Analyzing Chinese Text,https://www.deep-data-mining.com/2020/11/an-online-tool-for-analyzing-chinese.html,pre{border: 2px solid #666; padding: 10px; background-color: lightgray;} pre.code {border: 2px solid #666; padding: 10px; background-color: lightyellow;} pre.out {border: 2px solid #666; padding: 10px; background-color: lightgray;} div.code {color:blue} We have developed a free online tool for analyzing Chinese document. The URL of the tool is located at  here.  A user fills in the textbox with the content and click Submit button. The tool identifies words calculates frequencies for those words and display a word cloud picture.   . Enjoy!       
2020,4,21,Real COVID-19 Death Rate,https://www.deep-data-mining.com/2020/04/real-covid-19-death-rate.html,pre{border: 2px solid #666; padding: 10px; background-color: lightgray;} pre.code {border: 2px solid #666; padding: 10px; background-color: lightyellow;} pre.out {border: 2px solid #666; padding: 10px; background-color: lightgray;} div.code {color:blue} In my last post we did research on COVID-19 death rate based on the ratio between the number of deaths and the number of confirmed cases. However this method is inherently flawed. Some infected people did not show up at a hospital or a testing station to get tested. As a result the death rate is exaggerated.  Blood antibody tests on randomly sampled residents in Santa Clara California in early April  shows that the number of people infected is 55 to 85 times more than confirmed cases (https://www.cnn.com/2020/04/17/health/santa-clara-coronavirus-infections-study/index.html). Thus the real death rate for people who infected with coronavirus is between 0.1% and 0.17% which are similar to that of flu.   We use the following charts to illustrate two ways of calculating death rates.    COVID-19 Death Rate (Flawed) = Number of Deaths/ Number of Confirmed Cases  COVID-19 Death Rate (Real) = Number of Deaths/Number of Infected  As we can see the chance of COVID-19 bullet hitting the bullseye i.e. causing death is much slimmer that appears based on confirmed cases alone. 
2020,4,3,Study on COVID-19 Annualized Death Rate,https://www.deep-data-mining.com/2020/04/study-on-covid-19-annualized-death-rate.html,"pre{border: 2px solid #666; padding: 10px; background-color: lightgray;} pre.code {border: 2px solid #666; padding: 10px; background-color: lightyellow;} pre.out {border: 2px solid #666; padding: 10px; background-color: lightgray;} div.code {color:blue} Probably this is the first time you see a chart like this.  When people hear COVID-19 death rate for older people is high they panic. We did research and published a paper on COVID-19 death rate(   Study on COVID-19 Annualized Death Rate). Please notice the death rates for COVID-19 are ""annualized"".  But we have to look at things in context. When COVID-19 death rate is annualized it can be compared with other statistical data that are on annual basis. This is the key contribution of our research. Please click the chart for sharper view. Only when COVID-19 death rate is annualized it can be compared with other statistical data that are on annual basis. This is the key contribution of our research. The following are the conclusions.   Conclusions We propose a method to calculate the annualized death rate (ADR) related to COVID-19. Based on ADR related to COVID-19 and the 2018 death rate for the population of the United States we gain the following insights:Incremental annual death rates related to COVID-19 for age groups 45-54 55-64 65-74 75-84 and 85+ are 0.4% 1.2% 2.2% 3.1% and 6.0%， respectively.Percentages of incremental annual death rates related to COVID-19 for age groups 45-54 55-64 65-74 75-84 and 85+ are 100.9% 131.8% 124.4% 69.8% and 44.5% respectively. If the herd immunity strategy is used in the United States the incremental annual number of deaths related to COVID-19 for people equal to or older than 45 will be between 1.1 and 1.47 million. The PDF file for the paper can be downloaded Study on COVID-19 Annualized Death Rate "
2020,3,3,Querying Database Using Command Line Client and Powershell,https://www.deep-data-mining.com/2020/03/querying-database-using-command-line.html,"pre{border: 2px solid #666; padding: 10px; background-color: lightgray;} pre.code {border: 2px solid #666; padding: 10px; background-color: lightyellow;} pre.out {border: 2px solid #666; padding: 10px; background-color: lightgray;} div.code {color:blue} I have found Powershell is a powerful tool. When it combines with a command line tool such as mysql we can perform sophisticated tasks easily. The output of Powershell is an object instead of a text string. We can perform SQL-like operations such as selecting columns and filtering rows using where clause. Since I found out the power of Powershell last year I have been using Powershell as my major tool to query databases including SQL Server MySQl and Postgre. In this post I demonstrate how to query MySQL using mysql client and Powershell. First we run the following command to set up our host user name password and port.  mysql_config_editor set --login-path=local --host=localhost                          --user=root --port=3306 --passwordWhen we run the above command at a shell it will prompt us to input password. It will create a file .mylogin.cnf that is not humanly readable under home directory (I am using Mac. For Windows the file is under %APPDATA%\MySQL). The reason of doing this is to avoid an annoying security warning message if we supply the password in mysql command. If you don't mind the warning message you can skip the step.  Then I write a Powershell function to query MySQL database. I save the scripts as mysql_f.ps1 on my Mac. function runsql {param($s)mysql --login-path=local -D dataexp -B -e ""$s"" | convertfrom-csv -delimiter `t}  I issue the following command from the shell prompt>. ""./mysql_f.ps1"" Now the function is defined. I test the function. prompt>runsql ""select 1 as val""val---1It worked. The following query displays the table in the current database. prompt>runsql ""select TABLE_SCHEMA table_name create_time        from information_schema.tables where table_schema=database()""TABLE_SCHEMA TABLE_NAME             CREATE_TIME------------ ----------             -----------dataexp      flyway_schema_history  2019-11-21 11:56:31dataexp      mdl_sampled            2020-01-14 10:40:06dataexp      mdl_set                2020-01-14 09:46:59dataexp      sample_user_id         2020-01-14 09:58:34dataexp      t_all_pk1              2019-12-05 11:53:43dataexp      t_all_pk1_int          2019-12-05 22:14:54dataexp      t_all_tab_cols         2019-11-21 15:39:00 From the Powershell output we select only table_name an create_time by piping the output to ""select"" command of Powershell. prompt>runsql ""select TABLE_SCHEMA table_name create_time        from information_schema.tables   where table_schema=database()"" | select table_name create_timeTABLE_NAME             CREATE_TIME----------             -----------flyway_schema_history  2019-11-21 11:56:31mdl_sampled            2020-01-14 10:40:06mdl_set                2020-01-14 09:46:59sample_user_id         2020-01-14 09:58:34t_all_pk1              2019-12-05 11:53:43t_all_pk1_int          2019-12-05 22:14:54t_all_tab_cols         2019-11-21 15:39:00 The above operation is possible because in runsql function we use convertfrom-csv to convert text output of mysql to Powershell object. Convertfrom-csv is really a magic function! Once we have an Powershell object we can do all sorts of things.   "
2019,10,27,Optimizing Inventory Level: Reduce the Inventory Value and Increase Customer Satisfaction Simultaneously,https://www.deep-data-mining.com/2019/10/optimizing-inventory-level-reduce.html," For a large part manufacture company in the transportation industry maintaining the optimal inventory level in their warehouses is crucial to its bottom line. When too many parts are produced and stored it costs the company excessive financial investment and previous warehouse spaces. On the other hand if not enough parts in the warehouses customers will become dissatisfied when orders may not get fulfilled in time. Thus there are two conflicting goals to balance when planning the inventory: reducing inventory value and increasing customer satisfaction. The optimal strategy is to find the sweet spot of inventory level for each individual part that is most economical and maintaining high level of customer satisfaction at the same time.  In a recent project that Dr. Jay Zhou has preformed he is able to reduce the inventory level for his client company by $16 million and still maintain the same level of customer satisfaction. This work is highly received by the client. In this project Dr.Zhou takes advantage of machine learning models and reduces huge number of parts to a much smaller number of homogeneous groups. The ""Demand Satisfaction"" are calculated for these groups.    "
2019,4,25,Onsite Interactive Training Session for Data Analysts/Data Scientists,https://www.deep-data-mining.com/2019/04/onsite-interactive-training-session-for.html,"Dr. Zhou is offering a brand new service: a half-day (3 hours) interactive training session to help a company's data analysts/data scientists improve their skills and productivity. The format of the&nbsp;interactive training session&nbsp;is as follows:1.&nbsp;Dr. Zhou&nbsp;first gives a&nbsp;1.5 hours&nbsp;presentation&nbsp;to describe successful cases of using data analytics to solve business problems share best practices and talk about challenges.2. For the remaining 1.5 hours the audience asks questions and is actively engaged in discussion&nbsp;with Dr. Zhou.&nbsp; This the best part!&nbsp; As one of the scientists said ""Dr. Zhou has helped me solve an issue that I had struggled with&nbsp;for years"".The training session is designed for all data analysts/data scientists. Dr. Zhou shares his battle-tested strategies and best practices that are useful for them regardless what specific analytics tools or programming languages they use.On April 23 2019 Dr. Zhou delivered the interactive training session to one of the top three property insurance company located in Boston. It was extremely well received. The picture attached shows that I am making the presentation (in addition to people in the room there are more people joining the meeting through the phone.)&nbsp;The following are the testimonials&nbsp;from two persons who took the course.""We have learnt a lot from Dr.Zhou'"" Gang Xu Director Data Science at Lincoln Financial Group""Dr. Zhou gave us a great overview of procedures of doing a solid predictive analysis and illustrated real life AI consulting business cases. Dr. Zhou is really experienced in the AI space and his presentation was very well received by data scientists from Lincoln Financial Group. I would highly recommend any data science group to have Dr. Zhou sharing his experiences.&nbsp;""-&nbsp;&nbsp;Dr. Hao Zhou&nbsp;Principal Analyst&nbsp;&nbsp;Data Science at Lincoln Financial GroupThe following are testimonials about my previous talks and training activities.""It was a fortune to have Jay come to our computer science department to share his experience in solving business problems with predictive analytics on February 28 2017. What Jay had presented in his 3 talks each lasting for 1 hour in different topics of data mining was totally impressive and beyond our wildest expectation. Having built competition-winning predictive models for some of the biggest companies and produced hundreds of millions of dollars’ savings Jay shared the secret of his success with students and faculty without reservation. His strong presentations were such an inspiration for our computer science students and faculty and his methodology was innovative and powerful  even for very seasoned data scientists among the audience. Jay thank you so much for your hard work preparing and delivering these presentations! "" - Dr. Wei Ding Professor at University of Massachusetts Boston""Jay is more than just a coder he is a great trainer and a good presenter of theoretical data mining concepts so that they can be understood by most. ""-James Lukenbill Director of IT Project Management OptumBio of Dr.Jiang ZhouDr. Jiang Zhou has two decades of experience building predictive models across industries including telecommunication banking insurance  and smart city. These solutions have resulted in over $200 million savings for clients. Dr. Zhou has been involved in three real world competitions to build best predictive models i.e. a customer credit risk model for a top three cell phone company a bank card fraud detection model for a top 15 bank and a direct sales model for a marketing company. Dr. Zhou's models have won all these competitions. He has founded/co-founded data analytics companies including Business Data Miners Smart Credit and AI Strike. Previously he was a chief statistician at Lightbridge a vice president at Citizens Bank and a consulting member of technical staff at Oracle. Dr. Zhou is the author of an award-wining blog on data analytics https://www.deep-data-mining.com/The normal price for the training service is $6500.  If your company is interested in the service please contact Dr. Zhou at&nbsp;jay.zhou@deep-data-mining.com&nbsp;"
2019,3,7,From Hype to Reality – Powering the AI-Driven Future of Insurance at Insurance AI and Analytics USA - by Ira Sopic,https://www.deep-data-mining.com/2019/03/from-hype-to-reality-powering-ai-driven.html,With 2018 witnessing unprecedented advances in the investment and deployment of artificial intelligence within the insurance industry Insurance Nexus is delighted to announce that the Insurance AI and Analytics USA Summit will return to Chicago for a sixth time in 2019 welcoming more than 450 senior attendees to the Renaissance Chicago Downtown Hotel May 2-3.Featuring an agenda designed to tackle the biggest challenges and opportunities in AI and advanced analytics Insurance AI and Analytics USA is a must-attend for any analytics underwriting claims or marketing innovators seeking to both achieve efficient and seamless operations and deliver valuable and relevant products and experiences.&nbsp; “It’s impossible to open a magazine without seeing hype about analytics changing every aspect of your life” says Will Dubyak VP Analytics for Product Development &amp; Innovation USAA. “The Insurance AI &amp; Analytics USA Summit is the optimal place to cut through the noise hear the latest thinking from industry leaders in analytics and compare best practices with your colleagues”Across three in-depth tracks more than 40 expert speakers from leading North American carriers will explore and discuss the latest strategies and approaches being deployed to maximize the impact of AI machine learning and advanced analytics across the insurance value chain. Featuring a whole session dedicated to case studies the practical retelling of success stories will ensure attendees discover how and where technological innovations are having the biggest impacts on insurance and walk away with a holistic roadmap for success. Confirmed speakers so far include Tilia Tanner Global Head of Analytics AIG Eugene Wen VP of Group Advanced Analytics Manulife and Jerry Gupta SVP Digital Analyst Catalyst SwissRe as well as:&nbsp; &nbsp; &nbsp; &nbsp;Thomas Sheffield SVP and Head of Specialty Claims QBE&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Glenn Fung Chief Research Scientist AI and Machine Learning Research Director American Family Insurance &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;Laurie Pierman Vice President Claim Operations Amerisure Insurance&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Michiko Kurahashi Chief Marketing Officer AXIS Capital        Attendees to Insurance AI and Analytics USA will also become part of a truly international insurance community with over 25 hours of networking and interactive discussions aplenty. In addition our ‘Open Design Workshops’ will see attendees attempt to live-solve industry challenges giving insight into how peers and competitors alike approach a challenge and how their own methods might be improved.“At QBE we’ve spent a great deal of time figuring out how we can strategically deploy artificial intelligence in practical use cases to drive immediate value for business” states Ted Stuckey Managing Director QBE Ventures. “We’re excited to share some of our experience at the Insurance AI and Analytics Conference in Chicago on May 2-3!”In short however you are seeking to leverage AI Insurance AI and Analytics USA is the event for you. Don’t miss this unparalleled opportunity. Join us in making 2019 the year AI insurance changes forever. Ira Sopic
2019,1,15,About Dr. Zhou's Oracle SQL for Data Science Course,https://www.deep-data-mining.com/2018/07/about-dr-zhous-oracle-sql-for-data.html,"pre{border: 2px solid #666; padding: 10px; background-color: lightgray;} pre.code {border: 2px solid #666; padding: 10px; background-color: lightyellow;} pre.out {border: 2px solid #666; padding: 10px; background-color: lightgray;} div.code {color:blue} On January 31 2017 I was invited by Prof. Wei Ding at the Department of Computer Science University of Massachusetts Boston and gave 3 talks about my data science projects across different industries. These talks are extremely well received. The following is what Prof. Ding says about my talks. ""It was a fortune to have Jay come to our computer science department to share his experience in solving business problems with predictive analytics on February 28 2017. What Jay had presented in his 3 talks each lasting for 1 hour in different topics of data mining was totally impressive and beyond our wildest expectation. Having built competition-winning predictive models for some of the biggest companies and produced hundreds of millions of dollars’ savings Jay shared the secret of his success with students and faculty without reservation. His strong presentations were such an inspiration for our computer science students and faculty and his methodology was innovative and powerful even for very seasoned data scientists among the audience. Jay thank you so much for your hard work preparing and delivering these presentations!"" -Prof. Ding Wei Department of Computer Science University of Massachusetts Boston    The audience are particularly amazed by how I come up with solutions using Oracle SQL environment. To share my expertise I create the online course Oracle SQL for Data Science to show how to perform common data science tasks using Oracle SQL and the benefits for doing that. I let Charlie BergerSenior Director of Product Management Machine Learning AI and Cognitive Analytics at Oracle know about my course and he told me ""Your course is amazing.""document.getElementById(""thinkific-product-embed"") ||  document.write('');Enroll the Course!"
2019,1,15,Deep Learning World,https://www.deep-data-mining.com/2019/01/predictive-analytics-world-for-industry.html,The premier conference covering thecommercial deployment of deep learningDeepLearning World&nbsp;is the premier conference covering the commercial deployment of deep learning. The event’s mission is to foster breakthroughs in the value-driven operationalization of established deep learning methods. DLW runs parallel to the established&nbsp;PredictiveAnalytics World for Industry 4.0&nbsp;at the same venue. Combo passes are available.How to turn Deep Tech into Broad ApplicationThe hype is over: deep learning enters the “trough of disillusionment”. Companies are realizing that not every business problem requires the deep learning hammer. Of course there are use cases that are best solved with artificial neural networks: image speech and text recognition; anomaly detection and predictive maintenance on sensor data; complex data synthesis and sampling; reinforcement and sparse learning; and many more applications show the potential of artificial intelligence for real-world business scenarios. At the Deep Learning World conference data science experts present projects that went beyond experimentation and prototyping and showcase solutions that created economic value for the company. The case study sessions will focus on how it worked and what didn’t work while the deep dive sessions will explain topics such as RNN CNN LSTM transfer learning and further in analytical and technical detail. Meet the European deep learning community in May in Munich and learn from well-known industry leaders!Deep-data-mining.com blog readers receive 15% discount with code: DDMPAWDLW
2019,1,15,Predictive Analytics World for Industry 4.0,https://www.deep-data-mining.com/2019/01/predictiveanalytics-world-for-industry-4.html,6-7 May 2019 – MunichPredictive Analytics World is the leading vendor independent conference for applied machine learning for industry&nbsp;4.0.Business users decision makers and experts in predictive analytics will meet on 6-7 May 2019 in Munich to discover and discuss the&nbsp;latest trends and technologies in machine &amp; deep learning for the era of Internet of Things and artificial intelligence.Putting Machine Intelligence into ProductionSmart Factory Smart Supply Chain Smart Grid Smart Transport: artificial intelligence promises an intelligent and fully automated future but reality is: most machines most vehicles and most grids lack sensors and even where sensors do exist they might not be connected to the Internet of Things. Many companies invested in their infrastructure and are experimenting with prototypes e.g. for predictive maintenance dynamic replenishment route optimization and more but even if they succeeded in delivering a proof of concept they face the challenge to deploy their predictive model into production and scale their analytics solution to company wide adoption. The issues are not merely analytical but a combination of technical organisational judicial and economic details. At the Predictive Analytics World for Industry 4.0 experienced data scientists and business decision makers from a wide variety of industries will meet for two days to demonstrate and to discuss dozens of real-world case studies from well-known industry leaders. In addition predictive analytics experts will explore new methods and tools in special deep dive sessions in detail. Finally the Predictive Analytics World is accompanied by the Deep Learning World conference which focuses on the industry and business application of neural networks. Take the chance learn from the experts and meet your industry peers in Munich in May!&nbsp;Hot topics on the 2019 Agenda:Predictive Maintenance &amp;      LogisticsAnomaly Detection &amp; Root Cause      AnalysisFault Prediction &amp; Failure      DetectionRisk Management &amp; PreventionRoute &amp; Stock OptimizationIndustry &amp; Supply Chain AnalyticsImage &amp; Video RecognitionInternet of Things &amp; Smart      DevicesStream Mining &amp; Edge AnalyticsMachine ~ Ensemble ~ &amp; Deep      LearningProcess Mining &amp; Network AnalysesMining Open &amp; Earth Observation      DataEdge Analytics &amp; Federated      Learning… and many more related topicsPredictiveAnalytics World 4.0 will be co-located with&nbsp;Deep Learning World the premier conference covering the commercial deployment of deep learning in 2019. Deep-data-mining.com blog readers receive 15% discount with code:&nbsp;DDMPAWDLW
2019,1,8,Analytics & AI in Travel North America,https://www.deep-data-mining.com/2019/01/analytics-ai-in-travel-north-america.html,pre{border: 2px solid #666; padding: 10px; background-color: lightgray;} pre.code {border: 2px solid #666; padding: 10px; background-color: lightyellow;} pre.out {border: 2px solid #666; padding: 10px; background-color: lightgray;} div.code {color:blue} Analytics & AI in Travel North America launched by EyeForTravel  will take place on March 14-15 at the Hilton Parc 55 Hotel San Francisco USA. There will be over 350 senior data analytics pricing product development and digital marketing experts from the world’s leading travel companies the event will explore the strategies for brands to address the biggest opportunity right now – how to conquer hyper-personalization.  Confirmed speakers include Hilton’s SVP of Analytics Google’s head of AI global product partnerships Expedia’s Head of Platform – Loyalty Wyndham Hotel Group’s Vice President of Global Revenue Management Operations and Sales Carlson Wagonlit’s Principal Data Scientist and many more. Attendees can expect to explore insights into the following: • Harnessing AI and Data to Transform your Loyalty Strategy: Discover how weaving AI into your business capturing preference data and delivering a truly personalized service will give you the edge in winning loyal customers from your competition• Overcoming Pricing Peril with Personalized & Real-Time Revenue Generation Tactics: Make the shift to real-time pricing on an individual level Nail down the use-cases of how to overcome this forecast like a pro and optimize direct revenue• Getting Up Close and Personal with the Customer and Capitalize on Every Channel: Use AI to fuel CRM and CS to bring customer data to life at every touchpoint use the rich and famous on social to avoid brand erosion and secure market share.• Immersing Yourself in an AI-driven Predictive Future to Seize New Profits: It all comes down to being predictive if you want to turn new profits. Deliver AI-led futures in your company for more efficient internal mechanics and travel customer-centricity• Driving Real-Time Hyper-Personalization to Move Your Profit Needle: Delve into new levels of granularity become the Amazon of travel and deliver the perfect travel itinerary every time for unstoppable loyalty• Seizing Voice AR and VR Makes You Grab that Conversion: Be part of the lucky few that benefits from voice enabled search drive direct bookings and use AR and VR to give your customer the confidence to convert• Dominating Direct Bookings Through A Mastery of Mobile: Create an AI-enabled mobile product that drives direct bookings focus on UI and UX that screams out loyalty and bolster your bottom line• Outclassing your Competition with Total RM and Surge Ancillary Sales: Build state-of-the-art infrastructure that supports ancillary revenue and squeeze every ounce of profit from all revenue streams  Please check out the following icon for more information.  
2018,12,20,Could Not Connect to Amazon RDS Oracle Database From Car Dealer WiFi,https://www.deep-data-mining.com/2018/12/could-not-connect-to-amazon-rds-oracle.html,"pre{border: 2px solid #666; padding: 10px; background-color: lightgray;} pre.code {border: 2px solid #666; padding: 10px; background-color: lightyellow;} pre.out {border: 2px solid #666; padding: 10px; background-color: lightgray;} div.code {color:blue} I am trying to connect to my Oracle database on Amazon RDS at a car dealer while my car is in service. My laptop is connecting to the public WiFi. When I try to connect to the Oracle server I got ""Error Message = IO Error: The Network Adapter could not establish the connection"".I realized the issue is caused by the new ip address not included Amazon Security Group inbound rules. I find my ip address. Then I log onto Amazon AWS console and find the security group associated with the DB instance. After I add a inbound rule 64.188.5.xxx/32 I am able to connect to the DB immediately. "
2018,12,20,Statistically Manufactured Personal Data,https://www.deep-data-mining.com/2018/12/statistically-manufactured-personal-data.html,pre{border: 2px solid #666; padding: 10px; background-color: lightgray;} pre.code {border: 2px solid #666; padding: 10px; background-color: lightyellow;} pre.out {border: 2px solid #666; padding: 10px; background-color: lightgray;} div.code {color:blue} To avoid the trouble of dealing with personal data when we test our analytics processes I have created mock personal data that closely reflect American population from statistical point of view. The largest data set has 1 million records with variables including first name last name sex date of birth social security number address phone number and email. The values of these variables are produced to be as realistic as possible to real American population. They represents about 0.33% of population in the United States.  These observations about the data 1 million mock personal data records are very close to the real statistics of the population in USA. 1.The top 4 states that have the most people are: California(138223 persons %13.82) Texas(99217 persons %9.92) Florida(69640 persons %6.96) and  New York(49979 persons %5). These are close to the real distribution of the population in  USA. 2. The female are 51% and the male are 49%. 3. Top 3 last names are Smith(10800 persons %1.08) Williams(8000 persons %.8) and  Jones(6900 persons %.69). 4. Top 3 female first names are Ava(4707 persons %.93) Olivia(4508 persons %.89) and Isabella(4311 persons %.85) and top 3 male first names are Noah(5075 persons %1.03) Elijah(4736 persons %.96) and Liam(4434 persons %.9). 5. The following table shows distributions of persons by age for both sexes. Women live longer than men.                         Female           MaleAge Group        #        %       #  %    .Under 5 years 34603 6.81% 35656 7.25%   .5 to 9 years 34707 6.83% 34010 6.92%   .10 to 14 years 30192 5.94% 33013 6.72%   .15 to 19 years 34361 6.76% 32689 6.65%   .20 to 24 years 32512 6.39% 36647 7.45%   .25 to 29 years 35626 7.01% 37278 7.58%   .30 to 34 years 34344 6.76% 31977 6.50%   .35 to 39 years 33325 6.55% 31927 6.49%   .40 to 44 years 33332 6.56% 34456 7.01%   .45 to 49 years 35070 6.90% 35443 7.21%   .50 to 54 years 37321 7.34% 34876 7.09%   .55 to 59 years 31623 6.22% 31315 6.37%   .60 to 64 years 28801 5.67% 24218 4.93%   .65 to 69 years 20999 4.13% 19881 4.04%   .70 to 74 years 16617 3.27% 14065 2.86%   .75 to 79 years 13520 2.66% 10272 2.09%   .80 to 84 years 10693 2.10% 7983 1.62%   .85 years and over 10754 2.12% 5894 1.20%You may download a small file with 100 records free here. Free Download. Files with 5k 50K 250K and 1 million records are available for purchase at https://www.datamanufacturer.com.                File Name   Description   Price   Buy       dm_mock_person_100.csv   100 mock personal data records. CSV format.            free   Free Download       dm_mock_person_5k.csv   5K mock personal data records. About 0.7M bytes. CSV format.            $2.95   &nbsp;       dm_mock_person_50k.csv   50K mock personal data records. About 7M bytes. CSV format.            $7.95   &nbsp;       dm_mock_person_250k.csv   250K mock personal data records. About 35M bytes. CSV format.            $9.95   &nbsp;       dm_mock_person_1m.csv   1 million mock personal data records. About 140M bytes. CSV format.            $39.95   &nbsp;    
2018,12,19,Generate Random String in Oracle,https://www.deep-data-mining.com/2018/12/generate-random-string-in-oracle.html,pre{border: 2px solid #666; padding: 10px; background-color: lightgray;} pre.code {border: 2px solid #666; padding: 10px; background-color: lightyellow;} pre.out {border: 2px solid #666; padding: 10px; background-color: lightgray;} div.code {color:blue} The following query generate random email address. SQL> select dbms_random.string('l' 8)||'@'||dbms_random.string('l' 7)||'.com'         email from dual;EMAIL------------------------------------------------------------------------------------irslsxrf@wltikyv.comThe first parameter 'l' means string will be created in lower cases. 
2018,12,18,Find the Most Frequent Values,https://www.deep-data-mining.com/2018/12/find-most-frequent-values.html,pre{border: 2px solid #666; padding: 10px; background-color: lightgray;} pre.code {border: 2px solid #666; padding: 10px; background-color: lightyellow;} pre.out {border: 2px solid #666; padding: 10px; background-color: lightgray;} div.code {color:blue}  To find the most frequent values we can use STATS_MODE function. The following query shows areacode in state Missouri. SQL> select areacode from T_PHONE_AREA where state='Missouri' order by 1;  AREACODE----------       314       314       314       314       314       314       314       314       314       314       314       314       417       417       573       573       573       636       636       636       636       636       636       660       816       816       816       816       816       816       816       816       816       81634 rows selected.In the following query stats_mode(areacode) returns the areacode 314 that is the most frequent value. SQL> select stats_mode(areacode) from T_PHONE_AREA where state='Missouri';STATS_MODE(AREACODE)--------------------                 314
2018,12,17,Remove the Last Word From a String Using Oracle SQL,https://www.deep-data-mining.com/2018/12/remove-last-word-from-string-using.html,pre{border: 2px solid #666; padding: 10px; background-color: lightgray;} pre.code {border: 2px solid #666; padding: 10px; background-color: lightyellow;} pre.out {border: 2px solid #666; padding: 10px; background-color: lightgray;} div.code {color:blue} I use the following query to remove the last word of a sentence. with tbl as (select 'Boston city' as name  from dual)select  name substr(name 1 instr(name' '-1)-1 ) simple_name  from tbl;NAME        SIMPLE_NAME----------- -----------Boston city Boston     
2018,12,17,Find Out Table Columns That Are Indexed,https://www.deep-data-mining.com/2018/12/find-out-table-columns-that-are-indexed.html,pre{border: 2px solid #666; padding: 10px; background-color: lightgray;} pre.code {border: 2px solid #666; padding: 10px; background-color: lightyellow;} pre.out {border: 2px solid #666; padding: 10px; background-color: lightgray;} div.code {color:blue} I use the following query to find out columns that are indexed for a table. select index_name column_name from USER_IND_COLUMNS where table_name='MYTABLE'
2018,12,17,Oracle Function Based Index is Handy,https://www.deep-data-mining.com/2018/12/oracle-function-based-index-is-handy.html,pre{border: 2px solid #666; padding: 10px; background-color: lightgray;} pre.code {border: 2px solid #666; padding: 10px; background-color: lightyellow;} pre.out {border: 2px solid #666; padding: 10px; background-color: lightgray;} div.code {color:blue} In table t_my_table column name is in lower case. However I want to join this table with another table where names are in upper cases. I create a function based index. create index t_my_tablei on t_my_table(upper(name));That way I don't to create another column or table that contains upper(name) and create index on it. When I join the two tables based on upper(a.name) = b.name function based index upper(a.name) is used and it is fast. select a.* b.* from t_my_table a my_another_table b where upper(a.name) = b.name;
2018,12,16,Amazon RDS Oracle Instance Running Out of Disc Space,https://www.deep-data-mining.com/2018/12/amazon-rds-oracle-instance-running-out.html,pre{border: 2px solid #666; padding: 10px; background-color: lightgray;} pre.code {border: 2px solid #666; padding: 10px; background-color: lightyellow;} pre.out {border: 2px solid #666; padding: 10px; background-color: lightgray;} div.code {color:blue} My Oracle database instance on Amazon RDS runs out of disc space. I add more of them by modifying the instance and add extra disc space.  This is the link to instructions.
2018,12,15,Roulette Wheel Selection Using SQL,https://www.deep-data-mining.com/2018/12/roulette-wheel-selection-using-sql.html,"Roulette wheel selection is a very useful algorithm found in many applications such as Genetic Algorithm(GA). In GA solutions with higher fitness values are given larger probabilities of being selected to produce children just like natural evolution. I implemented an Oracle SQL version of the Roulette wheel selection algorithm. The first step is to calculate for each record the cumulative value for the variable that the selection will be based on such as fitness function probability or other. I used sum() over(order by) analytics function. Make sure the ""order by"" is using a unique key so that the cumulative value is also unique.  create table tbl as select id num sum(num) over(order by id) as cum_count  from t_mydata; The following is the roulette wheel selection scripts.  create table t_rw(sel number);declare  mx number;  rnd  number;  x number;begin   select max(cum_count) into mx from tbl;  for i in 1..10000 loop     execute immediate 'select ora_hash(:1:2) from dual '          into rnd using i mx;     select min(cum_count) into x from tbl where cum_count >= rnd;     insert into t_rw(sel) values(x);     end loop;end;create view v_selected as select a.* from tbl a t_rw b where a.cum_count=b.sel;In the above scripts ora_hash() generates a uniformly distributed random number between 0 and maximum cum_count. The selected cum_count is inserted into t_rw. The final result is the view v_selected which is based on the inner join of table tbl and t_rw."
2018,12,4,Generate Serial Number for Existing Table,https://www.deep-data-mining.com/2018/12/generate-serial-number-for-existing.html,pre{border: 2px solid #666; padding: 10px; background-color: lightgray;} pre.code {border: 2px solid #666; padding: 10px; background-color: lightyellow;} pre.out {border: 2px solid #666; padding: 10px; background-color: lightgray;} div.code {color:blue} Table T_REVIEW has column id as the unique key and dt as the timestamp. I want to add a serial number to the table based on the order by dt and id. In the following scripts I use window function row_number to generate the serial number and update the table. alter table t_review add(seq number);update t_review a set seq= (with tbl as (select id row_number() over(order by dt id) rnk from t_review)select rnk from tbl b where b.id=a.id);
2018,12,4,Incrementally Add New Records to Table,https://www.deep-data-mining.com/2018/12/incrementally-add-new-records-to-table.html,pre{border: 2px solid #666; padding: 10px; background-color: lightgray;} pre.code {border: 2px solid #666; padding: 10px; background-color: lightyellow;} pre.out {border: 2px solid #666; padding: 10px; background-color: lightgray;} div.code {color:blue} I have a table t_review that stores historical records including key sid and timestamp dt. Every day more records come into table t_new. I use the following scripts to add those new records identified by sid in t_new to t_review. begininsert into t_review(sid dt) select sid sysdate from  (select sid from t_new minus select sid from t_review);commit;end;
2021,6,15,Accelerating model velocity through Snowflake Java UDF integration,https://blog.dominodatalab.com/accelerating-model-velocity-through-snowflake-java-udf-integration/,"Integrating Domino and Snowflake and using in-database machine learning / data processing techniques via user defined functions (UDF).
The post Accelerating model velocity through Snowflake Java UDF integration appeared first on Data Science Blog by Domino."
2021,6,8,Fitting Support Vector Machines via Quadratic Programming,https://blog.dominodatalab.com/fitting-support-vector-machines-quadratic-programming/,"A deep dive inside Support Vector Machines by deriving a Linear SVM classifier explain its advantages and show the fitting process.
The post Fitting Support Vector Machines via Quadratic Programming appeared first on Data Science Blog by Domino."
2021,5,20,ML internals: Synthetic Minority Oversampling (SMOTE) Technique,https://blog.dominodatalab.com/smote-oversampling-technique/,"In this article we discuss why fitting models on imbalanced datasets is problematic and how class imbalance is typically addressed. We present the inner workings of the SMOTE algorithm and show a simple &#8220;from scratch&#8221; implementation of SMOTE. We use an artificially constructed imbalance dataset (based on Iris) to generate synthetic observations via our SMOTE [&#8230;]
The post ML internals: Synthetic Minority Oversampling (SMOTE) Technique appeared first on Data Science Blog by Domino."
2021,4,29,The Future of Data Science – Mining GTC 2021 for Trends,https://blog.dominodatalab.com/the-future-of-data-science-mining-gtc-2021-for-trends/,"Deep learning enthusiasts are increasingly putting NVIDIA’s GTC at the top of their gotta-be-there conference list. I enjoyed mining this year’s talks for trends that foreshadow where our industry is headed. Three of them were particularly compelling and inspired a new point of view on transfer learning that I feel is important for analytical practitioners [&#8230;]
The post The Future of Data Science &#8211; Mining GTC 2021 for Trends appeared first on Data Science Blog by Domino."
2021,4,21,"Credit Card Fraud Detection using XGBoost, SMOTE, and threshold moving",https://blog.dominodatalab.com/credit-card-fraud-detection-using-xgboost-smote-and-threshold-moving/,"In this article we&#8217;ll discuss the challenge organizations face around fraud detection how machine learning can be used to identify and spot anomalies that the human eye might not catch. We&#8217;ll use a gradient boosting technique via XGBoost to create a model and I&#8217;ll walk you through steps you can take to avoid overfitting and [&#8230;]
The post Credit Card Fraud Detection using XGBoost SMOTE and threshold moving appeared first on Data Science Blog by Domino."
2021,4,6,Ray for Data Science: Distributed Python tasks at scale,https://blog.dominodatalab.com/ray-for-data-science-distributed-python-tasks-at-scale/,"In this article Dr Dean Wampler provides an overview of Ray including raising the question of why we need it.  The article covers practical techniques and some walk through code to help users get started.
The post Ray for Data Science: Distributed Python tasks at scale appeared first on Data Science Blog by Domino."
2021,3,29,On-Demand Spark clusters with GPU acceleration,https://blog.dominodatalab.com/on-demand-spark-clusters-with-gpu-acceleration/,"Apache Spark has become the de facto standard for processing large amounts of stationary and streaming data in a distributed fashion. The addition of the MLlib library consisting of common learning algorithms and utilities opened up Spark for a wide range of machine learning tasks and paved the way for running complex machine learning workflows [&#8230;]
The post On-Demand Spark clusters with GPU acceleration appeared first on Data Science Blog by Domino."
2021,3,16,Trending Toward Concept Building – A Review of Model Interpretability for Deep Neural Networks,https://blog.dominodatalab.com/trending-toward-concept-building-a-review-of-model-interpretability-for-deep-neural-networks/,"We are at an interesting time in our industry when it comes to validating models &#8211; a crossroads of sorts when you think about it. There is an opportunity for practitioners and leaders to make a real difference by championing proper model validation. That work has to include interpretability and explainability techniques. Explaining how deep [&#8230;]
The post Trending Toward Concept Building &#8211; A Review of Model Interpretability for Deep Neural Networks appeared first on Data Science Blog by Domino."
2021,3,11,Choosing the right Machine Learning Framework,https://blog.dominodatalab.com/choosing-the-right-machine-learning-framework/,"Discover key considerations for selecting the right machine learning framework for your project and learn about 4 popular ML frameworks.
The post Choosing the right Machine Learning Framework appeared first on Data Science Blog by Domino."
2021,3,5,Fireside Chat: Stig Pedersen from Topdanmark,https://blog.dominodatalab.com/fireside-chat-stig-pedersen-from-topdanmark/,"Stig Pedersen joins David Bloch to discuss scaling data science inside the modern enterprise based on his experience leading the ML function at Topdanmark
The post Fireside Chat: Stig Pedersen from Topdanmark appeared first on Data Science Blog by Domino."
2018,6,20,How mature is your advanced analytics program?,https://datamakesworld.com/2018/06/20/how-mature-is-your-advanced-analytics-program/,Advanced analytics is being deployed in a range of use cases across business units and industries especially as data types and volume increase. One of the top use cases for advanced analytics we see at TDWI is predictive analytics to understand customer or operational behavior. Statistical as well as machine learning models are used to &#8230; Continue reading How mature is your advanced analytics&#160;program?
2018,6,18,5 Best Practices for Building a program to become data-driven,https://datamakesworld.com/2018/06/18/5-best-practices-for-building-a-program-to-become-data-driven/,There are organizational and technology components critical for a business to succeed in becoming data-driven. On the organizational side a key component to succeeding with data and analytics is to create a culture that supports these efforts. Companies that succeed are typically goal-driven transparent empowering and collaborative. They have strong leadership that believes in data &#8230; Continue reading 5 Best Practices for Building a program to become&#160;data-driven
2018,6,12,The Predictive Analytics Conundrum,https://datamakesworld.com/2018/06/12/the-predictive-analytics-conundrum/,&#160; TDWI research indicates that if users stuck to their plans around predictive analytics adoption would be at 75–80% versus the 35–40% we currently see. &#160; Predictive analytics is on the cusp of widespread adoption. Many organizations are excited to make use of the power of predictive analytics (including machine learning) because they understand the &#8230; Continue reading The Predictive Analytics&#160;Conundrum
2018,6,11,You guessed it – women still earn less in IT than men,https://datamakesworld.com/2018/06/11/you-guessed-it-women-still-earn-less-in-it-than-men/,Since joining TDWI about six years ago I have been following and reporting on our salary survey. The report quantifies and interprets the compensation roles responsibilities skills and experience of individual BI and IT professionals. It also provides detailed profiles of the 10 most common BI and data warehousing roles examining age gender education salary &#8230; Continue reading You guessed it &#8211; women still earn less in IT than&#160;men
2018,2,6,3 Best Practices for Becoming More Self-Sufficient with Self-Service Analytics,https://datamakesworld.com/2018/02/06/3-best-practices-for-becoming-more-self-sufficient-with-self-service-analytics/,My colleague Dave Stodder and I recently lead a Webinar in conjunction with our best practices report about becoming a data-driven organization.  Audience questions included several about self-service. In particular attendees were interested in how to make self-service more accessible to managers and leaders in their organization. Let me set the stage for self-service analytics &#8230; Continue reading 3 Best Practices for Becoming More Self-Sufficient with Self-Service&#160;Analytics
2018,1,15,Advanced Analytics: What’s Ahead for 2018,https://datamakesworld.com/2018/01/15/advanced-analytics-whats-ahead-for-2018/,2018 will be the year of the three As:  AI Automation and Advancing analytics skills In 2017 advanced analytics maintained its momentum in the enterprise. Open source technologies such R and Python gained ground.  Other technologies such as machine learning continued to pique interest.  Use of the cloud become more mainstream. TDWI expects these technologies &#8230; Continue reading Advanced Analytics: What’s Ahead for&#160;2018
2018,1,12,Three Organizational Best Practices for Becoming Data-Driven,https://datamakesworld.com/2018/01/12/three-organizational-best-practices-for-becoming-data-driven/,Dave Stodder and I just finished writing our 4Q Best Practices Report on “What it Takes to Be Data-Driven: Technologies and Practices for Becoming a Smarter Organization.”  What struck me in analyzing the data for the report is that although organizations have embraced BI and analytics they still have a journey in front of them &#8230; Continue reading Three Organizational Best Practices for Becoming&#160;Data-Driven
2015,4,2,Does Gender Matter in BI Salaries?,https://datamakesworld.com/2015/04/02/does-gender-matter-in-bi-salaries/,As a female and a feminist who has worked in male-dominated fields for most of my career what immediately caught my eye when reading the most recent TDWI Salary Survey report was the on-going pay disparity between women and men in BI. According to TDWI research men continue to out-earn women in the BI field &#8230; Continue reading Does Gender Matter in BI&#160;Salaries?
2015,2,6,Achieving Analytics Maturity:   3 Tips from the experts,https://datamakesworld.com/2015/02/06/achieving-analytics-maturity-3-tips-from-the-experts/,What does it take to achieve analytics maturity?  Earlier this week Dave Stodder and I hosted a webcast with a panel of vendor experts from Cloudera Microstrategy and Tableau.  These three companies are all sponsors of the Analytics Maturity Model; an analytics assessment tool that measures where your organization stands relative to its peers in &#8230; Continue reading Achieving Analytics Maturity:   3 Tips from the&#160;experts
2015,1,30,Next-Generation Analytics: Four Findings from TDWI’s Latest Best Practices Report,https://datamakesworld.com/2015/01/30/next-generation-analytics-four-findings-from-tdwis-latest-best-practices-report/,I recently completed TDWI’s latest Best Practices Report: Next Generation Analytics and Platforms for Business Success. Although the phrase &#8220;next-generation analytics and platforms&#8221; can evoke images of machine learning big data Hadoop and the Internet of things (IoT) most organizations are somewhere in between the technology vision and today’s reality of BI and dashboards. For &#8230; Continue reading Next-Generation Analytics: Four Findings from TDWI’s Latest Best Practices&#160;Report
2021,6,22,Seasonal functional autoregressive models,https://robjhyndman.com/publications/sfar/,Functional autoregressive models are popular for functional time series analysis but the standard formulation fails to address seasonal behaviour in functional time series data. To overcome this shortcoming we introduce seasonal functional autoregressive time series models. For the model of order one we derive sufficient stationarity conditions and limiting behavior and provide estimation and prediction methods. Some properties of the general order P model are also presented. The merits of these models are demonstrated using simulation studies and via an application to real data.
2021,6,18,Useful extensions for online books,https://robjhyndman.com/hyndsight/fpp-extensions/,"I&rsquo;ve had two recent questions from readers of my online textbook (with George Athanasopoulos) which could be solved using Google Chrome extensions.
 Hi I&rsquo;m an MSc student and am shortly starting my project/dissertation on time series data. I&rsquo;ve started reading Version 3 of your book and improving my R skills but am wondering if there&rsquo;s any way I can read V3 that will allow annotation? Thanks
 For personal annotation of websites the Hypothesis extension is very useful."
2021,6,4,Understanding links between water-quality variables and nitrate concentration in freshwater streams using high-frequency sensor data,https://robjhyndman.com/publications/water-quality-gam/,Real time monitoring using in situ sensors is becoming a common approach for measuring water quality within watersheds. High frequency measurements produce big data sets that present opportunities to conduct new analyses for improved understanding of water quality dynamics and more effective management of rivers and streams. Of primary importance is enhancing knowledge of the relationships between nitrate one of the most reactive forms of inorganic nitrogen in the aquatic environment and other water quality variables.
2021,6,3,STR: Seasonal-Trend decomposition using Regression,https://robjhyndman.com/publications/str/,We propose a new method for decomposing seasonal data: STR (a Seasonal-Trend decomposition using Regression). Unlike other decomposition methods STR allows for multiple seasonal and cyclic components covariates seasonal patterns that may have non-integer periods and seasonality with complex topology. It can be used for time series with any regular time index including hourly daily weekly monthly or quarterly data. It is competitive with existing methods when they exist but tackles many more decomposition problem than other methods allow.
2021,6,3,What is forecasting?,https://robjhyndman.com/hyndsight/assa-video/,
2021,6,1,Fast forecast reconciliation using linear models,https://robjhyndman.com/publications/lhf/,Forecasting hierarchical or grouped time series usually involves two steps: computing base forecasts and reconciling the forecasts. Base forecasts can be computed by popular time series forecasting methods such as Exponential Smoothing (ETS) and Autoregressive Integrated Moving Average (ARIMA) models. The reconciliation step is a linear process that adjusts the base forecasts to ensure they are coherent. However using ETS or ARIMA for base forecasts can be computationally challenging when there are a large number of series to forecast as each model must be numerically optimized for each series.
2021,6,1,Visualizing probability distributions across bivariate cyclic temporal granularities,https://robjhyndman.com/publications/gravitas/,Deconstructing a time index into time granularities can assist in exploration and automated analysis of large temporal data sets. This paper describes classes of time deconstructions using linear and cyclic time granularities. Linear granularities respect the linear progression of time such as hours days weeks and months. Cyclic granularities can be circular such as hour-of-the-day quasi-circular such as day-of-the-month and aperiodic such as public holidays. The hierarchical structure of granularities creates a nested ordering: hour-of-the-day and second-of-the-minute are single-order-up.
2021,5,28,Situational assessment of COVID-19 in Australia,https://robjhyndman.com/publications/covid19b/,
2021,5,17,Monash Time Series Forecasting Archive,https://robjhyndman.com/publications/monash-forecasting-data/,Many businesses and industries nowadays rely on large quantities of time series data making time series forecasting an important research area. Global forecasting models that are trained across sets of time series have shown a huge potential in providing accurate forecasts compared with the traditional univariate forecasting models that work on isolated series. However there are currently no comprehensive time series archives for forecasting that contain datasets of time series from similar sources available for the research community to evaluate the performance of new global forecasting algorithms over a wide variety of datasets.
2021,5,16,Time series cross-validation using fable,https://robjhyndman.com/hyndsight/tscv-fable/,"Time series cross-validation is handled in the fable package using the stretch_tsibble() function to generate the data folds. In this post I will give two examples of how to use it one without covariates and one with covariates.
Quarterly Australian beer production Here is a simple example using quarterly Australian beer production from 1956 Q1 to 2010 Q2. First we create a data object containing many training sets starting with 3 years (12 observations) and adding one quarter at a time until all data are included."
2021,5,11,Quantile forecasting with ensembles and combinations,https://robjhyndman.com/publications/quantile-ensembles/,
2021,5,10,Principles and Algorithms for Forecasting Groups of Time Series: Locality and Globality,https://robjhyndman.com/publications/global-forecasting/,Forecasting of groups of time series (e.g. demand for multiple products offered by a retailer server loads within a data center or the number of completed ride shares in zones within a city) can be approached locally by considering each time series as a separate regression task and fitting a function to each or globally by fitting a single function to all time series in the set. While global methods can outperform local for groups composed of similar time series recent empirical evidence shows surprisingly good performance on heterogeneous groups.
2021,5,6,Forecasting elements that stand the test of time,https://robjhyndman.com/seminars/lokadtv/,"Interview for Lokad TV Lokad is a supply chain software company based in Paris. They have a TV channel on which they discuss supply chain issues.
Recently I was interviewed on Lokad TV discussing my R packages for forecasting."
2021,4,14,Forecasting podcasts,https://robjhyndman.com/hyndsight/podcasts/,"I&rsquo;ve been interviewed for several podcasts over the last year or so. It&rsquo;s always fun to talk about my work and I hope there is enough differences between them to make it interesting for listeners. Here is a full list of them.
(Updated: 30 May 2021)
   Date Podcast Episode     24 May 2021 Data Skeptic Forecasting principles and practice   12 April 2021 Seriously Social Forecasting the future: the science of prediction   6 February 2021 Forecasting Impact Rob Hyndman   19 July 2020 The Curious Quant Forecasting COVID time series and why causality doesnt matter as much as you think‪   27 May 2020 The Random Sample Forecasting the future &amp; the future of forecasting   9 October 2019 Thought Capital Forecasts are always wrong (but we need them anyway)"
2021,4,12,Seriously social podcast,https://robjhyndman.com/seminars/assa-podcast/,Podcast interview for Seriously Social Recently I was interviewed for the podcast &ldquo;Seriously Social&rdquo;. You can listen to the episode here.
2021,3,26,Dimension reduction for outlier detection using DOBIN,https://robjhyndman.com/publications/dobin/,This paper introduces DOBIN a new approach to select a set of basis vectors tailored for outlier detection. DOBIN has a solid mathematical foundation and can be used as a dimension reduction tool for outlier detection tasks. We demonstrate the effectiveness of DOBIN on an extensive data repository by comparing the performance of outlier detection methods using DOBIN and other bases. We further illustrate the utility of DOBIN as an outlier visualization tool.
2021,3,26,Forecasting the old-age dependency ratio to determine a sustainable pension age,https://robjhyndman.com/publications/pensionage/,We forecast the old-age dependency ratio for Australia under various pension age proposals and estimate a pension age scheme that will provide a stable old-age dependency ratio at a specified level. Our approach involves a stochastic population forecasting method based on coherent functional data models for mortality fertility and net migration which we use to simulate the future age-structure of the population. Our results suggest that the Australian pension age should be increased to 68 by 2030 69 by 2036 and 70 by 2050 in order to maintain the old-age dependency ratio at 23% just above the 2018 level.
2021,3,23,Developing good research habits,https://robjhyndman.com/seminars/research_habits/,Presentation for the 2021 honours and masters students Magic button for library access to papers  Drag this Monash proxy link to your bookmarks.  Links  Mendeley Zotero Paperpile Google Scholar Rmarkdown Happy git with R Rmarkdown thesis template
2021,3,2,Forecasting for Social Good,https://robjhyndman.com/publications/fsg/,Forecasting plays a critical role in the development of organisational business strategies. Despite a considerable body of research in the area of forecasting the focus has largely been on the financial and economic outcomes of the forecasting process as opposed to societal benefits. Our motivation in this study is to promote the latter with a view to using the forecasting process to advance social and environmental objectives such as equality social justice and sustainability.
2021,2,15,Manifold learning with approximate nearest neighbours,https://robjhyndman.com/publications/mlann/,Manifold learning algorithms are valuable tools for the analysis of high-dimensional data many of which include a step where nearest neighbors of all observations are found. This can present a computational bottleneck when the number of observations is large or when the observations lie in more general metric spaces such as statistical manifolds which require all pairwise distances between observations to be computed. We resolve this problem by using a broad range of approximate nearest neighbor algorithms within manifold learning algorithms and evaluating their impact on embedding accuracy.
2021,2,13,Assessing mortality inequality in the US: What can be said about the future?,https://robjhyndman.com/publications/us-longevity/,This paper investigates mortality inequality across U.S. states by modelling and forecasting mortality rates via a forecast reconciliation approach. Understanding the heterogeneity in state-level mortality experience is of fundamental importance as it can assist decision-making for policy makers health authorities as well as local communities who are seeking to reduce inequalities and disparities in life expectancy. A key challenge of multi-population mortality modeling is high dimensionality and the resulting complex dependence structures across sub-populations.
2021,2,6,Forecasting impact podcast,https://robjhyndman.com/seminars/iif-podcast/,Podcast interview for Forecasting Impact Recently I was interviewed for the IIF podcast &ldquo;Forecasting Impact&rdquo;. You can listen to the episode here.
2021,2,6,Leave-one-out kernel density estimates for outlier detection,https://robjhyndman.com/publications/lookout/,This paper introduces lookout a new approach to detect outliers using leave-one-out kernel density estimates and extreme value theory. Outlier detection methods that use kernel density estimates generally employ a user defined parameter to determine the bandwidth. Lookout uses persistent homology to construct a bandwidth suitable for outlier detection without any user input. We demonstrate the effectiveness of lookout on an extensive data repository by comparing its performance with other outlier detection methods based on extreme value theory.
2021,1,20,Forecast reconciliation: A geometric view with new insights on bias correction,https://robjhyndman.com/publications/hierarchical-geometry/,A geometric interpretation is developed for so-called reconciliation methodologies used to forecast time series that adhere to known linear constraints. In particular a general framework is established that nests many existing popular reconciliation methods within the class of projections. This interpretation facilitates the derivation of novel theoretical results. First reconciliation via projection is guaranteed to improve forecast accuracy with respect to a class of loss functions based on a generalised distance metric.
2021,1,15,The road to recovery from COVID-19 for Australian tourism,https://robjhyndman.com/publications/covidtourism/,COVID-19 has had a devastating effect on many industries around the world including tourism and policy makers are interested in mapping out what the recovery path will look like. In this paper we focus on Australian tourism analysing international arrivals and domestic flows. Both sectors have been severely affected by travel restrictions in the form of international and interstate border closures and regional lockdowns. We use statistical models of historical data to generate COVID-free counterfactual forecasts pretending that the pandemic never occurred.
2021,1,1,Anomaly detection in high-dimensional data,https://robjhyndman.com/publications/stray/,The HDoutliers algorithm is a powerful unsupervised algorithm for detecting anomalies in high-dimensional data with a strong theoretical foundation. However it suffers from some limitations that significantly hinder its performance level under certain circumstances. In this article we propose an algorithm that addresses these limitations. We define an anomaly as an observation that deviates markedly from the majority with a large distance gap. An approach based on extreme value theory is used for the anomalous threshold calculation.
2021,1,1,Forecasting Swiss Exports using Bayesian Forecast Reconciliation,https://robjhyndman.com/publications/swiss-exports/,This paper conducts an extensive forecasting study on 13118 time series measuring Swiss goods exports grouped hierarchically by export destination and product category. We apply existing state of the art methods in forecast reconciliation and introduce a novel Bayesian reconciliation framework. This approach allows for explicit estimation of reconciliation biases leading to several innovations: Prior judgment can be used to assign weights to specific forecasts and the occurrence of negative reconciled forecasts can be ruled out.
2021,1,1,Hierarchical Probabilistic Forecasting of Electricity Demand with Smart Meter Data,https://robjhyndman.com/publications/hpf-electricity/,Decisions regarding the supply of electricity across a power grid must take into consideration the inherent uncertainty in demand. Optimal decision-making requires probabilistic forecasts for demand in a hierarchy with various levels of aggregation such as substations cities and regions. The forecasts should be coherent in the sense that the forecast of the aggregated series should equal the sum of the forecasts of the corresponding disaggregated series. Coherency is essential since the allocation of electricity at one level of the hierarchy relies on the appropriate amount being provided from the previous level.
2021,1,1,Nonlinear mixed effects models for time series forecasting of smart meter demand,https://robjhyndman.com/publications/nlme-smart-meters/,Buildings are typically equipped with smart meters to measure electricity demand at regular intervals. Smart meter data for a single building have many uses such as forecasting and assessing overall building performance. However when data are available from multiple buildings there are additional applications that are rarely explored. For instance we can explore how different building characteristics influence energy demand. If each building is treated as a random effect and building characteristics are handled as fixed effects a mixed effects model can be used to estimate how characteristics affect energy usage.
2020,11,25,ASSA New Fellows Presentations,https://robjhyndman.com/seminars/fassa/,Presentation given to the Academy of the Social Sciences in Australia My talk starts at 42:41
2020,11,3,Self-promotion for researchers,https://robjhyndman.com/seminars/self-promotion/,Talk given at the 2020 ACEMS retreat I discuss maintaining a public profile and using social media to your advantage. I cover personal websites online repositories such as arXiv Google Scholar profiles ORCID profiles and using twitter and other social media platforms.
2020,11,2,Modern strategies for time series regression,https://robjhyndman.com/publications/moderntsreg/,This paper discusses several modern approaches to regression analysis involving time series data where some of the predictor variables are also indexed by time. We discuss classical statistical approaches as well as methods that have been proposed recently in the machine learning literature. The approaches are compared and contrasted and it will be seen that there are advantages and disadvantages to most currently available approaches. There is ample room for methodological developments in this area.
2020,10,28,COVID-19 impacts on our energy system,https://robjhyndman.com/seminars/covid19-energy/,Presentation given for the Energy Research Institutes Council of Australia My talk starts at 19:00.
2020,10,27,Ten years of forecast reconciliation,https://robjhyndman.com/seminars/reconciliation_review_talk/,"Keynote talk given at International Symposium on Forecasting 2020 It is common to forecast at different levels of aggregation. For example a retail company will want national forecasts state forecasts and store-level forecasts. And they will want them for all products for groups of products and for individual products.
Ten years ago anyone doing such forecasts needed to select between bottom-up top-down or middle-out methods. Then optimal forecast reconciliation was introduced and a new and better approach was available."
2020,10,26,Call for papers: Innovations in hierarchical forecasting,https://robjhyndman.com/hyndsight/ijf-hierarchical/,"There is a new call for papers for a special issue of the International Journal of Forecasting on &ldquo;Innovations in hierarchical forecasting&rdquo;.
Guest editors: George Athanasopoulos Rob J Hyndman Anastasios Panagiotelis and Nikolaos Kourentzes.
Submission deadline: 31 August 2021."
2020,10,21,Model selection in reconciling hierarchical time series,https://robjhyndman.com/publications/chfr/,Model selection has been proven an effective strategy for improving accuracy in time series forecasting applications. However when dealing with hierarchical time series apart from selecting the most appropriate forecasting model forecasters have also to select a suitable method for reconciling the base forecasts produced for each series to make sure they are coherent. Although some hierarchical forecasting methods like minimum trace are strongly supported both theoretically and empirically for reconciling the base forecasts there are still circumstances under which they might not produce the most accurate results being outperformed by other methods.
2020,10,20,Co-authorships for sale,https://robjhyndman.com/hyndsight/coauthorships-for-sale/,"This is an interesting development! How many papers are published by bogus authors and what is the going price for a coauthorship? Needless to say this is appalling and contrary to every academic integrity policy I&rsquo;ve seen. See the Monash authorship policy for example.
 Dear Hyndman Rob J.
Hope you are doing well.
I write this letter on behalf of authors seeking to co-publish. We have seen your previous works (https://www."
2020,8,28,The geometry of forecast reconciliation,https://robjhyndman.com/seminars/geometry-reconciliation/,"Talk given at Macquarie University Sydney. It is common to forecast at different levels of aggregation. For example a retail company will want national forecasts state forecasts and store-level forecasts. And they will want them for all products for groups of products and for individual products. Forecast reconciliation methods allow for the forecasts at all levels of aggregation to be adjusted so they are consistent with each other.
I will describe a geometric interpretation for reconciliation methods used to forecast time series that adhere to known linear constraints."
2020,8,14,Ensemble forecasts using fable,https://robjhyndman.com/seminars/nyrc2020/,Talk given at the 2020 R conference New York. For over 50 years we have known that ensemble forecasts perform better than individual methods yet they are not as widely used as they should be. Perhaps this is because users think it is more work or that it is hard to get prediction intervals or that it is difficult to determine the relative weights of the component methods. The fable package solves these problems and makes it easy to produce probabilistic forecasts using ensembles across many time series.
2020,8,4,"Probabilistic forecast reconciliation: properties, evaluation and score optimisation",https://robjhyndman.com/publications/coherentprob/,We develop a framework for prediction of multivariate data that follow some known linear constraints such as the example where some variables are aggregates of others. This is particularly common when forecasting time series (predicting the future) but also arises in other types of prediction. For point prediction an increasingly popular technique is reconciliation whereby predictions are made for all series (so-called &ldquo;base&rdquo; predictions) and subsequently adjusted to ensure coherence with the constraints.
2020,7,24,Contraceptive forecasting competition,https://robjhyndman.com/hyndsight/contraceptive-forecasting/,"Here&rsquo;s an interesting new forecasting competition that came via my inbox this week.
 Contraceptive access is vital to safe motherhood healthy families and prosperous communities. Greater access to contraceptives enables couples and individuals to determine whether when and how often to have children. In low- and middle-income countries (LMIC) around the world health systems are often unable to accurately predict the quantity of contraceptives necessary for each health service delivery site in part due to insufficient data limited staff capacity and inadequate systems."
2020,7,21,Distributed ARIMA Models for Ultra-long Time Series,https://robjhyndman.com/publications/darima/,Providing forecasts for ultra-long time series plays a vital role in various activities such as investment decisions industrial production arrangements and farm management. This paper develops a novel distributed forecasting framework to tackle challenges associated with forecasting ultra-long time series by utilizing the industry-standard MapReduce framework. The proposed model combination approach facilitates distributed time series forecasting by combining the local estimators of ARIMA (AutoRegressive Integrated Moving Average) models delivered from worker nodes and minimizing a global loss function.
2020,7,19,Podcast episode: the curious quant,https://robjhyndman.com/seminars/curious-quant-podcast/,Podcast episode for The Curious Quant Last week I had a chat with Michael Kollo for the Curious Quant podcast.
2020,7,17,Estimating temporal variation in transmission of SARS-CoV-2 and physical distancing behaviour in Australia,https://robjhyndman.com/publications/covid19/,
2020,7,14,Spatial modelling of the two-party preferred vote in Australian federal elections: 2001-2016,https://robjhyndman.com/publications/elections/,We examine the relationships between electoral socio-demographic characteristics and two-party preference in the six Australian federal elections held between 2001 to 2016. Socio-demographic information is derived from the Australian Census which occurs every five years. Since a Census is not directly available for each election spatio-temporal imputation is employed to estimate Census data for the electorates at the time of each election. This accounts for both spatial and temporal changes in electoral characteristics between Censuses.
2020,7,11,Early classification of spatio-temporal events using partial information,https://robjhyndman.com/publications/eventstream/,This paper investigates early event classification in spatio-temporal data streams where events need to be classified using partial information i.e. while the event is still ongoing. The framework incorporates two early event classification algorithms with different strengths as well as an event extraction algorithm. We apply this framework to synthetic and real world problems and demonstrate its reliability and broad applicability. The algorithms and data are available in the R package eventstream and other code in the supplementary material.
2020,6,26,Terminology matters,https://robjhyndman.com/hyndsight/terminology-matters/,"I was reminded again this week that getting the right terminology is important. Some of my colleagues who work in machine learning wrote a paper entitled “Time series regression” which began with “This paper introduces Time Series Regression (TSR): a little-studied task …”. Statisticians and econometricians have done time series regression for many decades so this beginning led to the paper being lampooned on Twitter.
The problem arose due to clashes in terminology being used in different fields."
2020,6,3,Hierarchical forecast reconciliation with machine learning,https://robjhyndman.com/publications/hfrml/,Hierarchical forecasting methods have been widely used to support aligned decision-making by providing coherent forecasts at different aggregation levels. Traditional hierarchical forecasting approaches such as the bottom-up and top-down methods focus on a particular aggregation level to anchor the forecasts. During the past decades these have been replaced by a variety of linear combination approaches that exploit information from the complete hierarchy to produce more accurate forecasts. However the performance of these combination methods depends on the particularities of the examined series and their relationships.
2020,5,27,Forecasting the Future & the Future of Forecasting,https://robjhyndman.com/seminars/acems-podcast/,Podcast interview for The Random Sample Recently I was interviewed for the ACEMS podcast &ldquo;The Random Sample&rdquo; on the topic of forecasting. You can listen to the episode here.
2020,5,24,Seasonal mortality rates,https://robjhyndman.com/hyndsight/seasonal-mortality-rates/,"


The weekly mortality data recently published by the Human Mortality Database can be used to explore seasonality in mortality rates. Mortality rates are known to be seasonal due to temperatures and other weather-related effects (Healy 2003)."
2020,5,21,Excess deaths for 2020,https://robjhyndman.com/hyndsight/excess-deaths/,"The reported COVID19 deaths in each country are often undercounts due to different reporting practices or people dying of COVID19 related causes without ever being tested. One way to explore the true mortality effect of the pandemic is to look at “excess deaths” — the difference between death rates this year and the same time in previous years.
The Financial Times (and other media outlets) have been collecting data from many countries to try to measure this effect."
2020,5,17,You are what you vote: the social and demographic factors that influence your vote,https://robjhyndman.com/publications/voting/,"Australia has changed in many ways over the past two decades. Rising house prices country-wide improvements in education an ageing population and a decline in religious affiliation are just some of the ways it has changed. At the same time political power has moved back and forth between the two major parties. How much can we attribute changes in political power to changes in who we are?
Quite a lot as it turns out."
2020,4,30,GRATIS: GeneRAting TIme Series with diverse and controllable characteristics,https://robjhyndman.com/publications/gratis/,The explosion of time series data in recent years has brought a flourish of new time series analysis methods for forecasting clustering classification and other tasks. The evaluation of these new methods requires either collecting or simulating a diverse set of time series benchmarking data to enable reliable comparisons against alternative approaches. We propose GeneRAting TIme Series with diverse and controllable characteristics named GRATIS with the use of mixture autoregressive (MAR) models.
2020,4,19,Anomaly detection in streaming nonstationary temporal data,https://robjhyndman.com/publications/oddstream/,This article proposes a framework that provides early detection of anomalous series within a large collection of non-stationary streaming time series data. We define an anomaly as an observation that is very unlikely given the recent distribution of a given system. The proposed framework first forecasts a boundary for the system&rsquo;s typical behavior using extreme value theory. Then a sliding window is used to test for anomalous series within a newly arrived collection of series.
2020,4,8,Optimal non-negative forecast reconciliation,https://robjhyndman.com/publications/nnmint/,The sum of forecasts of disaggregated time series are often required to equal the forecast of the aggregate giving a set of coherent forecasts. The least squares solution for finding coherent forecasts uses a reconciliation approach known as MinT proposed by Wickramasuriya et al (2019). The MinT approach and its variants do not guarantee that the coherent forecasts are non-negative even when all of the original forecasts are non-negative in nature.
2020,4,5,Why log ratios are useful for tracking COVID-19,https://robjhyndman.com/hyndsight/logratios-covid19/,"There have been some great data visualizations produced of COVID-19 case and deaths data the best known of which is the graph from John Burn-Murdoch in the Financial Times. To my knowledge it was first used by Matt Cowgill from the Grattan Institute and has been widely copied. This is a great visualization and has helped introduce log-scale graphics to a wide audience.
Reproducing the Financial Times cumulative confirmed cases graph To produce something like it we can use the tidycovid19 package from Joachim Gassen:"
2020,3,31,Developing good research habits,https://robjhyndman.com/seminars/research_habits2020/,Presentation for the 2020 honours and masters students Magic button for library access to papers  Drag this Monash proxy link to your bookmarks.  Links  Mendeley Zotero Paperpile Google Scholar Rmarkdown Happy git with R Rmarkdown thesis template
2020,3,22,Forecasting COVID-19,https://robjhyndman.com/hyndsight/forecasting-covid19/,"What makes forecasting hard? Forecasting pandemics is harder than many people think. In my book with George Athanasopoulos we discuss the contributing factors that make forecasts relatively accurate. We identify three major factors:
how well we understand the factors that contribute to it; how much data is available; whether the forecasts can affect the thing we are trying to forecast.  For example tomorrow’s weather can be forecast relatively accurately using modern tools because we have good models of the physical atmosphere there is tons of data and our weather forecasts cannot possibly affect what actually happens."
2020,2,27,On normalization and algorithm selection for unsupervised outlier detection,https://robjhyndman.com/publications/normalization-outliers/,This paper demonstrates that the performance of various outlier detection methods depends sensitively on both the data normalization schemes employed as well as characteristics of the datasets. Recasting the challenge of understanding these dependencies as an algorithm selection problem we perform the first instance space analysis of outlier detection methods. Such analysis enables the strengths and weaknesses of unsupervised outlier detection methods to be visualized and insights gained into which method and normalization scheme should be selected to obtain the most likely best performance for a given dataset.
2020,2,7,Electricity demand data in tsibble format,https://robjhyndman.com/hyndsight/electrictsibbles/,"The tsibbledata packages contains the vic_elec data set containing half-hourly electricity demand for the state of Victoria along with corresponding temperatures from the capital city Melbourne. These data cover the period 2012-2014.
Other similar data sets are also available and these may be of interest to researchers in the area.
For people new to tsibbles please read my introductory post.
 Australian state-level demand The rawdata for other states are also stored in the tsibbledata github repository (under the data-raw folder) but these are not included in the package to satisfy CRAN space constraints."
2020,2,1,"Hospital characteristics, rather than surgical volume, predict length of stay following colorectal cancer surgery",https://robjhyndman.com/publications/hospital-los/,"Objective Length of hospital stay (LOS) is considered a vital component for successful colorectal surgery treatment. Evidence of an association between hospital surgery volume and LOS has been mixed. Data modelling techniques may adversely impact conclusions to the extent of reversing them. This study applied techniques to overcome possible drawbacks.
Method An additive quantile regression model formulated to isolate hospital contextual effects was applied to every colorectal surgery for cancer conducted in Victoria Australia between 2005 and 2015: 28343 admissions in 90 Victorian hospitals."
2020,1,30,How Rmarkdown changed my life,https://robjhyndman.com/seminars/rmarkdown/,Talk given at rstudio::conf San Francisco. Over the last few years Rmarkdown seems to have taken over my life or at least my written communication. These days I use Rmarkdown to maintain my website write my blog write textbooks write academic papers prepare slides for talks keep my CV up-to-date help my students write theses prepare university policy documents write letters prepare exams write reports for clients and more. I haven&rsquo;t quite got to the point of using it for shopping lists but perhaps that&rsquo;s my next Rmarkdown template.
2020,1,27,Tidy time series & forecasting in R,https://robjhyndman.com/seminars/workshop2020/,"knitr::opts_chunk$set(echo = TRUE cache = TRUE) options(digits = 3 width = 75) library(tidyverse) Venue rstudio:conf2020 San Francisco
Course description It is becoming increasingly common for organizations to collect huge amounts of data over time and existing time series analysis tools are not always suitable to handle the scale frequency and structure of the data collected. In this workshop we will look at some new packages and methods that have been developed to handle the analysis of large collections of time series."
2020,1,24,ABS time series as tsibbles,https://robjhyndman.com/hyndsight/abs2tsibble/,"library(tidyverse) library(tsibble) library(readabs) Australian data analysts will know how frustrating it is to work with time series data from the Australian Bureau of Statistics. They are stored as multiple ugly Excel files (each containing multiple sheets) with inconsistent formatting embedded comments meta data stored along with the actual data dates stored in a painful Excel format and so on.
Fortunately there is an R package available to make this a little easier."
2020,1,7,Calendar-based graphics for visualizing people's daily schedules,https://robjhyndman.com/publications/calendar-vis/,Calendars are broadly used in society to display temporal information and events. This paper describes a new calendar display for plotting data that includes a layout algorithm with many options and faceting functionality. The functions use modular arithmetic on the date variable to restructure the data into a calendar format. The user can apply the grammar of graphics to create plots inside each calendar cell and thus the displays synchronize neatly with ggplot2 graphics.
2020,1,6,Hierarchical forecasting,https://robjhyndman.com/publications/hierarchical-forecasting/,Accurate forecasts of macroeconomic variables are crucial inputs into the decisions of economic agents and policy makers. Exploiting inherent aggregation structures of such variables we apply forecast reconciliation methods to generate forecasts that are coherent with the aggregation constraints. We generate both point and probabilistic forecasts for the first time in the macroeconomic setting. Using Australian GDP we show that forecast reconciliation not only returns coherent forecasts but also improves the overall forecast accuracy in both point and probabilistic frameworks.
2020,1,4,A new tidy data structure to support exploration and modeling of temporal data,https://robjhyndman.com/publications/tsibble/,Mining temporal data for information is often inhibited by a multitude of formats: irregular or multiple time intervals point events that need aggregating multiple observational units or repeated measurements on multiple individuals and heterogeneous data types. On the other hand the software supporting time series modeling and forecasting makes strict assumptions on the data to be provided typically requiring a matrix of numeric data with implicit time indexes. Going from raw data to model-ready data is painful.
2020,1,3,FFORMA: Feature-based Forecast Model Averaging,https://robjhyndman.com/publications/fforma/,We propose an automated method for obtaining weighted forecast combinations using time series features. The proposed approach involves two phases. First we use a collection of time series to train a meta-model to assign weights to various possible forecasting methods with the goal of minimizing the average forecasting loss obtained from a weighted forecast combination. The inputs to the meta-model are features extracted from each series. In the second phase we forecast new series using a weighted forecast combination where the weights are obtained from our previously trained meta-model.
2020,1,2,Forecasting in social settings: the state of the art,https://robjhyndman.com/publications/forecasting-sota/,This paper provides a non-systematic review of the progress of forecasting in social settings. It is aimed at someone outside the field of forecasting wanting to appreciate the results of the M4 Competition by reading a survey paper to get informed about the state of the art of this discipline. It discusses the recorded improvements over time in forecast accuracy the need to capture forecast uncertainty and what can go wrong with predictions.
2020,1,1,A brief history of forecasting competitions,https://robjhyndman.com/publications/forecasting-competitions/,Forecasting competitions are now so widespread that it is often forgotten how controversial they were when first held and how influential they have been over the years. I briefly review the history of forecasting competitions and discuss what we have learned about their design and implementation and what they can tell us about forecasting. I also provide a few suggestions for potential future competitions and for research about forecasting based on competitions.
2019,12,8,Machine learning applications in time series hierarchical forecasting,https://robjhyndman.com/publications/mlhts/,Hierarchical forecasting (HF) is needed in many situations in the supply chain (SC) because managers often need different levels of forecasts at different levels of SC to make a decision. Top-Down (TD) Bottom-Up (BU) and Optimal Combination (COM) are common HF models. These approaches are static and often ignore the dynamics of the series while disaggregating them. Consequently they may fail to perform well if the investigated group of time series are subject to large changes such as during the periods of promotional sales.
2019,12,8,Predicting the whole distribution with methods for depth data analysis demonstrated on a colorectal cancer treatment study,https://robjhyndman.com/publications/colorectal/,We demonstrate the utility of predicting the whole distribution of an outcome rather than a marginal change. We overcome inconsistent data modelling techniques in a real world problem. A model based on additive quantile regression and boosting was used to predict the whole distribution of length of hospital stay (LOS) following colorectal cancer surgery. The model also assessed the association of hospital and patient characteristics over the whole distribution of LOS.
2019,10,29,The journal game,https://robjhyndman.com/seminars/journalgame/,Presentation for the department retreat  Monash University authorship policy
2019,10,17,Non-Gaussian forecasting using fable,https://robjhyndman.com/hyndsight/fable2/,"library(tidyverse) library(tsibble) library(lubridate) library(feasts) library(fable) In my previous post about the new fable package we saw how fable can produce forecast distributions not just point forecasts. All my examples used Gaussian (normal) distributions so in this post I want to show how non-Gaussian forecasting can be done.
As an example we will use eating-out expenditure in my home state of Victoria.
vic_cafe &lt;- tsibbledata::aus_retail %&gt;% filter( State == &quot;Victoria&quot; Industry == &quot;Cafes restaurants and catering services&quot; ) %&gt;% select(Month Turnover) vic_cafe %&gt;% autoplot(Turnover) + ggtitle(&quot;Monthly turnover of Victorian cafes&quot;) Forecasting with transformations Clearly the variance is increasing with the level of the series so we will consider modelling a Box-Cox transformation of the data."
2019,10,9,Forecasts are always wrong (but we need them anyway),https://robjhyndman.com/seminars/thoughtcapital/,Podcast interview for Thought Capital Recently I was interviewed for the Monash Business School podcast “Thought Capital” on the topic of forecasting. You can listen to the episode here (or read the transcript).
2019,9,29,Tidy forecasting in R,https://robjhyndman.com/hyndsight/fable/,"The fable package for doing tidy forecasting in R is now on CRAN. Like tsibble and feasts it is also part of the tidyverts family of packages for analysing modelling and forecasting many related time series (stored as tsibbles).
For a brief introduction to tsibbles see this post from last month.
Here we will forecast Australian tourism data by state/region and purpose. This data is stored in the tourism tsibble where Trips contains domestic visitor nights in thousands."
2019,9,27,Feature-based time series analysis,https://robjhyndman.com/seminars/unsw-fbtsa/,Presentation given at UNSW It is becoming increasingly common for organizations to collect very large amounts of data over time. Data visualization is essential for exploring and understanding structures and patterns and to identify unusual observations. However the sheer quantity of data available means that new time series visualisation methods are needed. I will demonstrate an approach to this problem using a vector of features on each time series measuring characteristics of the series.
2019,9,27,Tidy time series analysis in R,https://robjhyndman.com/seminars/tidyverts/,tidyverts R packages
2019,9,26,Tidy time series analysis in R,https://robjhyndman.com/seminars/fable-cardiff/,Presentation at meetup for Cardiff Business School tidyverts R packages
2019,9,17,A feature-based procedure for detecting technical outliers in water-quality data from in situ sensors,https://robjhyndman.com/publications/oddwater/,Outliers due to technical errors in water‐quality data from in situ sensors can reduce data quality and have a direct impact on inference drawn from subsequent data analysis. However outlier detection through manual monitoring is infeasible given the volume and velocity of data the sensors produce. Here we introduce an automated procedure named oddwater that provides early detection of outliers in water‐quality data from in situ sensors caused by technical issues.
2019,9,16,Feature-based time series analysis,https://robjhyndman.com/hyndsight/fbtsa/,"In my last post I showed how the feasts package can be used to produce various time series graphics.
The feasts package also includes functions for computing FEatures And Statistics from Time Series (hence the name). In this post I will give three examples of how these might be used.
library(tidyverse) library(tsibble) library(feasts) Exploring Australian tourism data I used this example in my talk at useR!2019 in Toulouse and it is also the basis of a vignette in the package and a recent blog post by Mitchell O’Hara-Wild."
2019,8,30,Time series graphics using feasts,https://robjhyndman.com/hyndsight/feasts/,"This is the second post on the new tidyverts packages for tidy time series analysis. The previous post is here.
For users migrating from the forecast package it might be useful to see how to get similar graphics to those they are used to. The forecast package is built for ts objects while the feasts package provides features statistics and graphics for tsibbles. (See my first post for a description of tsibbles."
2019,8,29,Tidy time series data using tsibbles,https://robjhyndman.com/hyndsight/tsibbles/,"There is a new suite of packages for tidy time series analysis that integrates easily into the tidyverse way of working. We call these the tidyverts packages and they are available at tidyverts.org. Much of the work on these packages has been done by Earo Wang and Mitchell O’Hara-Wild.
The first of the packages to make it to CRAN was tsibble providing the data infrastructure for tidy temporal data with wrangling tools."
2019,8,23,Predicting sediment and nutrient concentrations from high-frequency water-quality data,https://robjhyndman.com/publications/water-quality/,Water-quality monitoring in rivers often focuses on the concentrations of sediments and nutrients constituents that can smother biota and cause eutrophication. However the physical and economic constraints of manual sampling prohibit data collection at the frequency required to adequately capture the variation in concentrations through time. Here we developed models to predict total suspended solids (TSS) and oxidized nitrogen (NOx) concentrations based on high-frequency time series of turbidity conductivity and river level data from in situ sensors in rivers flowing into the Great Barrier Reef lagoon.
2019,8,21,Forecasting is not prophecy: dealing with high-dimensional probabilistic forecasts in practice,https://robjhyndman.com/seminars/isi_prophecy/,Invited talk for ISI-WSC 2019 in Kuala Lumpur Many large organizations need to forecast huge numbers of related time series every week. Manufacturing companies forecast product demand to plan their supply chains; call centres forecast call volume to inform staff scheduling; technology companies forecast web traffic to maintain service levels; energy companies forecast electricity demand to prevent blackouts. In each case what is required is a high-dimensional probabilistic forecast describing multivariate quantiles of the uncertain future not a vector of point forecasts.
2019,8,17,High-dimensional time series analysis,https://robjhyndman.com/seminars/isi2019workshop/,"Venue Sasana Kijang Bank Negara Malaysia Kuala Lumpur
Presenters  Rob J Hyndman Mitchell O&rsquo;Hara-Wild  Course description It is becoming increasingly common for organizations to collect huge amounts of data over time and existing time series analysis tools are not always suitable to handle the scale and type of data collected. In this workshop we will look at some new methods that have been developed to handle the analysis of large collections of time series."
2019,6,19,A feast of time series tools,https://robjhyndman.com/seminars/isf-feasts/,Presentation at the International Symposium on Forecasting Thessaloniki Greece and at useR!2019 Toulouse France. Modern time series are often high-dimensional and observed at high frequency but most existing R packages for time series are designed to handle low-dimensional and low frequency data such as annual monthly and quarterly data. The feasts package is part of new collection of tidyverts packages designed for modern time series analysis using the tidyverse framework and structures.
2019,6,19,Advancing forecasting research and practice,https://robjhyndman.com/seminars/isf-panel-2019/,Panel discussion at the International Symposium on Forecasting Thessaloniki Greece Topic: Advancing forecasting research and practice: the contribution of the Institute and its journals Speakers:  Robert Fildes. Director Centre for Marketing Analytics and Forecasting Lancaster University Spyros Makridakis. Professor University of Nicosia Gael Martin. Professor of Econometrics Monash University Esther Ruiz. Catedrático de Universidad Universidad Carlos III de Madrid Rob Hyndman. Professor of Statistics Monash University  Brief: The Institute and the two journals dedicated to forecasting (International Journal of Forecasting Journal of Forecasting) were set up starting in 1981 to act as a central focus for research into all the wide variety of methods and forecasting practice.
2019,6,14,Optimal forecast reconciliation for hierarchical and grouped time series through trace minimization,https://robjhyndman.com/publications/mint/,Large collections of time series often have aggregation constraints due to product or geographical groupings. The forecasts for the most disaggregated series are usually required to add-up exactly to the forecasts of the aggregated series a constraint we refer to as &ldquo;coherence&rdquo;. Forecast reconciliation is the process of adjusting forecasts to make them coherent. The reconciliation algorithm proposed by Hyndman et al. (2011) is based on a generalized least squares estimator that requires an estimate of the covariance matrix of the coherency errors (i.
2019,6,8,Quantification of energy savings from energy conservation measures in buildings using machine learning,https://robjhyndman.com/publications/energy-savings/,"This paper demonstrates how machine learning is used to measure energy savings from energy conservation measures (ECMs); in particular ECMs with a low expected saving.
We develop a model that predict energy consumption in buildings on an hourly level. The model is trained on energy data from the main meter before the ECMs took place. The model is then used to predict energy consumption after the ECMs.
The difference between the prediction (the estimated energy consumption in the building given no ECMs) and the actual usage is the estimated savings."
2019,5,26,Poll position: statistics and the Australian federal election,https://robjhyndman.com/hyndsight/poll-position/,"One of the few people in Australia who did not write off a possible Coalition win at the recent federal election was Peter Ellis. We’ve invited him to come and give a talk about making sense of opinion polls and the Australian federal election on Friday this week at Monash University. Visitors are welcome. Here are the details.
11am 31 May 2019. Room G03 Learning and Teaching Building 19 Ancora Imparo Way Clayton Campus Monash University"
2019,5,17,You are what you vote,https://robjhyndman.com/hyndsight/election-conversation/,I’ve tried my hand at writing for the wider public with an article for The Conversation based on my paper with Di Cook and Jeremy Forbes on “Spatial modelling of the two-party preferred vote in Australian federal elections: 2001-2016”. With the next Australian election taking place tomorrow we thought it was timely to put out a publicly accessible version of our analysis.
2019,4,23,"Translations of ""Forecasting: principles and practice""",https://robjhyndman.com/hyndsight/fpptranslations/,"There are now translations of my forecasting textbook (coauthored with George Athanasopoulos) into Chinese and Korean.
The Chinese translation was produced by a team led by Professor Yanfei Kang (Beihang University) and Professor Feng Li (Central University of Finance and Economics). The following students were also involved: Cheng Fan Liu Yu Long Xiaoyu Wang Xiaoqian Zeng Jiayue Zhang Bohan and Zhu Shuaidong.
The Korean translation was produced by Dr Daniel Young Ho Kim."
2019,4,22,Revealing high-frequency trading provision of liquidity with visualization,https://robjhyndman.com/publications/hft-liquidity/,Liquidity is crucial for successful financial markets. It ensures that all investors are able to buy and sell assets quickly at a fair price. High Frequency Traders (HFTs) utilize sophisticated algorithms operating with extreme speed and are frequently cited as liquidity providers. The objective of this paper is to investigate the liquidity provision of a number of HFTs to determine their effects on aggregate marketplace liquidity. We consider a large data set collected from the Australian Securities Exchange throughout 2013 providing a near complete picture of all trading activity.
2019,3,20,Developing good research habits,https://robjhyndman.com/seminars/research_habits2019/,Presentation for the 2019 honours and masters students Magic button for library access to papers  Drag this Monash proxy link to your bookmarks.  Links  Mendeley Zotero Paperpile Google Scholar Rmarkdown Happy git with R Rmarkdown thesis template
2019,3,9,Detection of cybersecurity attacks through analysis of web browsing activities using principal component analysis,https://robjhyndman.com/publications/nids-anomalies/,Organizations such as government departments and financial institutions provide online service facilities accessible via an increasing number of internet connected devices which make their operational environment vulnerable to cyber attacks. Consequently there is a need to have mechanisms in place to detect cyber security attacks in a timely manner. A variety of Network Intrusion Detection Systems (NIDS) have been proposed and can be categorized into signature-based NIDS and anomaly-based NIDS. The signature-based NIDS which identify the misuse through scanning the activity signature against the list of known attack activities are criticized for their inability to identify new attacks (never-before-seen attacks).
2019,3,3,Time Series Data Library,https://robjhyndman.com/tsdl/,The Time Series Data Library is no longer hosted on this website. You can get the data from the tsdl R package.
2019,2,21,Post-docs in wind and solar power forecasting,https://robjhyndman.com/hyndsight/wind-solar-jobs/,"We currently have two postdoc opportunities together with an industry partner in the field of wind and solar power forecasting (full time Level B). They are suitable for recently graduated PhD students that can start between now and June-July.
The opportunities are as follows:
Wind power forecasting:  1 year contract Good programming skills in R and/or Python Solid background in Machine Learning and/or Statistics Background in time series forecasting desirable   Solar power forecasting:  6 months contract Good programming skills in R and/or Python Solid background in Machine Learning and/or Statistics Data will be cloud coverage data from sky cams so some image processing background is necessary Background in time series forecasting desirable  Please contact Christoph Bergmeir if you are interested."
2019,2,6,A framework for automated anomaly detection in high frequency water-quality data from in situ sensors,https://robjhyndman.com/publications/water-quality-2/,River water-quality monitoring is increasingly conducted using automated in situ sensors enabling timelier identification of unexpected values. However anomalies caused by technical issues confound these data while the volume and velocity of data prevent manual detection. We present a framework for automated anomaly detection in high-frequency water-quality data from in situ sensors using turbidity conductivity and river level data. After identifying end-user needs and defining anomalies we ranked their importance and selected suitable detection methods.
2019,1,25,Advice to PhD applicants,https://robjhyndman.com/hyndsight/phdapplicants/,"For students who are interested in doing a PhD at Monash under my supervision.
First check that you satisfy the following criteria:
 You must have completed a degree in statistics that involved some research component (e.g. an honours or masters thesis). A degree in computer science mathematics or econometrics might be acceptable if it contained a substantial amount of statistics. A degree in any other field is not sufficient background to work with me."
2019,1,25,Feature-based forecasting algorithms for large collections of time series,https://robjhyndman.com/seminars/acems-fforma/,"Talk given at ACEMS workshop on &ldquo;Statistical Methods for the Analysis of High-Dimensional and Massive Data Sets&rdquo; I will discuss two algorithms used in forecasting large collections of diverse time series. Each of these algorithms uses a meta-learning approach with vectors of features computed from time series to guide the way the forecasts are computed.
In FFORMS (Feature-based FORecast Model Selection) we use a random forest classifier to identify the best forecasting method using only time series features."
2019,1,18,forecast 8.5,https://robjhyndman.com/hyndsight/forecast85/,"The latest minor release of the forecast package has now been approved on CRAN and should be available in the next day or so.
Version 8.5 contains the following new features
 Updated tsCV() to handle exogenous regressors. Reimplemented naive() snaive() rwf() for substantial speed improvements. Added support for passing arguments to auto.arima() unit root tests. Improved auto.arima() stepwise search algorithm (some neighbouring models were missed previously).  We haven’t done a major release for two years and there is unlikely to be another one now."
2019,1,1,Macroeconomic forecasting for Australia using a large number of predictors,https://robjhyndman.com/publications/ausmacrofcast/,A popular approach to forecasting macroeconomic variables is to utilize a large number of predictors. Several regularization and shrinkage methods can be used to exploit such high-dimensional datasets and have been shown to improve forecast accuracy for the US economy. To assess whether similar results hold for economies with different characteristics an Australian dataset containing observations on 151 aggregate and disaggregate economic series as well as 185 international variables is introduced.
2018,12,22,Network for early career researchers in forecasting,https://robjhyndman.com/hyndsight/iif-ecr-network/,"The International Institute of Forecasters has established interest group sections devoted to specialized domains of forecasting. One of the first such sections will be for early career researchers.
So if you are a PhD student post-doc or otherwise a relatively junior researcher working in forecasting this is for you! The first events will be during the ISF in Thessaloniki in June 2019 including the following:
 ECR welcoming event. A meet and greet event prior to the ISF welcome reception."
2018,12,18,Why doesn't auto.arima() return the model with the lowest AICc value?,https://robjhyndman.com/hyndsight/badarima/,"This question seems to come up frequently on crossvalidated.com or in my inbox.
 I have this time series however it yields different results when I use the auto.arima and Arima functions.
 library(forecast) xd &lt;- ts(c(23786 25955 54373 21561 14552 13284 12714 11821 15445 21307 17228 20007 23065 32811 43147 15127 13497 12224 11412 11888 1421018978 15782 17216 16417 22861 42616 17057 9741 10503 7170 10686 9762 15773 15280 13212 14784 26104 29947) frequency = 12 start=c(20141) end=c(20173)) fit1 &lt;- auto."
2018,12,12,Using ggplot2 for functional time series,https://robjhyndman.com/hyndsight/ftsviz/,"This week I’ve been attending the Functional Data and Beyond workshop at the Matrix centre in Creswick.
I spoke yesterday about using ggplot2 for functional data graphics rather than the custom-built plotting functionality available in the many functional data packages including my own rainbow package written with Hanlin Shang.
It is a much more powerful and flexible way to work so I thought it would be useful to share some examples."
2018,12,11,Data visualization for functional time series,https://robjhyndman.com/seminars/ftsviz/,Presentation to MATRIX Workshop on Functional Data Analysis and Beyond Any good data analysis begins with a careful graphical exploration of the observed data. For functional time series data this area of statistical analysis has been largely neglected. I will look at the tools that are available such as rainbow plots and functional box plots and propose several new tools including functional ACF plots functional season plots calendar plots and embedded pairwise distance plots.
2018,12,9,Seasonal functional autoregressive models,https://robjhyndman.com/seminars/sfar/,Presentation to ACEMS/MATRIX Conference on Functional Data Analysis Functional autoregressive models have been widely used in functional time series analysis but no attention has been given to handling seasonality within this framework. I will discuss a proposed seasonal functional autoregressive model and explore some of its statistical properties including stationarity conditions and limiting behaviour. I will also look at methods for estimation and prediction of seasonal functional autoregressive time series of order one.
2018,12,5,High-dimensional time series analysis,https://robjhyndman.com/seminars/highdimts/,"Presentation to the Australasian Actuarial Education and Research Symposium It is becoming increasingly common for organizations to collect huge amounts of data over time. Traditional time series methods are not well suited to this new paradigm. I will review some new tools that have been developed to analyse large collections of time series including visualization anomaly detection and forecasting.
Data visualization is essential for exploring and understanding structures and patterns and to identify unusual observations."
2018,11,30,Forecasting competitions,https://robjhyndman.com/seminars/forecasting-competitions/,Presentation for the Monash Executive MBA students
2018,11,28,M4 Forecasting Conference,https://robjhyndman.com/hyndsight/m4conference/,Following the highly successful M4 Forecasting Competition there will be a conference held on 10-11 December at Tribeca Rooftop New York to discuss the results. The conference will elaborate on the findings of the M4 Competition with prominent speakers from leading business firms (Amazon Uber Google Microsoft SAS and ProLogistica) and top universities. Nassim Nicholas Taleb will deliver a keynote address about uncertainty in forecasting and elaborate on his claims that &ldquo;tail risks are much worse now than in 2007&rdquo; while Spyros Makridakis will discuss how organizations can benefit by improving the accuracy of their predictions and assessing uncertainty realistically.
2018,11,26,Feature-based time series analysis,https://robjhyndman.com/seminars/monash-fbtsa/,Presentation to the Monash international workshop on time series and panel data It is becoming increasingly common for organizations to collect very large amounts of data over time. Data visualization is essential for exploring and understanding structures and patterns and to identify unusual observations. However the sheer quantity of data available means that new time series visualisation methods are needed. I will demonstrate an approach to this problem using a vector of features on each time series measuring characteristics of the series.
2018,10,16,Writing for Researchers,https://robjhyndman.com/seminars/writing/,"ACEMS Mentoring Seminar
Researchers need to write a lot: theses papers reviewer reports responses to reviewer reports grant applications industry reports media releases blog posts and more. We will look at some of the tools available to help researchers with these writing tasks and how to structure different types of writing depending on the audience and purpose.
Slides References  Mendeley Zotero Paperpile Google Scholar  LaTeX and Rmarkdown  TeXlive Rmarkdown Rmarkdown thesis template Rmarkdown templates for papers  Text Editors  SublimeText Atom TeXstudio RStudio  Version control  Happy git with R Github"
2018,9,18,Forecasting the future of the power industry: What can you learn from smart meter data?,https://robjhyndman.com/seminars/monash-masterclass-2018/,"Monash Masterclass Smart electricity meters are providing very rich data in their third year of deployment in Victoria. We now have the potential to see how millions of households and businesses consume high-frequency electricity. But what does this type of data tell us? And how should we use it?
I will discuss how to:
 visualise data from smart electricity meters; identify common household consumption patterns and anomalies; consider the methods for forecasting demand for household electricity; use information from smart electricity meters to monitor changes in the power industry and anticipate consumption trends."
2018,8,26,MeDaScIn 2018,https://robjhyndman.com/hyndsight/medascin-2018/,"The annual Melbourne Data Science Initiative (or MeDaScIn pronounced medicine) is on again next month (24-27 September) with lots of tutorials and the annual datathon.
This year there will be a &ldquo;Forecasting with R&rdquo; workshop (25 September) led my two of my Monash colleagues &ndash; George Athanasopoulos and Elena Sanina.
Another great tutorial will be led by Steph Kovalchik (from Tennis Australia) on sports analytics with R (24 September).
For the full list of tutorials see the MeDaScIn website."
2018,8,22,Developing good research habits,https://robjhyndman.com/seminars/research_habits2018/,"Presentation for the 2018 honours students
Slides Links  Mendeley Zotero Paperpile Google Scholar Rmarkdown Happy git with R Rmarkdown thesis template"
2018,8,9,Crude oil price forecasting based on internet concern using an extreme learning machine,https://robjhyndman.com/publications/crude-oil-price/,The growing internet concern (IC) over the crude oil market and related events influences market trading thus creating further instability within the oil market itself. We propose a modeling framework for analyzing the effects of IC on the oil market and for predicting the price volatility of crude oil’s futures market. This novel approach decomposes the original time series into intrinsic modes at different time scales using bivariate empirical mode decomposition (BEMD).
2018,8,6,National Science Week Melbourne Mathematics Activities,https://robjhyndman.com/hyndsight/natsciweek2018/,"Next week is National Science Week and there are a few mathematics activities happening around Melbourne that are being sponsored by ACEMS.

 Elsewhere in Melbourne: Mon 13 Aug 2018 6:00pm - 7:30pm
Public Talk: Is this your card?
Location: University of Melbourne
Speakers: Anthony Mays &amp; Jen Palisse
Pick a card any card! The immortal phrase of the magician. In this talk we&rsquo;ll look at some great card tricks that have simple maths behind them."
2018,8,3,Saving ts objects as csv files,https://robjhyndman.com/hyndsight/ts2csv/,Occasionally R might not be the tool you want to use (hard to believe but apparently that happens). Then you may need to export some data from R via a csv file. When the data is stored as a ts object the time index can easily get lost. So I wrote a little function to make this easier using the tsibble package to do almost all of the work in looking after the time index.
2018,7,26,ACEMS Forecasting Workshop,https://robjhyndman.com/seminars/acemsforecasting2018/,"Date: 26 July 2018
Location: University of Melbourne
This page is for people enrolled in my ACEMS half-day workshop.
Prerequisites Please bring your own laptop with a recent version of R and RStudio installed along with the fpp2 package and its dependencies.
Participants will be assumed to be familiar with basic statistical tools such as multiple regression but no knowledge of time series or forecasting will be assumed.
Reference ![Online textbook on forecasting](https://otexts."
2018,7,15,UseR!2018 talks,https://robjhyndman.com/hyndsight/user2018-talks/,"All talks from useR!2018 held in Brisbane last week are now available on YouTube.
Links to talks from members of my research team are given below."
2018,7,14,Seasonal decomposition of short time series,https://robjhyndman.com/hyndsight/tslm-decomposition/,"Many users have tried to do a seasonal decomposition with a short time series and hit the error “Series has less than two periods”.
The problem is that the usual methods of decomposition (e.g. decompose and stl) estimate seasonality using at least as many degrees of freedom as there are seasonal periods. So you need at least two observations per seasonal period to be able to distinguish seasonality from noise."
2018,7,13,Tidy forecasting in R,https://robjhyndman.com/seminars/user-fable/,"Presentation at the useR 2018 conference in Brisbane Australia The forecast package in R is widely used and provides good tools for monthly quarterly and annual time series. But it is not so well-developed for daily and sub-daily data and it does not interact easily with modern tidy packages such as dplyr purrr and tidyr.
I will describe our plans and progress in developing a collection of packages to provide tidy tools for time series and forecasting which will interact seamlessly with tidyverse packages and provide functions to handle time series at any frequency."
2018,7,6,Bivariate smoothing of mortality surfaces with cohort and period ridges,https://robjhyndman.com/publications/mortality-smoothing/,Mortality rates typically vary smoothly over age and time. Exceptions occur due to events such as wars and epidemics which create ridges in the age-period surface of mortality rates in a particular year or for cohorts born in a particular year. We propose a new practical method for modelling the age-period surface of mortality rates. Our approach uses $L_1$ regularization with bivariate smoothing and allows for age-varying period and cohort ridges in the otherwise smooth surface.
2018,6,25,Forecasting: principles and practice (NYC),https://robjhyndman.com/seminars/nyc2018/,"Date: 25-27 June 2018
Location: eBay 625 6th Ave New York NY 10011 USA.
This page is for people enrolled in my NYC 3-day workshop.
Prerequisites Please bring your own laptop with a recent version of R and RStudio installed along with the following packages and their dependencies:
 fpp2 readxl thief knitr  Participants will be assumed to be familiar with basic statistical tools such as multiple regression but no knowledge of time series or forecasting will be assumed."
2018,6,24,A forecast ensemble benchmark,https://robjhyndman.com/hyndsight/benchmark-combination/,"Forecasting benchmarks are very important when testing new forecasting methods to see how well they perform against some simple alternatives. Every week I get sent papers proposing new forecasting methods that fail to do better than even the simplest benchmark. They are rejected without review.
Typical benchmarks include the naïve method (especially for finance and economic data) the seasonal naïve method (for seasonal data) an automatically selected ETS model and an automatically selected ARIMA model."
2018,6,21,Feature-based time series analysis,https://robjhyndman.com/seminars/nyc-fbtsa/,"Presentation to the New York R Meetup It is becoming increasingly common for organizations to collect very large amounts of data over time. Data visualization is essential for exploring and understanding structures and patterns and to identify unusual observations. However the sheer quantity of data available means that new time series visualisation methods are needed.
I will demonstrate an approach to this problem using a vector of features on each time series measuring characteristics of the series."
2018,6,19,Tidy forecasting in R,https://robjhyndman.com/seminars/isf-fable/,"Presentation at the International Symposium on Forecasting Boulder USA. The forecast package in R is widely used and provides good tools for monthly quarterly and annual time series. But it is not so well-developed for daily and sub-daily data and it does not interact easily with modern tidy packages such as dplyr purrr and tidyr.
I will describe our plans and progress in developing a collection of packages to provide tidy tools for time series and forecasting which will interact seamlessly with tidyverse packages and provide functions to handle time series at any frequency."
2018,6,6,Meta-learning how to forecast time series,https://robjhyndman.com/publications/fforms/,A crucial task in time series forecasting is the identification of the most suitable forecasting method. We present a general framework for forecast-model selection using meta-learning. A random forest is used to identify the best forecasting method using only time series features. The framework is evaluated using time series from the M1 and M3 competitions and is shown to yield accurate forecasts comparable to several benchmarks and other commonly used automated approaches of time series forecasting.
2018,4,23,Forecasting in NYC: 25-27 June 2018,https://robjhyndman.com/hyndsight/forecasting-nyc-2018/,"In late June I will be in New York to teach my 3-day workshop on Forecasting using R. Tickets are available at Eventbrite.
This is the first time I&rsquo;ve taught this workshop in the US having previously run it in the Netherlands and Australia. It will be based on the 2nd edition of my book &ldquo;Forecasting: Principles and Practice&rdquo; with George Athanasopoulos. All participants will get a print version of the book."
2018,4,23,Upcoming talks: May-July 2018,https://robjhyndman.com/hyndsight/upcoming-talks-2018/,"First semester teaching is nearly finished and that means conference season for me. Here are some talks I&rsquo;m giving in the next two months. Click the links for more details.
 Melbourne Australia. 28 May: Panel discussion: Forecasting models the uncertainties and associated risk Boulder Colorado USA. 17-20 June: International Symposium on Forecasting. I&rsquo;ll be talking about &ldquo;Tidy forecasting in R&rdquo;. New York USA. 21 June: Feature-based time series analysis. New York Open Statistical Programming Meetup eBay NYC."
2018,4,14,forecast v8.3 now on CRAN,https://robjhyndman.com/hyndsight/forecast83/,"The latest version of the forecast package for R is now on CRAN. This is the version used in the 2nd edition of my forecasting textbook with George Athanasopoulos. So readers should now be able to replicate all examples in the book using only CRAN packages.
A few new features of the forecast package may be of interest. A more complete Changelog is also available.
mstl() handles multiple seasonality STL decomposition was designed to handle a single type of seasonality but modern data often involves several seasonal periods (e."
2018,4,11,A brief history of time series forecasting competitions,https://robjhyndman.com/hyndsight/forecasting-competitions/,"Prediction competitions are now so widespread that it is often forgotten how controversial they were when first held and how influential they have been over the years.
To keep this exercise manageable I will restrict attention to time series forecasting competitions &mdash; where only the history of the data is available when producing forecasts.
Nottingham studies The earliest non-trivial study of time series forecast accuracy was probably by David Reid as part of his PhD at the University of Nottingham (1969)."
2018,4,9,High dimensional time series analysis,https://robjhyndman.com/seminars/abs-hdts/,"Presentation for the Australian Bureau of Statistics I will provide an overview of some of my recent research on methods to deal with high-dimensional time series.
1. Visualizing many time series
It is becoming increasingly common for organizations to collect very large amounts of data over time. Data visualization is essential for exploring and understanding structures and patterns and to identify unusual observations. However the sheer quantity of data available challenges current time series visualisation methods."
2018,3,26,R package for M4 Forecasting Competition,https://robjhyndman.com/hyndsight/m4comp2018/,"The M4 forecasting competition is well under-way and a few of my PhD students have been working on submissions.
Pablo Montero-Manso Carla Netto and Thiyanga Talagala have made an R package containing all of the data (100000 time series) which should make it substantially easier for other contestants to load the data into R in order to compute forecasts.
Grab the package from this github repository.
For more details about the M4 competition see this post or go to the M4 website."
2018,3,23,Research++: what you should know about being a researcher but probably don't,https://robjhyndman.com/seminars/research++/,"Presentation for my research group about aspects of being a researcher.
Some of this material has appeared as blog posts at some time.
Slides Links  Paperpile My Google Scholar profile How to Write a Lot: A Practical Guide to Productive Academic Writing Monash University authorship policy Antarctica Journal of Mathematics"
2018,3,13,IJF Tao Hong Award 2018,https://robjhyndman.com/hyndsight/ijf-hong-nominations-2018/,"Every two years the International Journal of Forecasting awards a prize to the best paper on energy forecasting. The prize is generously funded by Professor Tao Hong. This year we will award the prize to a paper published in the IJF during the period 2015-2016. The prize will be US$1000 plus an engraved plaque. The award committee is Rob J Hyndman Pierre Pinson and James Mitchell.
Nominations are invited from any reader of the IJF."
2018,2,4,Exploring the sources of uncertainty: why does bagging for time series forecasting work?,https://robjhyndman.com/publications/bagging-uncertainty/,In a recent study Bergmeir Hyndman and Benítez (2016) successfully employed a bootstrap aggregation (bagging) technique for improving the performance of exponential smoothing. Each series is Box-Cox transformed and decomposed by Seasonal and Trend decomposition using Loess (STL); then bootstrapping is applied on the remainder series before the trend and seasonality are added back and the transformation reversed to create bootstrapped versions of the series. Subsequently they apply automatic exponential smoothing on the original series and the bootstrapped versions of the series with the final forecast being the equal-weight combination across all forecasts.
2018,2,1,Visualizing big energy data,https://robjhyndman.com/publications/visualizing-big-energy-data/,Visualization is a crucial component of data analysis. It is always a good idea to plot the data before fitting any models making any predictions or drawing any conclusions. As sensors of the electric grid are collecting large volumes of data from various sources power industry professionals are facing the challenge of visualizing such data in a timely fashion. In this article we demonstrate several data visualization solutions for big energy data through three case studies involving smart meter data phasor measurement unit (PMU) data and probabilistic forecasts respectively.
2018,1,1,A note on the validity of cross-validation for evaluating autoregressive time series prediction,https://robjhyndman.com/publications/cv-time-series/,One of the most widely used standard procedures for model evaluation in classification and regression is $K$-fold cross-validation (CV). However when it comes to time series forecasting because of the inherent serial correlation and potential non-stationarity of the data its application is not straightforward and often omitted by practitioners in favour of an out-of-sample (OOS) evaluation. In this paper we show that in the case of a purely autoregressive model the use of standard $K$-fold CV is possible as long as the models considered have uncorrelated errors.
2017,12,22,M4 Forecasting Competition update,https://robjhyndman.com/hyndsight/m4competition/,"The official guidelines for the M4 competition have now been published and there have been several developments since my last post on this.
  There is now a prize for prediction interval accuracy using a scaled version of the Mean Interval Score. If the $100(1-\alpha)$% prediction interval for time $t$ is given by $[L_{t}U_{t}]$ for $t=1\dotsh$ then the MIS is defined as $$\frac{1}{h}\sum_{t=1}^{h} \left[ (U_t-L_t) + \frac{2}{\alpha}(L_t-Y_t)1(Y_t &lt; L_t) + \frac{2}{\alpha}(Y_t-U_t)1(Y_t &gt; U_t) \right] $$ where $Y_t$ is the observation at time $t$."
2017,12,13,Probabilistic outlier detection and visualization of smart metre data,https://robjhyndman.com/seminars/nzsa2017/,"Talk given at meeting of New Zealand Statistical Association and International Association for Statistical Computing (11-14 December 2017) Auckland New Zealand.
It is always a good idea to plot your data before fitting any models making any predictions or drawing any conclusions. But how do you actually plot data on thousands of smart meters each comprising thousands of observations over time? We cannot simply produce time plots of the demand recorded at each meter due to the sheer volume of data involved."
2017,12,5,Data Science for Managers: May 2018,https://robjhyndman.com/hyndsight/data-science-for-managers/,"For the last few years I have been involved with running a 3-day short course on &ldquo;Data Science for Managers&rdquo;. We have run it twice each year since 2015 and it continues to prove very popular. We have some awesome presenters including Monash University professors Di Cook Geoff Webb and Kim Marriott as well as several very experienced data scientists working in industry.
The next course will be held on 8-10 May 2018."
2017,12,5,Trends in Indigenous mortality and life expectancy 2001-2015,https://robjhyndman.com/publications/aihw2017/,The Enhanced Mortality Database (EMD) was developed in 2010 by the Australian Institute of Health and Welfare to explore the feasibility of creating an ongoing enhanced mortality data set that allows analysis of key mortality indicators including life expectancy and causes of death to assist with monitoring ‘Closing the Gap’ health targets. The method involves using data linkage to enhance the identification of Aboriginal and Torres Strait Islander people in death registrations.
2017,11,29,Some new time series packages,https://robjhyndman.com/hyndsight/tspackages/,This week I have finished preliminary versions of two new R packages for time series analysis. The first (tscompdata) contains several large collections of time series that have been used in forecasting competitions; the second (tsfeatures) is designed to compute features from univariate time series data. For now both are only on github. I will probably submit them to CRAN after they’ve been tested by a few more people.
2017,11,20,M4 Forecasting Competition: response from Spyros Makridakis,https://robjhyndman.com/hyndsight/m4comp-response/,"Following my post on the M4 competition yesterday Spyros Makridakis sent me these comments for posting here.
I would like to thank Rob my friend and co-author for his insightful remarks concerning the upcoming M4 competition. As Rob says the two of us have talked a great deal about competitions and I certainly agree with him about the “ideal” forecasting competition. In this reply I will explain why I have deviated from the “ideal” mostly for practical reasons and to ensure higher participation."
2017,11,19,M4 Forecasting Competition,https://robjhyndman.com/hyndsight/m4comp/,"The &ldquo;M&rdquo; competitions organized by Spyros Makridakis have had an enormous influence on the field of forecasting. They focused attention on what models produced good forecasts rather than on the mathematical properties of those models. For that Spyros deserves congratulations for changing the landscape of forecasting research through this series of competitions.
Makridakis &amp; Hibon (JRSSA 1979) was the first serious attempt at a large empirical evaluation of forecast methods."
2017,11,18,2017 Beijing Workshop on Forecasting,https://robjhyndman.com/seminars/beijing2017/,"Location: Central University of Finance and Economics Beijing China.
Slides  Forecast Accuracy and Evaluation Automatic Forecasting Algorithms Hierarchical Forecasting Probabilistic Hierarchical Forecasting  Textbook 
Relevant papers  Rob J Hyndman Anne B Koehler Ralph D Snyder Simone Grose (2002) A state space framework for automatic forecasting using exponential smoothing methods. International Journal of Forecasting 18(3) 439-454. Rob J Hyndman Anne B Koehler (2006) Another look at measures of forecast accuracy."
2017,11,14,Come and work with me,https://robjhyndman.com/hyndsight/acems-postdoc/,"I have funding for a new post-doctoral research fellow on a 2-year contract to work with me and Professor Kate Smith-Miles on analysing large collections of time series data. We are particularly seeking someone with a PhD in computational statistics or statistical machine learning.
Desirable characteristics:
 Experience with time series data. Experience with R package development. Familiarity with reproducible research practices (e.g. git rmarkdown etc). A background in machine learning or computational statistics."
2017,11,1,2017 Beijing Workshop on Forecasting,https://robjhyndman.com/hyndsight/beijing-2017/,"Later this month I&rsquo;m speaking at the 2017 Beijing Workshop on Forecasting to be held on Saturday 18 November at the Central University of Finance and Economics.
I&rsquo;m giving four talks as part of the workshop. Other speakers are Junni Zhang Lei Song Hui Bu Feng Li and Yanfei Kang.
Full program details are available online."
2017,11,1,High dimensional time series analysis,https://robjhyndman.com/seminars/acems-hdts/,Keynote presentation for ACEMS retreat.
2017,10,12,Analysing sub-daily time series data,https://robjhyndman.com/seminars/subdaily-2017/,"Talk given by me Earo Wang and Mitchell O&rsquo;Hara-Wild at the Melbourne Users of R Network meeting.
Slides   

 Packages"
2017,9,26,Forecasting: principles and practice (UWA),https://robjhyndman.com/seminars/uwa2017/,"Date: 26-28 September 2017
Location: The University Club University of Western Australia
This page is for people enrolled in my UWA 3-day workshop.
Prerequisites Please bring your own laptop with a recent version of R and RStudio installed along with the following packages and their dependencies:
 fpp2 readxl thief knitr  Participants will be assumed to be familiar with basic statistical tools such as multiple regression but no knowledge of time series or forecasting will be assumed."
2017,9,21,High-dimensional time series,https://robjhyndman.com/seminars/high-dimensional-time-series/,Presentation for ACEMS Monash node giving an overview of my current major research interests.
2017,9,7,Looking for a new research assistant,https://robjhyndman.com/hyndsight/ra-wanted/,"I&rsquo;m currently looking for a new research assistant to help (primarily) with some modelling and R coding as part of a project on forecasting mobile phone sales. The position is likely to last for about 6&ndash;9 months and will be casual.
 Requirements    Based in Melbourne. I&rsquo;d rather not communicate remotely.    Able to work at least 20 hours per week. Some of that can be from home if necessary but you do need to be at Monash University (Clayton campus) at least some of the time."
2017,9,7,rOpenSci OzUnconference coming to Melbourne,https://robjhyndman.com/hyndsight/ozunconf2017/,"For a second year running there will be another rOpenSci OzUnconference in Australia. This one will be held in Melbourne on 26-27 October 2017.
Unlike regular conferences there are no talks and there is no pre-determined agenda. It brings together scientists developers and open data enthusiasts from academia industry government and non-profit to get together for a few days to work on R-related projects. The agenda is mostly decided during the conference itself and involves participants dividing into small groups to work on the projects of most interest to them."
2017,8,31,Finding distinct rows of a tibble,https://robjhyndman.com/hyndsight/distinct/,"I’ve been using R or its predecessors for about 30 years so I tend to I know a lot about R but I don’t necessarily know how to use modern R tools. Lately I’ve been teaching my students the tidyverse approach to data analysis which means that I need to unlearn some old approaches and to re-learn them using new tools. But old dogs and new tricks…
Yesterday I was teaching a class where I needed to extract some rows of a data set."
2017,8,25,Biggish time series data,https://robjhyndman.com/seminars/biggish-time-series-data/,Informal presentation for a UNSW research group giving an overview of some of my research interests over the last few years.
2017,8,25,Optimal forecast reconcilation,https://robjhyndman.com/seminars/unsw2017/,"Talk given at UNSW
Abstract
Time series can often be naturally disaggregated in a hierarchical or grouped structure. For example a manufacturing company can disaggregate total demand for their products by country of sale retail outlet product type package size and so on. As a result there can be millions of individual time series to forecast at the most disaggregated level plus additional series to forecast at higher levels of aggregation."
2017,8,11,Visualizing and forecasting big time series data,https://robjhyndman.com/seminars/icml2017/,"ICML Time Series Workshop
Sydney Australia"
2017,7,30,Forecasting workshop in Perth,https://robjhyndman.com/hyndsight/forecasting-workshop-perth/,"On 26-28 September 2017 I will be running my 3-day workshop in Perth on &ldquo;Forecasting: principles and practice&rdquo; based on my book of the same name.
Topics to be covered include seasonality and trends exponential smoothing ARIMA modelling dynamic regression and state space models as well as forecast accuracy methods and forecast evaluation techniques such as cross-validation.
Workshop participants are expected to be familiar with basic statistical tools such as multiple regression but no knowledge of time series or forecasting will be assumed."
2017,7,14,Using data to tackle poverty,https://robjhyndman.com/seminars/poverty2017/,"Talk to Year 11 students as part of the Monash Scholars Knowledge Summit on &ldquo;Ending Global Poverty&rdquo;.
Introductory remarks    Hans Rosling: New insights on poverty"
2017,7,12,IJF Best Paper Award 2017,https://robjhyndman.com/hyndsight/ijf-best-paper-award-2017/,"At the recent International Symposium on Forecasting I announced the awards for the best paper published in the International Journal of Forecasting in the period 2014&ndash;2015.
We make an award every two years to the best paper(s) published in the journal. There is always about 18 months delay after the publication period to allow time for reflection citations etc. The selected papers are selected by vote of the editorial board. The best paper wins an engraved bronze plaque and US$1000."
2017,7,9,IIF Tao Hong Award 2016,https://robjhyndman.com/hyndsight/iif-hong-award-2016/,"A generous donation from Professor Tao Hong has funded this new award for papers on energy forecasting published in the International Journal of Forecasting. The award for 2016 is for papers published within 2013&ndash;2014. Next year we will award a paper published in 2015&ndash;2016 and we will make the award every two years after that.
The award decision was made by a committee consisting of Professors Pierre Pinson James Mitchell and Rob J Hyndman."
2017,7,7,IIF Fellow Ralph Snyder,https://robjhyndman.com/hyndsight/ralph-snyder/,"
At the International Symposium on Forecasting last week my friend and colleague Ralph Snyder was made a Fellow of the International Institute of Forecasters."
2017,6,26,Hex stickers for the forecast package,https://robjhyndman.com/hyndsight/forecast-stickers/,I&rsquo;ve caved in to the hex sticker craze and produced some hex stickers for the forecast package for R. If you attend a workshop I teach I&rsquo;ll give you one.
2017,6,22,Probabilistic outlier detection and visualization of smart metre data,https://robjhyndman.com/seminars/isea2017/,"Talk to be given at International Symposium on Energy Analytics (22-23 June 2017) Cairns Australia.
Standard time series analysis and visualization tools fail on smart metre data due to the sheer volume of available data (both in the time dimension and due to the large numbers of smart metres providing data). In addition smart metre data is often messy with missing observations periods where some metres are offline periods of relatively constant low level energy usage occasional days of unusually high demand and so on."
2017,6,21,Why I'm not celebrating the 2016 impact factors,https://robjhyndman.com/hyndsight/2016-impact-factors/,"Once every year the journal citation reports are released including journal impact factors. This year the International Journal of Forecasting 2-year impact factor has increased to 2.642 which is the highest it has been in the journal’s history and puts the journal higher than such notable titles as Journal of the American Statistical Association and just below Management Science.
The 2-year impact factor is the average number of citations for articles published in the previous 2 years."
2017,6,2,Coherent Probabilistic Forecasts for Hierarchical Time Series,https://robjhyndman.com/publications/probhts/,Many applications require forecasts for a hierarchy comprising a set of time series along with aggregates of subsets of these series. Although forecasts can be produced independently for each series in the hierarchy typically this does not lead to coherent forecasts — the property that forecasts add up appropriately across the hierarchy. State-of-the-art hierarchical forecasting methods usually reconcile these independently generated forecasts to satisfy the aggregation constraints. A fundamental limitation of prior research is that it has looked only at the problem of forecasting the mean of each time series.
2017,6,1,My new DataCamp course: Forecasting Using R,https://robjhyndman.com/hyndsight/datacamp/,"For the past few months I’ve been working on a new DataCamp course teaching Forecasting using R. I’m delighted that it is now available for anyone to do.
Course blurb Forecasting involves making predictions about the future. It is required in many situations such as deciding whether to build another power generation plant in the next ten years requires forecasts of future demand; scheduling staff in a call center next week requires forecasts of call volumes; stocking an inventory requires forecasts of stock requirements."
2017,5,29,Time Series in R: Forecasting and Visualisation,https://robjhyndman.com/seminars/forecasting-medascin/,"This is a one-day workshop given as part of the Melbourne Data Science Week.
Date: 29 May 2017
Presenters: Rob J Hyndman and Earo Wang
Location: KPMG Tower Two Collins Square 727 Collins St Melbourne
Prerequisites Please bring your own laptop with a recent version of R installed along with the following packages and their dependencies:
 devtools fpp2 knitr plotly shiny tidyverse  Participants will be assumed to be familiar with basic statistical tools such as multiple regression but no knowledge of time series or forecasting will be assumed."
2017,5,25,Prediction intervals for NNETAR models,https://robjhyndman.com/hyndsight/nnetar-prediction-intervals/,"The nnetar function in the forecast package for R fits a neural network model to a time series with lagged values of the time series as inputs (and possibly some other exogenous inputs). So it is a nonlinear autogressive model and it is not possible to analytically derive prediction intervals. Therefore we use simulation.
Suppose we fit a NNETAR model to the famous Canadian lynx data:
library(forecast) set.seed(2015) (fit &lt;- nnetar(lynx lambda=0."
2017,5,23,ISI Karl Pearson Prize for 2017,https://robjhyndman.com/hyndsight/kpprize2017/,"Recently I was privileged to sit on the committee that selects the winner of the Karl Pearson Prize. KP was of course an early mathematical statistician famous for many commonly-used statistical methods and tools including histograms the correlation coefficient the method of moments p-values the chi-squared test and principal components analysis. He is also infamous for his highly racist views support for eugenics anti-semitism and for refusing a knighthood.
All that aside the job of the committee was to select an English-language article or book published in the last 30 years that has made a stand-alone research contribution and which has had major influence on one or more of statistical theory statistical methodology statistical practice and application."
2017,5,13,Forecasting with temporal hierarchies,https://robjhyndman.com/publications/temporal-hierarchies/,This paper introduces the concept of Temporal Hierarchies for time series forecasting. A temporal hierarchy can be constructed for any time series by means of non-overlapping temporal aggregation. Predictions constructed at all aggregation levels are combined with the proposed framework to result in temporally reconciled accurate and robust forecasts. The implied combination mitigates modelling uncertainty while the reconciled nature of the forecasts results in a unified prediction that supports aligned decisions at different planning horizons: from short-term operational up to long-term strategic planning.
2017,5,4,Handgun acquisitions in California after two mass shootings,https://robjhyndman.com/hyndsight/handguns/,My new paper (Studdert et al 2017) on &ldquo;Handgun acquisitions in California after two mass shootings&rdquo; has been attracting some press.  Here are some of the news items I&rsquo;ve found:
2017,5,3,Monthly seasonality,https://robjhyndman.com/hyndsight/monthly-seasonality/,"I regularly get asked why I don&rsquo;t consider monthly seasonality in my models for daily or sub-daily time series. For example this recent comment on my post on seasonal periods or this comment on my post on daily data. The fact is I&rsquo;ve never seen a time series with monthly seasonality although that does not mean it does not exist.
Monthly seasonality is defined as a regular pattern that recurs every month in data that is observed more frequently than monthly."
2017,5,2,Handgun acquisitions in California after two mass shootings,https://robjhyndman.com/publications/handguns/,"Background Mass shootings are common in the United States. They are the most visible form of firearm violence. Their effect on personal decisions to purchase firearms is not well understood.
Objective To determine changes in handgun acquisition patterns after the mass shootings in Newtown Connecticut in 2012 and San Bernardino California in 2015.
Design Time-series analysis using seasonal autoregressive integrated moving-average (SARIMA) models.
Setting California.
Population Adults who acquired handguns between 2007 and 2016."
2017,4,30,Converting to blogdown,https://robjhyndman.com/hyndsight/blogdown/,"This website has gone through several major updates over the years. It began in 1993 as some handcrafted html files transitioned to Joomla and later to Wordpress. Then it slowly grew into a collection of ten connected Wordpress installations that became increasingly difficult to maintain and rather slow.
So I&rsquo;ve now converted the entire site to Blogdown/Hugo. Nearly 700 pages of wordpress content have been translated to markdown. I decided to drop a few parts of the site notably the pages for my 1998 forecasting textbook."
2017,4,25,Grouped functional time series forecasting: an application to age-specific mortality rates,https://robjhyndman.com/publications/grouped-functional-time-series-forecasting-an-application-to-age-specific-mortality-rates/,Age-specific mortality rates are often disaggregated by different attributes such as sex state and ethnicity. Forecasting age-specific mortality rates at the national and sub-national levels plays an important role in developing social policy. However independent forecasts of age-specific mortality rates at the sub-national levels may not add up to the forecasts at the national level. To address this issue we consider the problem of reconciling age-specific mortality rate forecasts from the viewpoint of grouped univariate time series forecasting methods (Hyndman et al 2011) and extend these methods to functional time series forecasting where age is considered as a continuum.
2017,4,18,Follow-up Forecasting Forum,https://robjhyndman.com/seminars/eindhoven2017/,Follow up to my October 2016 3-day workshop in Eindhoven Netherlands
2017,4,4,Software for honours students,https://robjhyndman.com/hyndsight/software-for-honours-students/,"I spoke to our new crop of honours students this morning. Here are my slides example files and links.
 Managing References  Mendeley Zotero Paperpile  Data analysis and computation  Download R Download Rstudio Online R tutorial R packages for time series R packages for econometrics R packages for finance  Writing your thesis LaTeX  Windows: Download MikTeX Mac OSX: Download MacTeX Linux: Check your usual software source for TeXLive; otherwise install TeX Live directly."
2017,4,4,Software for honours students,https://robjhyndman.com/seminars/2017-04-04-software-for-honours-students/,"I spoke to our new crop of honours students this morning. Here are my slides example files and links.
 Managing References  Mendeley Zotero Paperpile  Data analysis and computation  Download R Download Rstudio Online R tutorial R packages for time series R packages for econometrics R packages for finance  Writing your thesis LaTeX  Windows: Download MikTeX Mac OSX: Download MacTeX Linux: Check your usual software source for TeXLive; otherwise install TeX Live directly."
2017,3,30,Monash Rmarkdown templates on github,https://robjhyndman.com/hyndsight/monash-rmarkdown-templates-on-github/,"Rmarkdown templates for staff and students in my department are now available on github.
For a PhD thesis fork the repository MonashThesis.
For an Honours thesis fork the repository MonashHonoursThesis.
For beamer slides with a Monash Business School theme use the binb package.
For other templates install the R package MonashEBSTemplates R package. This provides templates for
 working papers exams letters reports"
2017,3,28,A note on upper bounds for forecast-value-added relative to naïve forecasts,https://robjhyndman.com/publications/fvanaive/,In forecast value added analysis the accuracy of relatively sophisticated forecasting methods is compared to that of naïve 1 forecasts to see whether the extra costs and effort of implementing them are justified. In this note we derive a ratio that indicates the upper bound of a forecasting method’s accuracy relative to naïve 1 forecasts when the mean squared error is used to measure one-period-ahead accuracy. The ratio is applicable when a series is stationary or when its first differences are stationary.
2017,3,21,Probabilistic energy forecasting for smart grids and buildings,https://robjhyndman.com/seminars/melbuni-smartgrids/,"Energy Systems Integration in smart buildings communities and microgrids
The University of Melbourne Melbourne Australia
21-22 March 2017
Organised by The University of Melbourne and the International Institute for Energy Systems Integration (iiESI)"
2017,3,18,Academic phishing,https://robjhyndman.com/hyndsight/academic-phishing/,"Invitations to write for bogus journals and speak at bogus conferences keep rolling in. Here is one I received today.
 Dear Dr. Rob J. Hyndman It is our great pleasure to welcome you to join in Part 2: Knowledge Economy Symposium of GCKE-2017 which will be held in Qingdao China during September 19-21 2017. And we cordially invite you to propose a Speech on your recent research of Corrigendum to: “Hierarchical forecasts for Australian domestic tourism” [International Journal of Forecasting 25 (2009) 146–166]&hellip; ."
2017,3,7,Follow-up forecasting forum in Eindhoven,https://robjhyndman.com/hyndsight/eindhoven2/,"Last October I gave a 3-day masterclass on &ldquo;Forecasting with R&rdquo; in Eindhoven Netherlands. There is a follow-up event planned for Tuesday 18 April 2017. It is particularly designed for people who attended the 3-day class but if anyone else wants to attend they would be welcome.
Please register here if you want to attend.The preliminary schedule is as follows.
10.00 -- 11.00  New developments in forecasting using R   forecast v8."
2017,3,7,WOMBAT MeDaScIn 2017,https://robjhyndman.com/hyndsight/wombat-medascin-2017/,"Last year we had WOMBAT (Workshop Organized by the Monash Business Analytics Team) at the zoo and MeDaScIn (Melbourne Data Science Initiative) in the city.
This year we are combining forces to hold WOMBAT MeDaScIn 2017.
There will be four days of tutorials (Monday 29 May to Thursday 1 June) and the main conference on Friday 2 June. We have an impressive range of local and international presenters including Yihui Xie (author of Rmarkdown Knitr Bookdown Blogdown and more) Di Cook (data visualization guru) Stephanie Kovalchik (Data Scientist at Tennis Australia) Amy Shi-Nash (Head of Data Science at Commonwealth Bank of Australia) Graham Williams (Director of Data Science at Microsoft) and many more."
2017,3,1,IJF Best Paper Award 2014-2015,https://robjhyndman.com/hyndsight/ijf-nominations1415/,Every two years we award a prize for the best paper published in the International Journal of Forecasting. It is now time to identify the best paper published in the IJF during 2014 and 2015. There is always about 18 months delay after the publication period to allow time for reflection citations etc. The prize is US$1000 plus an engraved plaque. I will present the prize at the ISF in Cairns in late June.
2017,2,28,forecast 8.0,https://robjhyndman.com/hyndsight/forecast8/,"In what is now a roughly annual event the forecast package has been updated on CRAN with a new version this time 8.0.
A few of the more important new features are described below.
Check residuals A common task when building forecasting models is to check that the residuals satisfy some assumptions (that they are uncorrelated normally distributed etc.). The new function checkresiduals makes this very easy: it produces a time plot an ACF a histogram with super-imposed normal curve and does a Ljung-Box test on the residuals with appropriate number of lags and degrees of freedom."
2017,2,27,Invited sessions at ISF2017,https://robjhyndman.com/hyndsight/invited-sessions-at-isf2017/,We are still looking for a few more invited sessions for the International Symposium on Forecasting to be held in Cairns Australia 25-28 June 2017.An invited session consists of 3 or 4 talks around a specific forecasting theme. You are allowed to be one of the speakers in a session you organize (although it is not necessary). Therefore if you know what you are planning to speak about all you need to do is find 2 or 3 other speakers who will speak on a related topic and invite them.
2017,2,15,Forecasters: bring your family to Cairns,https://robjhyndman.com/hyndsight/isf-social/,"Courtesy of Tourism Tropical North Queensland   We know Australia is a long way to come for many forecasters so we are making it easy for you to bring your families along to the International Symposium on Forecasting and have a vacation at the same time.
During the International Symposium on Forecasting there will be a social program organized for family and friends. Monday 26 June Official Partners Tour: Kuranda Train &amp; Skyrail."
2017,2,15,The Australian Macro Database,https://robjhyndman.com/hyndsight/ausmacrodata/,AusMacroData is a new website that encourages and facilitates the use of quantitative publicly available Australian macroeconomic data. The Australian Macro Database hosted at ausmacrodata.org provides a user-friendly front end for searching among over 40000 economic variables and is loosely based on similar international sites such as the Federal Reserve Economic Database (FRED). In total data on 40304 variables are available for download from AusMacroData. The majority of variables are sourced from the Australian Bureau of Statistics (ABS) and include data on national accounts balance of payments and trade housing and finance labour force consumer price indices.
2017,2,14,The Australian Macro Database: An online resource for macroeconomic research in Australia,https://robjhyndman.com/publications/ausmacrodata/,A website that encourages and facilities the use of quantitative publicly available Australian macroeconomic data is introduced. The Australian Macro Database hosted at ausmacrodata.org provides a user friendly front end for searching among over 40000 economic variables sourced from the Australian Bureau of Statistics and the Reserve Bank of Australia. The search box tags and categories used to facilitate data retrieval are described in detail. Known issues with the website and future plans are discussed in the conclusion.
2017,2,2,Forecasting practitioner talks at ISF 2017,https://robjhyndman.com/hyndsight/isf2017-practitioners/,The International Symposium on Forecasting is a little unusual for an academic conference in that it has always had a strong presence of forecasters working in business and industry as well as academic forecasters mostly at universities. We value the combination and interaction as it helps the academics understand the sorts of problems facing forecasters in practice and it helps practitioners stay abreast of new methods and developments coming out of forecasting research.
2017,1,31,Associations between outdoor fungal spores and childhood and adolescent asthma hospitalisations,https://robjhyndman.com/publications/jaci2016/,"Background: Childhood asthma is a significant public health problem and severe exacerbation can result in diminished quality of life and hospitalisation.
Objective: To examine the contribution of outdoor fungi to childhood and adolescent asthma hospitalisations
Methods: The Melbourne Air Pollen Children and Adolescent (MAPCAH) study is a case-crossover study of 644 children and adolescents (aged 2-17 years) hospitalised for asthma between September 2009 and December 2011. MAPCAH collected individual data on human rhinovirus (HRV) infection and fungal sensitisation; and daily counts of ambient concentrations of fungal spores pollen and air pollutants."
2017,1,27,Simulating from a specified seasonal ARIMA model,https://robjhyndman.com/hyndsight/simulating-from-a-specified-seasonal-arima-model/,"From my email today
 You use an illustration of a seasonal arima model:
  ARIMA(111)(111)4
  I would like to simulate data from this process then fit a model… but I am unable to find any information as to how this can be conducted… if I set phi1 Phi1 theta1 and Theta1 it would be reassuring that for large n the parameters returned by Arima(fooorder=c(111)seasonal=c(111)) are in agreement…"
2017,1,18,Dynamic Algorithm Selection for Pareto Optimal Set Approximation,https://robjhyndman.com/publications/dynamic-pareto-approximation/,This paper presents a meta-algorithm for approximating the Pareto optimal set of costly black-box multiobjective optimization problems given a limited number of objective function evaluations. The key idea is to switch among different algorithms during the optimization search based on the predicted performance of each algorithm at the time. Algorithm performance is modeled using a machine learning technique based on the available information. The predicted best algorithm is then selected to run for a limited number of evaluations.
2017,1,12,Visualising forecasting algorithm performance using time series instance spaces,https://robjhyndman.com/publications/ts-feature-space/,It is common practice to evaluate the strength of forecasting methods using collections of well-studied time series datasets such as the M3 data. But how diverse are these time series how challenging and do they enable us to study the unique strengths and weaknesses of different forecasting methods? In this paper we propose a visualisation method for a collection of time series that enables a time series to be represented as a point in a 2-dimensional instance space.
2017,1,11,IJF Tao Hong Award for the best paper in energy forecasting 2013-2014,https://robjhyndman.com/hyndsight/ijf-hong-award/,"Professor Tao Hong has generously funded a new prize for the best IJF paper on energy forecasting to be awarded every two years. The first award will be for papers published in the International Journal of Forecasting during the period 2013-2014. The prize will be US$1000 plus an engraved plaque. The award committee is Rob J Hyndman Pierre Pinson and James Mitchell.
Nominations are invited from any reader of the IJF."
2016,12,12,What is going on?,https://robjhyndman.com/hyndsight/ijf-reject/,"I seem to be getting an increasing number of submissions where the author has clearly not bothered to actually check that the paper was submitted correctly. Here is a rejection letter I wrote today.
 Dear xxxxx
  I am writing concerning manuscript #INTFOR_16xxxxx entitled &ldquo;xxxxxxxxxxxxxxxx&rdquo; which you submitted to the International Journal of Forecasting.
  Thank you for this submission but as it consists entirely of the IJF author guidelines it is not suitable for publication in the IJF."
2016,12,7,"Exploring the influence of short-term temperature patterns on temperature-related mortality: a case-study of Melbourne, Australia",https://robjhyndman.com/publications/temperature-mortality/,"Background Several studies have identified the association between ambient temperature and mortality; however several features of temperature behavior and their impacts on health remain unresolved.
We obtain daily counts of nonaccidental all-cause mortality data in the elderly (65 + years) and corresponding meteorological data for Melbourne Australia during 1999 to 2006. We then characterize the temporal behavior of ambient temperature development by quantifying the rates of temperature change during periods designated by pre-specified windows ranging from 1 to 30 days."
2016,12,5,Cross-validation for time series,https://robjhyndman.com/hyndsight/tscv/,I&rsquo;ve added a couple of new functions to the forecast package for R which implement two types of cross-validation for time series.K-fold cross-validation for autoregression The first is regular k-fold cross-validation for autoregressive models. Although cross-validation is sometimes not valid for time series models it does work for autoregressions which includes many machine learning approaches to time series. The theoretical background is provided in Bergmeir Hyndman and Koo (2015). So cross-validation can be applied to any model where the predictors are lagged values of the response variable.
2016,11,28,Invited sessions at the International Symposium on Forecasting,https://robjhyndman.com/hyndsight/isf-invited-sessions/,"We are currently calling for invited session proposals for the ISF to be held in Cairns Australia in June 2017.
An invited session consists of 3 or 4 talks around a specific forecasting theme. You are allowed to be one of the speakers in a session you organize (although it is not necessary). So if you know what you are planning to speak about all you need to do is find 2 or 3 other speakers who will speak on something related and invite them to join you."
2016,11,6,We're still hiring at Monash,https://robjhyndman.com/hyndsight/still-hiring-at-monash/,We have another position available this time for a lecturer (equivalent to an assistant professor tenure track in the US). The department covers a wide range of areas in statistics and econometrics but for this position we are looking for someone with expertise in at least one of business analytics data science actuarial science computational statistics and machine learning. Applicants who have recently completed a PhD or expect to do so in the next 6 months are welcome to apply.
2016,10,24,Q&A: predictive analytics,https://robjhyndman.com/hyndsight/qa-predictive-analytics/,"A major news outlet interviewed me on predictive analytics. Here were my responses. Data mining is not just for tech companies in fact it can be especially useful for industries which are not typically thought of to be &lsquo;innovative&rsquo; such as agriculture. What are some of the main industries that you think benefit from predictive analysis?
 Any industry that collects data can use data mining and statistical modelling.
Agriculture is becoming a heavy user of data science methods with data being collected on every aspect of crop or livestock health and development."
2016,10,23,Q&A time,https://robjhyndman.com/hyndsight/qa-time/,"Someone sent me some questions by email and I decided to answer some of them here. How important is it that I know and understand the underlying mathematical framework to forecasting methods? I understand conceptually how most of them work but I feel as if I may benefit from truly understanding the math.
 The main benefit of understanding the mathematics behind different forecasting models is to be able to adapt the models when they don&rsquo;t work well for your data."
2016,10,22,Tourism forecasting competition data as an R package,https://robjhyndman.com/hyndsight/tcomp/,"The data used in the tourism forecasting competition discussed in Athanasopoulos et al (2011) have been made available in the Tcomp package for R. The objects are of the same format as for Mcomp package containing data from the M1 and M3 competitions.
Thanks to Peter Ellis for putting the package together. He has also produced a nice blog post about it."
2016,10,19,Forecasting Using R (Eindhoven),https://robjhyndman.com/seminars/eindhoven/,"Date: 19-21 October 2016
Location: Eurandom Metaforum Building MF11-12 Department of Mathematics and Computer Science TU Eindhoven the Netherlands
Prerequisites Please bring your own laptop with a recent version of R installed along with the following packages and their dependencies:
 fpp ggplot2 magrittr readxl thief knitr  Participants will be assumed to be familiar with basic statistical tools such as multiple regression but no knowledge of time series or forecasting will be assumed."
2016,10,14,GEFCom2017: Hierarchical Probabilistic Load Forecasting,https://robjhyndman.com/hyndsight/gefcom2017/,"After the great success of the previous two energy forecasting competitions we have run (GEFCom2012 and GEFCom2014) we are holding another one this time focused on hierarchical probabilistic load forecasting. Check out all the details over on Tao Hong&rsquo;s blog.
The previous GEFComs have led to some major advances in forecasting methodology available via IJF papers by the winning teams. I expect similar developments to arise out of this competition. Winners get to present their work in Cairns Australia at ISEA2017."
2016,10,13,Reconciling forecasts: the hts and thief packages,https://robjhyndman.com/seminars/erum2016/,eRum2016 Poznań Poland.
2016,10,12,"Come to Melbourne, even if not to Monash",https://robjhyndman.com/hyndsight/unimelb-prof-datascience/,The University of Melbourne is advertising for a &ldquo;Professor in Statistics (Data Science)&quot;. Melbourne (the city) is fast becoming a vibrant centre for data science and applied statistics with more than 4700 people signed up for the Data Science Meetup Group a thriving start-up scene the group at Monash Business School (including Di Cook and me) and the Monash Centre for Data Science (including Geoff Webb and Wray Buntine). Not to mention that Melbourne is a wonderful place to live having won the &ldquo;World&rsquo;s most liveable city&rdquo; award from the Economist for the last 6 years in a row.
2016,10,5,Hadley Wickham Master R Developer course coming to Melbourne,https://robjhyndman.com/hyndsight/hadley-course-dec-2016/,"Hadley Wickham&rsquo;s popular R developer course is coming to Melbourne on 12-13 December 2016. Bookings can be made via Eventbrite.Hadley of course is the developer of the wonderful tidyverse set of R packages including ggplot2 dplyr tidyr readr purrr tibble and many more. He is the author of several books including the new &ldquo;R for Data Science&rdquo; he is the chief scientist at RStudio and a fellow cocktail enthusiast.
From the course blurb:"
2016,9,28,"Call for forecasting workshops in Cairns, Australia",https://robjhyndman.com/hyndsight/cairns-workshops/,"The 37th annual International Symposium on Forecasting will be held in Cairns Australia from 25-28 June 2017. We plan to hold some workshops on Sunday 25 June before the main conference.We are currently calling for workshop proposals. Proposals can be for a half-day or full-day workshop. As usual there will be a Practitioner Track at the conference which generally leads to excellent attendance at the workshops.
Though June sounds far away planning for the conference is well underway!"
2016,9,27,Eindhoven seminar on time series visualization,https://robjhyndman.com/hyndsight/eindhoven-seminar/,"I&rsquo;m currently in the Netherlands for a few weeks and I&rsquo;ll be giving a seminar at the Data Science Centre in Eindhoven next Wednesday afternoon on &ldquo;Visualization of big time series data&rdquo;. Details follow.Date: 5 October 2016 Time: 12.30-13.30 Venue: Filmhuis De Zwarte Doos 2 Den Dolech Eindhoven
Registration is required but free. Please book here.
Abstract:
It is becoming increasingly common for organizations to collect very large amounts of data over time."
2016,9,15,Forecasting large collections of related time series,https://robjhyndman.com/seminars/augsburg2016/,"Keynote talk given at the German Statistical Week Augsburg.
Abstract
Time series can often be naturally disaggregated in a hierarchical or grouped structure. For example a manufacturing company can disaggregate total demand for their products by country of sale retail outlet product type package size and so on. As a result there can be millions of individual time series to forecast at the most disaggregated level plus additional series to forecast at higher levels of aggregation."
2016,9,14,Forecast intervals for aggregates,https://robjhyndman.com/hyndsight/forecast-intervals-for-aggregates/,"A common problem is to forecast the aggregate of several time periods of data using a model fitted to the disaggregated data. For example you may have monthly data but wish to forecast the total for the next year. Or you may have weekly data and want to forecast the total for the next four weeks.
If the point forecasts are means then adding them up will give a good estimate of the total."
2016,9,9,R package forecast v7.2 now on CRAN,https://robjhyndman.com/hyndsight/forecast-v7-2/,"I’ve pushed a minor update to the forecast package to CRAN. Some highlights are listed here.
Plotting time series with ggplot2 You can now facet a time series plot like this:
library(forecast) library(ggplot2) lungDeaths &lt;- cbind(mdeaths fdeaths) autoplot(lungDeaths facets=TRUE) So autoplot.mts now behaves similarly to plot.mts
 Multi-step fitted values The fitted function has a new argument h to allow computation of in-sample fitted values of more than one-step-ahead."
2016,9,1,R packages for forecast combinations,https://robjhyndman.com/hyndsight/forecast-combinations/,"It has been well-known since at least 1969 when Bates and Granger wrote their famous paper on “The Combination of Forecasts” that combining forecasts often leads to better forecast accuracy.
So it is helpful to have a couple of new R packages which do just that: opera and forecastHybrid.
opera Opera stands for “Online Prediction by ExpeRt Aggregation”. It was written by Pierre Gaillard and Yannig Goude and Pierre provides a nice introduction in the vignette."
2016,8,31,Sponsorship for the Cairns forecasting conference,https://robjhyndman.com/hyndsight/isf2017-sponsorship/,"Regular readers will know that the International Symposium on Forecasting is coming to Australia in June 2017. This is the leading international forecasting conference and one I&rsquo;ve attended every year for the past 17 years.
It will be held in Cairns Australia &mdash; one of the most beautiful locations in the country (and there is some stiff competition!) and right next to the Great Barrier Reef. Some further information is available on our website (still in progress)."
2016,8,28,Rmarkdown template for a Monash working paper,https://robjhyndman.com/hyndsight/rmarkdown-template/,"This is only directly relevant to my Monash students and colleagues but the same idea might be useful for adapting to other institutions.
Some recent changes in the rmarkdown and bookdown packages mean that it is now possible to produce working papers in exactly the same format as we previously used with LaTeX.Just install the MonashEBSTemplates package from github. You also need a recent version of LaTeX.
Then from within RStudio create a new document by selecting &ldquo;Rmarkdown&rdquo; &ldquo;From Template&rdquo; and select &ldquo;Monash EBS Working Paper&rdquo;."
2016,8,22,The thief package for R: Temporal HIErarchical Forecasting,https://robjhyndman.com/hyndsight/thief/,"I have a new R package available to do temporal hierarchical forecasting based on my paper with George Athanasopoulos Nikolaos Kourentzes and Fotios Petropoulos. (Guess the odd guy out there!)
It is called “thief” - an acronym for Temporal HIErarchical Forecasting. The idea is to take a seasonal time series and compute all possible temporal aggregations that result in an integer number of observations per year. For example a quarterly time series is aggregated to biannual and annual; while a monthly time series is aggregated to 2-monthly quarterly 4-monthly biannual and annual."
2016,8,18,"""Forecasting with R"" short course in Eindhoven",https://robjhyndman.com/hyndsight/eindhoven-course/,"I will be giving my 3-day short-course/workshop on &ldquo;Forecasting with R&rdquo; in Eindhoven (Netherlands) from 19-21 October.
Details at https://www.win.tue.nl/~adriemel/shortcourse.html
Register here"
2016,8,10,Tourism time series repository,https://robjhyndman.com/hyndsight/tourism-time-series/,"A few years ago I wrote a paper with George Athanasopoulos and others about a tourism forecasting competition. We originally made the data available as an online supplement to the paper but that has unfortunately since disappeared although the paper itself is still available.
So I am posting the data here in case anyone wants to use it for replicating our results or for other research purposes. The data are split into monthly quarterly and yearly data."
2016,6,20,Exploring time series collections used for forecast evaluation,https://robjhyndman.com/seminars/isf2016/,"International Symposium on Forecasting. Santander Spain
It is common practice to evaluate the strength of forecasting methods using collections of well-studied time series datasets such as the M3 data. But how diverse are these time series how challenging and do they enable us to study the unique strengths and weaknesses of different forecasting methods? In this paper we propose a visualisation method for a collection of time series that enables a time series to be represented as a point in a 2-dimensional feature space."
2016,6,10,The latest IJF issue with GEFCom2014 results,https://robjhyndman.com/hyndsight/ijf-gefcom2014/,"The latest issue of the IJF is a bumper issue with over 500 pages of forecasting insights.
The GEFCom2014 papers are included in a special section on probabilistic energy forecasting guest edited by Tao Hong and Pierre Pinson. This is a major milestone in energy forecasting research with the focus on probabilistic forecasting and forecast evaluation done using a quantile scoring method. Only a few years ago I was having to explain to energy professionals why you couldn&rsquo;t use a MAPE to evaluate a percentile forecast."
2016,6,3,2017 International Symposium on Energy Analytics,https://robjhyndman.com/hyndsight/isea2017/,"Predictive Energy Analytics in the Big Data World Cairns Australia June 22-23 2017 ISEA2017 
This will be a great conference and it is in a great location &mdash; Cairns Australia right by the Great Barrier Reef. Even better if you stay on you can attend the International Symposium on Forecasting which immediately follows the International Symposium on Energy Analytics.
So block out 22-28 June 2017 on your calendars so you can enjoy a tropical paradise in one of the most beautiful parts of Australia while attending two awesome conferences."
2016,6,1,Forecast v7 (part 2),https://robjhyndman.com/hyndsight/forecast7-part-2/,"As mentioned in my previous post on the forecast package v7 the most visible feature was the introduction of ggplot2 graphics. This post briefly summarizes the remaining new features of forecast v7.
library(forecast) library(ggplot2) tslm rewritten The tslm function is designed to fit linear models to time series data. It is intended to approximately mimic lm (and calls lm to do the estimation) but to package the output to remember the ts attributes."
2016,5,27,Explore Australian Elections Data with R,https://robjhyndman.com/hyndsight/eechidna/,This is a guest post by my colleague Professor Di Cook cross-posted from her Visiphilia blog. Di and I are two of the authors of the new eechidna package for R now on CRAN. The eechidna package has just been posted on CRAN in time for the longest election campaign in Australia since the 1950s. The next Federal election scheduled for 2nd July was announced a few weeks ago. A little before this a handful of academics met at the first ever Australian ROpenSci Unnconference at the Microsoft headquarters in Brisbane in an event primarily organised by students at the Queensland University of Technology.
2016,5,12,SSA helping you find a job,https://robjhyndman.com/hyndsight/ssa-jobs-board/,"One of the great services of the Statistical Society of Australia is an excellent jobs board advertising available jobs for statisticians data analysts data scientists etc. Jobs can be filtered by industry location and job function.
Today the SSA announced a new service to job seekers: CV/Resume Critique. As a job seeker registered on the SSA Job Board you now have the option to request a free confidential CV/resume evaluation from an expert and writer."
2016,5,9,forecast v7 and ggplot2 graphics,https://robjhyndman.com/hyndsight/forecast7-ggplot2/,"Version 7 of the forecast package was released on CRAN about a month ago but I’m only just getting around to posting about the new features.
The most visible feature was the introduction of ggplot2 graphics. I first wrote the forecast package before ggplot2 existed and so only base graphics were available. But I figured it was time to modernize and use the nice features available from ggplot2. The following examples illustrate the main new graphical functionality."
2016,5,6,Automatic foRecasting using R,https://robjhyndman.com/seminars/automatic-forecasting-using-r/,Melbourne Data Science Initiative
2016,4,28,Bagging exponential smoothing methods using STL decomposition and Box-Cox transformation,https://robjhyndman.com/publications/bagging-ets/,Exponential smoothing is one of the most popular forecasting methods. We present a method for bootstrap aggregation (bagging) of exponential smoothing methods. The bagging uses a Box-Cox transformation followed by an STL decomposition to separate the time series into trend seasonal part and remainder. The remainder is then bootstrapped using a moving block bootstrap and a new series is assembled using this bootstrapped remainder. On the bootstrapped series an ensemble of exponential smoothing models is estimated.
2016,4,13,Melbourne Data Science Initiative 2016,https://robjhyndman.com/hyndsight/medascin2016/,"In just over three weeks the inaugural MeDaScIn event will take place. This is an initiative to grow the talent pool of local data scientists and to promote Melbourne as a world city of excellence in Data Science.
The main event takes place on Friday 6th May with lots of interesting sounding titles and speakers from business and government. I&rsquo;m the only academic speaker on the program giving the closing talk on &ldquo;Automatic FoRecasting&rdquo;."
2016,4,9,Buy book,https://robjhyndman.com/unbelievable/buy-book/,"If you prefer a print or offline version you can buy a copy via one of the links below.
Buy a print copy via CreateSpace
Buy a print copy via Amazon
Buy an e-copy via Google Books"
2016,3,28,Sample quantiles 20 years later,https://robjhyndman.com/hyndsight/sample-quantiles-20-years-later/,"Almost exactly 20 years ago I wrote a paper with Yanan Fan on how sample quantiles are computed in statistical software. It was cited 43 times in the first 10 years and 457 times in the next 10 years making it my third paper to receive 500+ citations.
So what happened in 2006 to suddenly increase the citations? I think it was a combination of things:  I wrote a new quantile() function (with Ivan Frohne) which made it into R core v2."
2016,3,28,Bibliography,https://robjhyndman.com/unbelievable/bibliography/,"Books and articles cited Angelakis A. N. and S. V. Spyridakis (1996). ‘The status of water resources in Minoan times: A preliminary study’. In: Diachronic Climatic Impacts on Water Resources: with emphasis on the Mediterranean region. Edited by A. N. Angelakis and A. S. Issar. Volume 36. NATO ASI Series. Berlin: Springer pp.161–191.
Archer G. L. (1982). Encyclopedia of Bible difficulties. Grand Rapids MI USA: Zondervan.
Bartholomew D. J. (2008). God chance and purpose: Can God have it both ways?"
2016,3,28,23. Welcome to the dark side,https://robjhyndman.com/unbelievable/ch23/,"This final collection of messages were from people who had already left the Christadelphian community in some cases many years before me. I know very few of these people but I appreciated their thoughtfulness in writing to me.
Message 31  I bet you have been getting the emails lately huh? Some pulling this way some pulling that way?
  I can tell from the comments on the blog that there is a contest going on for the control of your thoughts and mind."
2016,3,27,22. Me too,https://robjhyndman.com/unbelievable/ch22/,"Several people contacted me to say they were going through a similar process of deconversion. I think it was comforting for them (and for me) to know that the journey need not be a lonely one.
Message 26  I have similar questions but it is hard for me to break away. I think it would be liberating but not sure I can bear the abject grief of my parents. So do I continue to live a lie?"
2016,3,27,21. What now?,https://robjhyndman.com/unbelievable/ch21/,"A few of my correspondents wondered about my future. What would I do now I was no longer writing books and giving talks about the Bible? Would I start an atheist crusade with the same energy and enthusiasm I once gave to my preaching? Others wrote to me a few months after I had resigned asking how I was getting on.
Message 23  I know that you feel as though you have had an epiphany and that you can finally see the ‘truth’ clearly but from my very simple perspective it looks more like you have been blinded as a result of your intelligence … that you have had your nose in the books so long that you have forgotten to look up and see the evidence of God all around you."
2016,3,27,20. You must be ignorant,https://robjhyndman.com/unbelievable/ch20/,"A few parcels sent to me contained books and DVDs that were presumably supposed to convince me of my errors. I appreciate the care demonstrated by these gifts but they do assume that I am misinformed and ignorant about what I believe.
I find it rather insulting to be told that I gave up on a lifetime of faith and religious activity without really thinking through the issues and without reading widely."
2016,3,27,19. Was the pressure too much?,https://robjhyndman.com/unbelievable/ch19/,"A lot of people have assumed that something awful must have happened to me or that all the criticism directed at me has had some cumulative effect that has hurt my faith. It is not like that. I just don’t believe it and I did not want to spend the rest of my life pretending otherwise.
Message 17 I know you have taken a hammering for having opinions different to some of the more traditional ones and I have to say some of them I share … but having them shouldn’t have made you a target for people’s fear."
2016,3,27,18. I'm praying for you,https://robjhyndman.com/unbelievable/ch18/,Understandably a lot of my religious friends have said they are praying for me. I would have done the same when I was a believer. It was my instinctive response to any problem and even after I came to the view that God was probably imaginary I would still find myself occasionally wanting to pray when faced with difficulties. It is comforting to think that someone else might deal with the problems.
2016,3,27,17. You made me cry,https://robjhyndman.com/unbelievable/ch17/,"In contrast to the letters in the previous chapter most of the messages I received expressed sadness love and friendship for which I am deeply grateful.
I have very few comments on these. What can I say? I feel sad too. I have stayed in touch with most of these writers and they remain dear friends even though we no longer have a shared faith.
Message 4 Just read your latest blog post … firstly wanted to say that I love and respect you and always will regardless of what you call yourself now But secondly you made me cry today … and for that you need a clip over the ears!"
2016,3,27,16. Repent or die,https://robjhyndman.com/unbelievable/ch16/,"I have received several hundred emails letters and other messages as a result of my deconversion and resignation. Many of them were from friends some from people who had read my books or heard me speak and some were from people who knew me only as a professor of statistics but who were moved to send me their thoughts.
In this last section I have included a selection of these messages along with some retrospective reflections."
2016,3,27,15. I am not an axe-murderer,https://robjhyndman.com/unbelievable/ch15/,Christians often claim that their beliefs underpin their morality and those who do not believe in God or the Bible must therefore lack a moral compass. They assume that values and morals are a consequence of holding the true doctrines. Well I have not become an axe-murderer since becoming an unbeliever I have not suddenly become unfaithful to my wife nor have I begun torturing kittens I have not even started swearing.
2016,3,27,14. Did you hear about the guy...?,https://robjhyndman.com/unbelievable/ch14/,If the miracles of Jesus happened as reported then they would provide evidence for him having supernatural power. If he really walked across the Sea of Galilee provided food for thousands of people from five loaves and two fish and raised Lazarus from the dead then of course he is supernatural. If all those things are true then his claims to be the Son of God would need to be taken seriously.
2016,3,27,13. Drink the magic potion,https://robjhyndman.com/unbelievable/ch13/,"The Way of Life book (pp.9–10) gives two examples of laws from Leviticus and Deuteronomy that supposedly demonstrate an understanding of science that was not possible for the Israelites to have developed.
Food prohibitions The first example is that the Law of Moses prohibited eating seafood (Leviticus 11:4–810–12) which we now know can cause intestinal problems and food poisoning without careful preparation and refrigeration.
However it was not necessary to understand the science for the Israelites to have noticed that people who ate seafood often became ill."
2016,3,27,12. Does God make mistakes?,https://robjhyndman.com/unbelievable/ch12/,The contradictions game Three times I have debated skeptics of the Bible and each time they raised Bible contradictions. I knew they would and I was prepared. I am rather good at reconciling contradictions. Give me a long list of apparent contradictions and I can usually think of explanations. The difficulty is that the explanations are often contrived and can seem like a desperate attempt to explain away the problem. They satisfy the believers but unbelievers remain unconvinced.
2016,3,27,11. Artefacts and anachronisms,https://robjhyndman.com/unbelievable/ch11/,I have given nearly 50 talks on biblical archaeology. One was entitled “Bible skeletons and fingerprints” in which I described some recent archaeological finds that supported and illuminated the biblical record. I showed photographs of the ossuary of Caiaphas from the first century AD containing a skeleton of a 60 year old man — probably Caiaphas the high priest mentioned in the gospels. I covered the tiny clay seals that were found in Jerusalem from around 586BC.
2016,3,27,10. Evolving views on creation,https://robjhyndman.com/unbelievable/ch10/,Sunday School nonsense In my early teens I was taught in Sunday School that scientists had got it wrong — that the universe was actually a few thousand years old not the billions of years claimed by the “evolutionists”. I found that confusing. I was aware that the light from the nearest galaxy to our own took at least a few tens of thousands of years to get here and the light from many other galaxies took millions of years to arrive.
2016,3,27,9. Legend of the empty tomb,https://robjhyndman.com/unbelievable/ch9/,"The resurrection of Jesus is often cited as evidence of the Bible’s inspiration or at least of God’s existence. There is an obvious problem here — all we know about the resurrection of Jesus comes from the Bible so it cannot be used as evidence in support of itself. But leaving that difficulty aside let’s review what the Bible actually says about it.
Harmonizing the accounts The resurrection accounts are notoriously difficult to harmonize."
2016,3,27,8. The biblical crystal ball,https://robjhyndman.com/unbelievable/ch8/,"Forecasting is my area of expertise. I am Editor-in-Chief of the International Journal of Forecasting and I have written several textbooks on statistical forecasting. I spend many hours every week thinking about the probabilities associated with future events. So prophecy which is closely related to forecasting has always been of particular interest to me.
When I was a believer my favourite fulfilled prophecies were
 the return of Israel to their land; Daniel’s prophecies of four empires; the destruction of Tyre; and the crucifixion of Jesus."
2016,3,27,7. Thank God,https://robjhyndman.com/unbelievable/ch7/,"It is natural for believers to thank God when they survive a natural disaster or even when they avoid the effects of man-made tragedies. When the great tsunami of Boxing Day 2004 wiped out huge areas of Thailand Indonesia India and Sri Lanka many survivors thanked God for protecting them. Others wondered what God was doing while 230000 people died.
When the Australian Black Saturday bushfires in February 2009 killed 173 people many survivors thanked God for protecting them."
2016,3,27,6. What are the chances?,https://robjhyndman.com/unbelievable/ch6/,"One of the first things I have been asked by people who find out that I no longer believe is “What about answered prayers?”.
Prayer is not falsifiable when you already believe Prayer is not falsifiable if you begin by assuming that God exists and is listening. If someone prays that X occurs and it does then they say that the prayer has been answered. If X does not occur then they say the answer was no it wasn’t God’s will."
2016,3,27,5. Talking to myself,https://robjhyndman.com/unbelievable/ch5/,I tried hard to have an active and regular prayer time every morning for many many years. I would rise early and spend time jotting some thoughts in my prayer journal. Then I would offer a prayer based on what I had written. For a few months in 2002 when we lived in Canberra I would walk to the top of a nearby mountain each morning and surrounded by grazing kangaroos I would say my prayers as the sun rose over Black Mountain.
2016,3,26,4. What would convince you?,https://robjhyndman.com/unbelievable/ch4/,There is an old joke about a psychiatrist working with a patient who was convinced that he was dead. The psychiatrist says “So you think you’re dead. Do dead people bleed?” to which the patient replies “No of course not.” Then the psychiatrist takes a sharp needle and pricks the patient’s finger drawing a drop of blood. The shocked patient says “Well what do you know. Dead people do bleed!”
2016,3,26,3. Which is the better story?,https://robjhyndman.com/unbelievable/ch3/,The beautiful movie Life of Pi is a story about a boy (Pi) raised in India where his father owns a small zoo. He is deeply religious and simultaneously embraces Hinduism Christianity and Islam. At one level the book is about Pi’s deep faith in the face of his father’s atheist disapproval. When the family and zoo emigrate for Canada the ship founders in a storm and Pi finds himself on a lifeboat with a hyena a chimpanzee a zebra and a Bengal tiger for company.
2016,3,26,2. An end of faith,https://robjhyndman.com/unbelievable/ch2/,"This is the blog post in which I announced my loss of faith and my resignation from the Christadelphian community — 29 July 2013
This week I resigned from the Christadelphian faith after nearly 30 years as a member and having attended Christadelphian activities almost every week of my life.
I resigned because I no longer believe. I don’t believe the Bible is inspired by God. I am not even sure there is a God."
2016,3,26,1. No reason to believe,https://robjhyndman.com/unbelievable/ch1/,Two years after I started my blog I bumped into an old friend at church one Sunday. I hadn’t seen her for a couple of years and I asked after her family. She told me that Josh her oldest son had left the Christadelphian community and that he no longer believed in God. I decided to write to him to ask what had happened. After some correspondence Josh read my blog and responded with the following email (slightly edited):
2016,3,26,Preface,https://robjhyndman.com/unbelievable/preface/,"I was a Christadelphian1 for nearly 30 years from age 16 until I resigned in July 2013 when I no longer thought that there was sufficient evidence to warrant belief in the Bible or Christianity. This book explains why I changed.
Many of my Christadelphian friends have asked why I no longer believe and I want to give them the answer they deserve. After many years of speaking at Christadelphian meetings in Australia and in several other countries there are a lot of people who have heard me talk about my faith and will have wondered what has happened."
2016,3,23,Plotting overlapping prediction intervals,https://robjhyndman.com/hyndsight/overlappingpi/,"I often see figures with two sets of prediction intervals plotted on the same graph using different line types to distinguish them. The results are almost always unreadable. A better way to do this is to use semi-transparent shaded regions. Here is an example showing two sets of forecasts for the Nile River flow.
library(forecast) f1 = forecast(auto.arima(Nile lambda=0) h=20 level=95) f2 = forecast(ets(Nile) h=20 level=95) plot(f1 shadecol=rgb(001.4) flwd=1 main=&quot;Forecasts of Nile River flow&quot; xlab=&quot;Year&quot; ylab=&quot;Billions of cubic metres&quot;) polygon(c(time(f2$mean)rev(time(f2$mean))) c(f2$lowerrev(f2$upper)) col=rgb(100."
2016,3,21,"rOpenSci unconference in Brisbane, 21-22 April 2016",https://robjhyndman.com/hyndsight/ropensci2016/,"The first rOpenSci unconference in Australia will be held on Thursday and Friday (April 21-22) in Brisbane at the Microsoft Innovation Centre.
This event will bring together researchers developers data scientists and open data enthusiasts from industry government and university. The aim is to conceptualise and develop R-based tools that address current challenges in data science open science and reproducibility.
Past examples of the projects can here here and here. Also here."
2016,3,9,Monash Business Analytics Team Profile,https://robjhyndman.com/hyndsight/monash-insider/,Our research group been growing lately as you can see below! We were featured in the latest issue of the Monash newsletter The Insider. Check it out.
2016,3,4,Model variance for ARIMA models,https://robjhyndman.com/hyndsight/model-variance-for-arima-models/,"From today&rsquo;s email:
 I wanted to ask you about your R forecast package in particular the Arima() function. We are using this function to fit an ARIMAX model and produce model estimates and standard errors which in turn can be used to get p-values and later model forecasts. To double check our work we are also fitting the same model in SAS using PROC ARIMA and comparing model coefficients and output."
2016,2,28,On sampling methods for costly multi-objective black-box optimization,https://robjhyndman.com/publications/sampling-multiobjective-optimization/,We investigate the impact of different sampling techniques on the performance of multi-objective optimization methods applied to costly black-box optimization problems. Such problems are often solved using an algorithm in which a surrogate model approximates the true objective function and provides predicted objective values at a lower cost. As the surrogate model is based on evaluations of a small number of points the quality of the initial sample can have a great effect on the overall effectiveness of the optimization.
2016,2,24,Omitting outliers,https://robjhyndman.com/hyndsight/omitting-outliers/,"Someone sent me this email today:
 One of my colleagues said that you once said/wrote that you had encountered very few real outliers in your work and that normally the &ldquo;outlier-looking&rdquo; data points were proper data points that should not have been treated as outliers. Have you discussed this in writing? If so I would love to read it.
 I don&rsquo;t think I&rsquo;ve ever said or written anything quite like that and I see lots of outliers in real data."
2016,2,24,Making data analysis easier: Hadley Wickham at WOMBAT2016,https://robjhyndman.com/hyndsight/wombat2016-hadley/,"Slides for Hadley&rsquo;s talk
The slides for all the other talks from the workshop are also now online at wombat2016.org"
2016,2,18,Making forecasting easier: forecast v7 for R,https://robjhyndman.com/seminars/wombat-2016/,Talk given at WOMBAT 2016 conference
2016,2,17,Electricity price forecasting competition,https://robjhyndman.com/hyndsight/electricity-price-forecasting-competition/,The GEFCom competitions have been a great success in generating good research on forecasting methods for electricity demand and in enabling a comprehensive comparative evaluation of various methods. But they have only considered price forecasting in a simplified setting. So I&rsquo;m happy to see this challenge is being taken up as part of the European Energy Market Conference for 2016 to be held from 6-9 June at the University of Porto in Portugal.
2016,2,4,Forecasting uncertainty in electricity smart meter data by boosting additive quantile regression,https://robjhyndman.com/publications/smart-meter-quantiles/,Smart electricity meters are currently deployed in millions of households to collect detailed individual electricity consumption data. Compared to traditional electricity data based on aggregated consumption smart meter data are much more volatile and less predictable. There is a need within the energy industry for probabilistic forecasts of household electricity consumption to quantify the uncertainty of future electricity demand in order to undertake appropriate planning of generation and distribution. We propose a probabilistic forecasting method where a different quantile regression model is estimated for each quantile of the future distribution.
2016,1,31,Fast computation of reconciled forecasts for hierarchical and grouped time series,https://robjhyndman.com/publications/hgts/,We show that the least squares approach to reconciling hierarchical time series forecasts can be extended to much more general collections of time series with aggregation constraints. The constraints arise due to the need for forecasts of collections of time series to add up in the same way as the observed time series. We also show that the computations involved can be handled efficiently by exploiting the structure of the associated design matrix or by using sparse matrix routines.
2016,1,30,Bayesian rank selection in multivariate regression,https://robjhyndman.com/publications/bayesian-rank-selection-in-multivariate-regression/,Estimating the rank of the coefficient matrix is a major challenge in multivariate regression including vector autoregression (VAR). In this paper we develop a novel fully Bayesian approach that allows for rank estimation. The key to our approach is reparameterizing the coefficient matrix using its singular value decomposition and conducting Bayesian inference on the decomposed parameters. By implementing a stochastic search variable selection on the singular values of the coefficient matrix the ultimate selected rank can be identified as the number of nonzero singular values.
2016,1,28,What's your Hall number?,https://robjhyndman.com/hyndsight/whats-your-hall-number/,Today I attended the funeral of Peter Hall one of the finest mathematical statisticians ever to walk the earth and easily the best from Australia. One of the most remarkable things about Peter was his astonishing productivity with over 600 papers. As I sat in the audience I realised that many of the people there were probably coauthors of papers with Peter and I wondered how many statisticians in the world would have been his coauthors or second-degree co-authors.
2016,1,25,Probabilistic Energy Forecasting: Global Energy Forecasting Competition 2014 and Beyond,https://robjhyndman.com/publications/gefcom2014/,The energy industry has been going through a significant modernization process over the last decade. Its infrastructure is being upgraded rapidly. The supply demand and prices are becoming more volatile and less predictable than ever before. Even its business model is being challenged fundamentally. In this competitive and dynamic environment many decision-making processes rely on probabilistic forecasts to quantify the uncertain future. Although most of the papers in the energy forecasting literature focus on point or single-valued forecasts the research interest in probabilistic energy forecasting research has taken off rapidly in recent years.
2016,1,24,Long-term forecasts of age-specific participation rates with functional data models,https://robjhyndman.com/publications/participation-rates/,Many countries have implemented social programs providing long-term financial or in-kind entitlements. These programs often focus on specific age-groups and consequently their expenditure streams are subject to demographic change. Given the strains already existing on public budgets long-term forecasts are an increasingly important instrument to monitor the budgetary consequences of social programs. The expected development of the labour force is a key input to these forecasts. We suggest combining a functional data approach to age-profiles of labour market participation rates with information on education marital status and other exogenous variables to improve long-term forecasts of labour supply.
2016,1,20,ACEMS Business Analytics Prize 2016,https://robjhyndman.com/hyndsight/acems-prize-2016/,"We have established a new annual prize for research students at Monash University in the general area of business analytics funded by the Australian Centre of Excellence in Mathematical and Statistical Frontiers (ACEMS). The rules of the award are listed below.
  The student must have submitted a paper to a high quality journal or refereed conference on some topic in the general area of business analytics computational statistics or data visualization."
2016,1,11,Farewell Peter Hall (1951-2016),https://robjhyndman.com/hyndsight/farewell-peter-hall-1951-2016/,Peter Hall passed away on Saturday after a long battle with illness over the last couple of years. No statistician will need reminding of Peter&rsquo;s extensive contributions to the field. He had over 500 published papers and had won every major award available many of them listed on his Wikipedia page.I spent a few months visiting Peter at ANU in 2002 and we wrote a couple of papers together (here and here).
2015,12,31,Another look at forecast-accuracy metrics for intermittent demand,https://robjhyndman.com/publications/accuracy-intermittent-demand/,
2015,12,31,Measuring forecast accuracy,https://robjhyndman.com/publications/measuring-forecast-accuracy/,
2015,12,30,Starting a career in data science,https://robjhyndman.com/hyndsight/starting-a-career-in-data-science/,"I received this email from one of my undergraduate students:
 I&rsquo;m writing to you asking for advice on how to start a career in Data Science. Other professions seem a bit more straight forward in that accountants for example simply look for Internships and ways into companies from there. From my understanding the nature of careers in data science seem to be on a project-to-project basis. I&rsquo;m not sure how to get my foot stuck in the door."
2015,12,20,Making data analysis easier,https://robjhyndman.com/hyndsight/wombat2016/,"Di Cook and I are organizing a workshop on &ldquo;Making data analysis easier&rdquo; for 18-19 February 2016.
We are calling it WOMBAT2016 which an acronym for Workshop Organized by the Monash Business Analytics Team. Appropriately it will be held at the Melbourne Zoo. Our plan is to make these workshops an annual event.
Some details are available on the workshop website. Key features are:
  Hadley Wickham is our keynote speaker."
2015,12,11,RStudio just keeps getting better,https://robjhyndman.com/hyndsight/rstudio-just-keeps-getting-better/,"RStudio has been a life-changer for the way I work and for how I teach data analysis. I still have a couple of minor frustrations with it but they are slowly disappearing as RStudio adds features.
I use dual monitors and I like to code on one monitor and have the console and plots on the other monitor. Otherwise I see too little context and long lines get wrapped making the code harder to read."
2015,12,9,Who's downloading the forecast package?,https://robjhyndman.com/hyndsight/fpp-downloads/,"The github page for the forecast package currently shows the following information 
Note the downloads figure: 264K/month. I know the package is popular but that seems crazy. Also the downloads figure on github only counts the downloads from the RStudio mirror and ignores downloads from the other 125 mirrors around the world.Here are the top ten downloaded packages from the last month:
library(cranlogs) cran_top_downloads(when=&#39;last-month&#39;) rank package count from to 1 zoo 308290 2015-11-09 2015-12-08 2 forecast 263797 2015-11-09 2015-12-08 3 Rcpp 260636 2015-11-09 2015-12-08 4 lmtest 258810 2015-11-09 2015-12-08 5 fpp 244989 2015-11-09 2015-12-08 6 expsmooth 244179 2015-11-09 2015-12-08 7 fma 243556 2015-11-09 2015-12-08 8 tseries 243172 2015-11-09 2015-12-08 9 stringi 199384 2015-11-09 2015-12-08 10 ggplot2 192072 2015-11-09 2015-12-08 OK that is very weird."
2015,11,29,The hidden benefits of open-source software,https://robjhyndman.com/hyndsight/oss-benefits/,"I&rsquo;ve been having discussions with colleagues and university administration about the best way for universities to manage home-grown software.
The traditional business model for software is that we build software and sell it to everyone willing to pay. Very often that leads to a software company spin-off that has little or nothing to do with the university that nurtured the development. Think MATLAB S-Plus Minitab SAS and SPSS all of which grew out of universities or research institutions."
2015,11,13,ODI looking for young postgrad statisticians,https://robjhyndman.com/hyndsight/odi-fellowships/,"The Overseas Development Institute Fellowship Scheme sends young postgraduate statisticians (and economists) to work in the public sectors of developing countries in Africa the Caribbean and the Pacific on two-year contracts. This is a great way to develop skills and gain experience working within a developing country&rsquo;s government. And you get to live in a fascinating place!
The application process for the 2016-2018 Fellowship Scheme is now open. Students are advised to apply before 17 December 2015 for a chance to be part of the ODI Fellowship Scheme."
2015,10,28,Piecewise linear trends,https://robjhyndman.com/hyndsight/piecewise-linear-trends/,"I prepared the following notes for a consulting client and I thought they might be of interest to some other people too.
Let \(y_t\) denote the value of the time series at time \(t\) and suppose we wish to fit a trend with correlated errors of the form \[ y_t = f(t) + n_t \] where \(f(t)\) represents the possibly nonlinear trend and \(n_t\) is an autocorrelated error process."
2015,10,24,Forecasting big time series data using R,https://robjhyndman.com/seminars/china2015/,Keynote address given at the Chinese R conference held in Nanchang Jianxi province. 24-25 October 2015.
2015,10,20,forecast package v6.2,https://robjhyndman.com/hyndsight/forecast6-2/,"It is a while since I last updated the CRAN version of the forecast package so I uploaded the latest version (6.2) today. The github version remains the most up-to-date version and is already two commits ahead of the CRAN version.
This update is mostly bug fixes and additional error traps. The full ChangeLog is listed below.  Many unit tests added using testthat.
  Fixed bug in ets() when very short seasonal series were passed in a data frame."
2015,10,7,Stanford seminar,https://robjhyndman.com/hyndsight/stanford-seminar/,"I gave a seminar at Stanford today. Slides are below. It was definitely the most intimidating audience I&rsquo;ve faced with Jerome Friedman Trevor Hastie Brad Efron Persi Diaconis Susan Holmes David Donoho and John Chambers all present (and probably other famous names I&rsquo;ve missed).
I&rsquo;ll be giving essentially the same talk at UC Davis on Thursday."
2015,10,6,Optimal forecast reconciliation for big time series data,https://robjhyndman.com/seminars/optimal-forecast-reconciliation/,Seminar given at Stanford University on 6th October and University of California (Davis) on 8th October.
2015,10,5,Google workshop: Forecasting and visualizing big time series data,https://robjhyndman.com/seminars/google-oct-2015/,"Workshop for Google Mountain View California.
 Automatic algorithms for time series forecasting Optimal forecast reconciliation for big time series data Visualization of big time series data"
2015,9,25,Reproducibility in computational research,https://robjhyndman.com/hyndsight/reproducibility/,"Jane Frazier spoke at our research team meeting today on &ldquo;Reproducibility in computational research&rdquo;. We had a very stimulating and lively discussion about the issues involved. One interesting idea was that reproducibility is on a scale and we can all aim to move further along the scale towards making our own research more reproducible. For example
 Can you reproduce your results tomorrow on the same computer with the same software installed?"
2015,9,24,Chinese R conference,https://robjhyndman.com/hyndsight/chinese-r-conference/,"I will be speaking at the Chinese R conference in Nanchang to be held on 24-25 October on &ldquo;Forecasting Big Time Series Data using R&rdquo;.
Details (for those who can read Chinese) are at china-r.org."
2015,9,22,Upcoming talks in California,https://robjhyndman.com/hyndsight/upcoming-talks-in-california/,"I&rsquo;m back in California for the next couple of weeks and will give the following talk at Stanford and UC-Davis.
Optimal forecast reconciliation for big time series data Time series can often be naturally disaggregated in a hierarchical or grouped structure. For example a manufacturing company can disaggregate total demand for their products by country of sale retail outlet product type package size and so on. As a result there can be millions of individual time series to forecast at the most disaggregated level plus additional series to forecast at higher levels of aggregation."
2015,9,21,International Symposium on Forecasting: Spain 2016,https://robjhyndman.com/hyndsight/isf2016/,"June 19-22 2016 Santander Spain – Palace of La Magdalena
The International Symposium on Forecasting (ISF) is the premier forecasting conference attracting the world&rsquo;s leading forecasting researchers practitioners and students. Through a combination of keynote speaker presentations academic sessions workshops and social programs the ISF provides many excellent opportunities for networking learning and fun.
Speakers: Greg Allenby The Ohio State University USA Todd Clark Federal Reserve Bank of Cleveland USA José Duato Polytechnic University of Valencia Spain Robert Fildes Lancaster University United Kingdom Edward Leamer UCLA Anderson USA Henrik Madsen Technical University of Denmark Adrian Raftery University of Washington USA"
2015,9,15,Unbelievable,https://robjhyndman.com/hyndsight/unbelievable/,"This is a very different book from my usual areas of forecasting and statistics. It is a personal memoir describing my journey of deconversion from Christianity.
Until a few years ago I was regularly speaking at church conferences internationally and my books are still used in Bible classes and Sunday Schools around the world. I even helped establish an innovative new church which became a model for similar churches in other countries."
2015,9,15,IJF vol 31(4): Forecasting in telecommunications and ICT,https://robjhyndman.com/hyndsight/ijf-vol-314/,The last issue of the International Journal of Forecasting for 2015 has been released. This one contains the usual mix of topics plus a special section on Forecasting in telecommunications and ICT including a nice review article by Nigel Meade and Towhidul Islam. Enjoy!
2015,9,13,Advice to other journal editors,https://robjhyndman.com/hyndsight/finding-reviewers/,"I get asked to review journal papers almost every day and I have to say no to almost all of them. I know it is hard to find reviewers but many of these requests indicate very lazy editors. So to all the editors out there looking for reviewers here is some advice.
  Never ask someone who is an editor for another journal. I am handling about 500 submissions per year for the International Journal of Forecasting and about 10 per year for the Journal of Statistical Software."
2015,9,2,Mathematical annotations on R plots,https://robjhyndman.com/hyndsight/latex2exp/,"I’ve always struggled with using plotmath via the expression function in R for adding mathematical notation to axes or legends. For some reason the most obvious way to write something never seems to work for me and I end up using trial and error in a loop with far too many iterations.
So I am very happy to see the new latex2exp package available which translates LaTeX expressions into a form suitable for R graphs."
2015,8,24,New IJF editors,https://robjhyndman.com/publications/new-ijf-editors/,
2015,8,17,Machine learning bootcamp,https://robjhyndman.com/seminars/machine-learning-bootcamp/,"A talk on time series forecasting for the Monash University Machine Learning Bootcamp.
Demo R code"
2015,8,7,Statistical issues with using herbarium data for the estimation of invasion lag-phases,https://robjhyndman.com/publications/lagphase/,Current methods for using herbarium data as time series for example to estimate the length of the invasion lag phase often make assumptions that are both statistically and logically inappropriate. We present an alternative statistical approach estimating the lag phase based on annual rather than cumulative data a generalized linear model incorporating a log link for overall collection effort and piecewise linear splines. We demonstrate the method on two species representing good and poor data quality then apply it to two data sets comprising 448 species/region combinations.
2015,8,3,The bias-variance decomposition,https://robjhyndman.com/hyndsight/bias-variance/,"This week I am teaching my Business Analytics class about the bias-variance trade-off. For some reason the proof is not contained in either ESL or ISL even though it is quite simple. I also discovered that the proof currently provided on Wikipedia makes little sense in places.
So I wrote my own for the class. It is longer than necessary to ensure there are no jumps that might confuse students."
2015,7,16,Murphy diagrams in R,https://robjhyndman.com/hyndsight/murphy-diagrams/,"At the recent International Symposium on Forecasting held in Riverside California Tillman Gneiting gave a great talk on “Evaluating forecasts: why proper scoring rules and consistent scoring functions matter”. It will be the subject of an IJF invited paper in due course.
One of the things he talked about was the “Murphy diagram” for comparing forecasts as proposed in Ehm et al (2015). Here’s how it works for comparing mean forecasts."
2015,7,1,Useful tutorials,https://robjhyndman.com/hyndsight/useful-tutorials/,"There are some tools that I use regularly and I would like my research students and post-docs to learn them too. Here are some great online tutorials that might help.
  ggplot tutorial from Winston Chang
  Writing an R package from Karl Broman
  Rmarkdown from RStudio
  Shiny from RStudio
  git/github guide from Karl Broman
  minimal make tutorial from Karl Broman"
2015,6,29,Exploring the feature space of large collections of time series,https://robjhyndman.com/seminars/banff2015/,"Work­shop on Fron­tiers in Func­tional Data Analy­sis Banff Canada.
It is becoming increasingly common for organizations to collect very large amounts of data over time. Data visualization is essential for exploring and understanding structures and patterns and to identify unusual observations. However the sheer quantity of data available challenges current time series visualisation methods.
For example Yahoo has banks of mail servers that are monitored over time. Many measurements on server performance are collected every hour for each of thousands of servers."
2015,6,29,My Yahoo talk is now online,https://robjhyndman.com/hyndsight/yahoo2015/,Last week I gave a talk in the Yahoo! Big Thinkers series. The video of the talk is now online and embedded below.
2015,6,27,Keeping up to date with my research papers,https://robjhyndman.com/hyndsight/new-papers/,"Many people ask me to let them know when I write a new research paper. I can&rsquo;t do that as there are too many people involved and it is not scalable.
The solution is simple. Take your pick from the following options. Each is automatic and will let you know whenever I produce a new paper.
  Subscribe to the rss feed on my website using feedly or some other rss reader."
2015,6,26,"Exploring the boundaries of predictability: what can we forecast, and when should we give up?",https://robjhyndman.com/seminars/yahoo2015/,"Yahoo Big Thinkers Sunnyvale California
Friday 26 June 2015 3:00-4:00 pm Location: Yahoo Sunnyvale Campus and LIVE at labs.yahoo.com
Why is it that we can accurately forecast a solar eclipse in 1000 years time but we have no idea whether Yahoo&rsquo;s stock price will rise or fall tomorrow? Or why can we forecast electricity consumption next week with remarkable precision but we cannot forecast exchange rate fluctuations in the next hour?"
2015,6,24,Automatic algorithms for time series forecasting,https://robjhyndman.com/seminars/google2015/,"Google Mountain View California.
Many applications require a large number of time series to be forecast completely automatically. For example manufacturing companies often require weekly forecasts of demand for thousands of products at dozens of locations in order to plan distribution and maintain suitable inventory stocks. In these circumstances it is not feasible for time series models to be developed for each series by an experienced analyst. Instead an automatic forecasting algorithm is required."
2015,6,23,IJF best paper awards,https://robjhyndman.com/hyndsight/ijf-best-paper-awards/,"Today at the International Symposium on Forecasting I announced the awards for the best paper published in the International Journal of Forecasting in the period 2012-2013.
We make an award every two years to the best paper(s) published in the journal. There is always about 18 months delay after the publication period to allow time for reflection citations etc. The selected papers are selected by vote of the editorial board. The best paper wins an engraved bronze plaque and US$1000."
2015,6,22,MEFM: An R package for long-term probabilistic forecasting of electricity demand,https://robjhyndman.com/seminars/isf2015/,"International Symposium on Forecasting Riverside California
I will describe and demonstrate a new open-source R package that implements the Monash Electricity Forecasting Model a semi-parametric probabilistic approach to forecasting long-term electricity demand. The underlying model proposed in Hyndman and Fan (2010) is now widely used in practice particularly in Australia. The model has undergone many improvements and developments since it was first proposed and these have been incorporated in this R implementation."
2015,6,18,Probabilistic forecasting of peak electricity demand,https://robjhyndman.com/seminars/sce2015/,"Southern California Edison Rosemead California
Electricity demand forecasting plays an important role in short-term load allocation and long-term planning for future generation facilities and transmission augmentation. It is a challenging problem because of the different uncertainties including underlying population growth changing technology economic conditions prevailing weather conditions (and the timing of those conditions) as well as the general randomness inherent in individual usage. It is also subject to some known calendar effects due to the time of day day of week time of year and public holidays."
2015,6,16,North American seminars: June 2015,https://robjhyndman.com/hyndsight/northamerica2015/,"For the next few weeks I am travelling in North America and will be giving the following talks.
  **19 June: **Southern California Edison Rosemead CA. &ldquo;Probabilistic forecasting of peak electricity demand&rdquo;.
  23 June: International Symposium on Forecasting Riverside CA. &ldquo;MEFM: An R package for long-term probabilistic forecasting of electricity demand&rdquo;.
  **25 June: **Google Mountain View CA. &ldquo;Automatic algorithms for time series forecasting&rdquo;.
  **26 June: **Yahoo Sunnyvale CA."
2015,6,10,Do human rhinovirus infections and food allergy modify grass pollen–induced asthma hospital admissions in children?,https://robjhyndman.com/publications/jaci2015/,
2015,6,4,Probabilistic time series forecasting with boosted additive models: an application to smart meter data,https://robjhyndman.com/publications/kdd2015/,A large body of the forecasting literature so far has been focused on forecasting the conditional mean of future observations. However there is an increasing need for generating the entire conditional distribution of future observations in order to effectively quantify the uncertainty in time series data. We present two different methods for probabilistic time series forecasting that allow the inclusion of a possibly large set of exogenous variables. One method is based on forecasting both the conditional mean and variance of the future distribution using a traditional regression approach.
2015,6,3,R vs Autobox vs ForecastPro vs ...,https://robjhyndman.com/hyndsight/show-me-the-evidence/,"Every now and then a commercial software vendor makes claims on social media about how their software is so much better than the forecast package for R but no details are provided.
There are lots of reasons why you might select a particular software solution and R isn&rsquo;t for everyone. But anyone claiming superiority should at least provide some evidence rather than make unsubstantiated claims.The M3 forecasting competition was organized by Spyros Makridakis and Michèle Hibon."
2015,6,1,Large-scale unusual time series detection,https://robjhyndman.com/publications/icdm2015/,"It is becoming increasingly common for organizations to collect very large amounts of data over time and to need to detect unusual or anomalous time series. For example Yahoo has banks of mail servers that are monitored over time. Many measurements on server performance are collected every hour for each of thousands of servers. We wish to identify servers that are behaving unusually.
We compute a vector of features on each time series measuring characteristics of the series."
2015,5,31,A new R package for detecting unusual time series,https://robjhyndman.com/hyndsight/anomalous/,"The anomalous package provides some tools to detect unusual time series in a large collection of time series. This is joint work with Earo Wang (an honours student at Monash) and Nikolay Laptev (from Yahoo Labs). Yahoo is interested in detecting unusual patterns in server metrics. The package is based on this paper with Earo and Nikolay.
The basic idea is to measure a range of features of the time series (such as strength of seasonality an index of spikiness first order autocorrelation etc."
2015,5,26,Visualization of big time series data,https://robjhyndman.com/seminars/big-time-series/,"Talk given to a joint meeting of the Statistical Society of Australia (Victorian branch) and the Melbourne Data Science Meetup Group.It is becoming increasingly common for organizations to collect very large amounts of data over time. Data visualization is essential for exploring and understanding structures and patterns and to identify unusual observations. However the sheer quantity of data available challenges current time series visualisation methods.
For example Yahoo has banks of mail servers that are monitored over time."
2015,5,22,Probabilistic forecasting of long-term peak electricity demand,https://robjhyndman.com/seminars/memsi/,Electricity demand forecasting plays an important role in long-term planning for future generation facilities and transmission augmentation. It is a challenging problem because of the different uncertainties including underlying population growth changing technology economic conditions prevailing weather conditions (and the timing of those conditions) as well as the general randomness inherent in individual usage. It is also subject to some known calendar effects due to the time of day day of week time of year and public holidays.
2015,5,19,Modelling the participation function with a one-parameter family of cubic splines,https://robjhyndman.com/publications/cubicsplinesiti/,We suggest that a simple one-parameter family of cubic spline functions would serve quite adequately as models of the participation curve that is the key component of ITI calculations. This would remove the subjectivity associated with the use of two-parameter logistic functions and would allow all states to use the same method for ITI calculations.
2015,5,15,New in forecast 6.0,https://robjhyndman.com/hyndsight/forecast6/,"This week I uploaded a new version of the forecast package to CRAN. As there were a lot of changes I decided to increase the version number to 6.0.
The changes are all outlined in the ChangeLog file as usual. I will highlight some of the more important changes since v5.0 here.ETS One of the most used functions in the package is ets() and it provides a stock forecasting engine for many organizations."
2015,5,14,More changes to the IJF editorial board,https://robjhyndman.com/hyndsight/ijf-board/,The editorial board of the International Journal of Forecasting is going through a renewal process with several changes to the team of editors and the team of associate editors in the last few weeks.New Editors Graham Elliott has decided to step down from the IJF editorial board after many years of service. Graham is best known for his research on optimal forecast combination and forecasting under asymmetric and flexible loss functions.
2015,5,7,Nominations for IJF Best Paper 2012-2013,https://robjhyndman.com/hyndsight/ijf-nominations2/,The following papers have been nominated for the best paper published in the International Journal of Forecasting in 2012-2013. I have included an excerpt from the nomination in each case. The papers in bold have been short-listed for the award and the editorial board are currently voting on them. Bellotti T. &amp; Crook J. (2012). Loss given default models incorporating macroeconomic variables for credit cards. IJF 28(1) 171-182.  &gt;The first rule for the award of best paper should be that the paper clearly reflects the value of the new method/approach when compared to established alternatives in the particular problem context chosen by the researchers.
2015,4,22,Thinking big at Yahoo,https://robjhyndman.com/hyndsight/thinking-big-at-yahoo/,"I&rsquo;m speaking in the &ldquo;Yahoo Labs Big Thinkers&rdquo; series on Friday 26 June. I hope I can live up to the title!
My talk is on &ldquo;Exploring the boundaries of predictability: what can we forecast and when should we give up?&quot; Essentially I will start with some of the ideas in this post and then discuss the features of hard-to-forecast time series.
So if you&rsquo;re in the San Francisco Bay area please come along."
2015,4,22,Travelling Thilaksha,https://robjhyndman.com/hyndsight/travelling-thilaksha/,"One of my PhD students Thilaksha Tharanganie has been very successful in getting travel funding to attend conferences. She was the subject of a write-up in today&rsquo;s Monash News.
We encourage students to attend conferences and provide funding for them to attend one international conference and one local conference during their PhD candidature. Thilaksha was previously funded to attend last year&rsquo;s COMPSTAT in Geneva Switzerland and IMS conference in Sydney. Having exhausted local funding she has now convinced several other organizations to support her conference habit."
2015,4,10,Feeling the FPP love,https://robjhyndman.com/hyndsight/fpp-reviews/,It is now exactly 12 months since the print version of my forecasting textbook with George Athanasopoulos was released on Amazon.com. Although the book is freely available online it seems that a lot of people still like to buy print books.It&rsquo;s nice to see that it has been getting some good reviews. It is rated 4.6 stars on Amazon.com with 6 out of 8 reviewers giving it 5 stars (the 3 reviewers on Amazon.
2015,4,9,Paperpile makes me more productive,https://robjhyndman.com/hyndsight/paperpile/,"One of the first things I tell my new research students is to use a reference management system to help them keep track of the papers they read and to assist in creating bib files for their bibliography. Most of them use Mendeley one or two use Zotero. Both do a good job and both are free.
I use neither. I did use Mendeley for several years but it became slower and slower to sync as my reference collection grew."
2015,4,8,Help,https://robjhyndman.com/hyndsight/help/,"This is not a help service for all your R and forecasting questions so please don&rsquo;t post questions in the comments or send them to me by email.
If you have questions about data analysis ask for help on crossvalidated.com.
If you have questions about R ask for help on stackoverflow.com.
If you think you have found a bug in one of my R packages report it on github as explained here."
2015,4,3,Discussion of “High-dimensional autocovariance matrices and optimal linear prediction”,https://robjhyndman.com/publications/mpcomments/,
2015,4,1,A new open source data set for detecting time series outliers,https://robjhyndman.com/hyndsight/yahoo-data/,Yahoo Labs has just released an interesting new data set useful for research on detecting anomalies (or outliers) in time series data. There are many contexts in which anomaly detection is important. For Yahoo the main use case is in detecting unusual traffic on Yahoo servers.The data set comprises real traffic to Yahoo services along with some synthetic data. There are 367 time series in the data set each of which contains between 741 and 1680 observations recorded at regular intervals.
2015,3,31,Change to the IJF editors,https://robjhyndman.com/publications/change-to-the-ijf-editors/,
2015,3,25,What to cite?,https://robjhyndman.com/hyndsight/what-to-cite/,"This question comes from a comment on another post:
 I&rsquo;ve seen authors citing as many references as possible to try to please potential referees. Many of those references are low quality papers though. Any general guidance about a typical length for the reference section?
 It depends on the subject and style of the paper. I&rsquo;ve written a paper with over 900 citations but that was a review of time series forecasting over a 25 year period and so it had to include a lot of references."
2015,3,18,Dark themes for writing,https://robjhyndman.com/hyndsight/dark-themes-for-writing/,"I spend much of my day sitting in front of a screen coding or writing. To limit the strain on my eyes I use a dark theme as much as possible. That is I write with light colored text on a dark background. I don&rsquo;t know why this is not the default in more software as it makes a big difference after a few hours of writing.
Most of the time I am writing using either Sublime Text RStudio or TeXstudio."
2015,3,12,Common reasons for rejection,https://robjhyndman.com/hyndsight/ijf-rejections/,Every week I reject some papers submitted to the International Journal of Forecasting without sending the papers off to associate editors or reviewers. Here are five of the most common reasons for rejection.1. Wrong Journal Submissions to the IJF should be about forecasting obviously. But we often get papers on econometrics or time series analysis or something else that is not forecasting. Even if a paper has some implications for forecasting if these are not discussed at all the paper is not within scope for the journal.
2015,2,23,Visualization and forecasting of big time series data,https://robjhyndman.com/seminars/big-time-series-data/,Talk given at the ACEMS Big data workshop QUT.
2015,2,22,Statistical modelling and analysis of big data,https://robjhyndman.com/hyndsight/bigdata2015/,"I&rsquo;m currently attending the one day workshop on this topic at QUT in Brisbane. This morning I spoke on &ldquo;Visualizing and forecasting big time series data&rdquo;. My slides are here.
OVERVIEW Big data is now endemic in business industry government environmental management medical science social research and so on. One of the commensurate challenges is how to effectively model and analyse these data.
This workshop will bring together national and international experts in statistical modelling and analysis of big data to share their experiences approaches and opinions about future directions in this field."
2015,2,9,Thanks Paul and welcome Dilek,https://robjhyndman.com/hyndsight/ijf-change-of-editors/,Today there is a change in editors at the International Journal of Forecasting. Paul Goodwin is retiring from the editorial board and Dilek Önkal is taking his place.Paul Goodwin was appointed as an associate editor in 1999 and as an editor in 2010. Paul is retiring from his position as Professor of Management Science at the University of Bath UK and has decided to also retire from the IJF editorial board.
2015,2,4,Standard error: a poem,https://robjhyndman.com/hyndsight/standard-error-poem/,"This poem was written by David Goddard from the Monash University Department of Epidemiology and Preventive Medicine. It is reproduced here with his permission. The poem won the inaugural Monash University poetry competition and will soon be published in an anthology of contemporary poetry.For those who like this sort of thing (as I do) there is a nice collection of statistical poetry here.
Standard error David Gordon Goddard
An inference that’s very often made –"
2015,2,2,IASC Data Analysis Competition 2015,https://robjhyndman.com/hyndsight/iasc-competition-2015/,The International Association for Statistical Computing (IASC) is holding a Data Analysis Competition. Winners will be invited to present their work at the Joint Meeting of IASC-ABE Satellite Conference for the 60th ISI WSC 2015 to be held at Atlântico Búzios Convention &amp; Resort in Búzios RJ Brazil (August 2-4 2015). They will also be invited to submit a manuscript for possible publication (following peer review) to IASC&rsquo;s official journal Computational Statistics &amp; Data Analysis.
2015,1,23,RSS feeds for statistics and related journals,https://robjhyndman.com/hyndsight/rss-feeds/,I&rsquo;ve now resurrected the collection of research journals that I follow and set it up as a shared collection in feedly. So anyone can easily subscribe to all of the same journals or select a subset of them to follow on feedly.There are about 90 journals on the list mostly in statistics but some from machine learning operations research and econometrics. I excluded probability journals and areas of application that are well outside my research interests (such as bioinformatics psychology and pharmacology).
2015,1,12,Visualizing and forecasting big time series data,https://robjhyndman.com/seminars/visualizing-and-forecasting-big-time-series-data/,"Institute of Statistical Science Academia Sinica 時　間 2015/01/12 11:00 星期一 地　點 中研院-統計所 2F 交誼廳 備　註 茶 會：上午10：40統計所二樓交誼廳
Time series can often be naturally disaggregated in a hierarchical or grouped structure. For example a manufacturing company can disaggregate total demand for their products by country of sale retail outlet product type package size and so on. As a result there can be millions of individual time series to forecast at the most disaggregated level plus additional series to forecast at higher levels of aggregation."
2015,1,5,Seminars in Taiwan,https://robjhyndman.com/hyndsight/seminars-in-taiwan/,I&rsquo;m currently visiting Taiwan and I&rsquo;m giving two seminars while I&rsquo;m here &mdash; one at the National Tsing Hua University in Hsinchu and the other at Academia Sinica in Taipei. Details are below for those who might be nearby.Automatic Time Series Forecasting College of Technology Management Institute of Service Science National Tsing Hua University Hsinchu 時間及地點：2015.1.7 (Wed.) 5pm @ TSMC building 6F room 622. 台積館6F孫運璿紀念中心 Many applications require a large number of time series to be forecast completely automatically.
2014,12,24,Di Cook is moving to Monash,https://robjhyndman.com/hyndsight/di-cook-is-moving-to-monash/,"I&rsquo;m delighted that Professor Dianne Cook will be joining Monash University in July 2015 as a Professor of Business Analytics. Di is an Australian who has worked in the US for the past 25 years mostly at Iowa State University. She is moving back to Australia and joining the Department of Econometrics and Business Statistics in the Monash Business School as part of our initiative in Business Analytics.
Di is a world leader in data visu­al­iza­tion and is well-​​known for her work on inter­ac­tive graph­ics."
2014,12,17,New R package for electricity forecasting,https://robjhyndman.com/hyndsight/mefm/,Shu Fan and I have developed a model for electricity demand forecasting that is now widely used in Australia for long-term forecasting of peak electricity demand. It has become known as the &ldquo;Monash Electricity Forecasting Model&rdquo;. We have decided to release an R package that implements our model so that other people can easily use it. The package is called &ldquo;MEFM&rdquo; and is available on github. We will probably also put in on CRAN eventually.
2014,12,9,Am I a data scientist?,https://robjhyndman.com/hyndsight/am-i-a-data-scientist/,"Last night I gave a very short talk (less than 5 minutes) at the Melbourne Analytics Charity Christmas Gala a combined event of the Statistical Society of Australia Data Science Melbourne Big Data Analytics and Melbourne Users of R Network.
This is (roughly) what I said.Statisticians seem to go through regular periods of existential crisis as they worry about other groups of people who do data analysis. A common theme is: all these other people (usually computer scientists) are doing our job!"
2014,12,8,Honoring Herman Stekler,https://robjhyndman.com/hyndsight/honoring-herman-stekler/,"The first issue of the IJF for 2015 has just been published and I&rsquo;m delighted that it includes a special section honoring Herman Stekler. It includes articles covering a range of his forecasting interests although not all of them (sports forecasting is missing). Herman himself wrote a paper for it looking at &ldquo;Forecasting—Yesterday Today and Tomorrow&rdquo;.
He is in a unique position to write such a paper as he has been doing forecasting research longer than anyone else on the planet &mdash; his first published paper on forecasting appeared in 1959."
2014,12,8,Am I a data scientist?,https://robjhyndman.com/seminars/christmasgala2014/,"Talk given at the Melbourne Analytics Charity Christmas Gala a combined event of the Statistical Society of Australia Data Science Melbourne Big Data Analytics and Melbourne Users of R Network.
This is (roughly) what I said.Statisticians seem to go through regular periods of existential crisis as they worry about other groups of people who do data analysis. A common theme is: all these other people (usually computer scientists) are doing our job!"
2014,12,5,Prediction competitions,https://robjhyndman.com/hyndsight/prediction-competitions/,"Competitions have a long history in forecasting and prediction and have been instrumental in forcing research attention on methods that work well in practice. In the forecasting community the M competition and M3 competition have been particularly influential. The data mining community have the annual KDD cup which has generated attention on a wide range of prediction problems and associated methods. Recent KDD cups are hosted on kaggle.
In my research group meeting today we discussed our (limited) experiences in competing in some Kaggle competitions and we reviewed the following two papers which describe two prediction competitions:"
2014,11,27,New Australian data on the HMD,https://robjhyndman.com/hyndsight/hmd-australia/,"The Human Mortality Database is a wonderful resource for anyone interested in demographic data. It is a carefully curated collection of high quality deaths and population data from 37 countries all in a consistent format with consistent definitions. I have used it many times and never cease to be amazed at the care taken to maintain such a great resource.
The data are continually being revised and updated. Today the Australian data has been updated to 2011."
2014,11,21,Visualization of probabilistic forecasts,https://robjhyndman.com/hyndsight/visualization-of-probabilistic-forecasts/,"This week my research group discussed Adrian Raftery’s recent paper on “Use and Communication of Probabilistic Forecasts” which provides a fascinating but brief survey of some of his work on modelling and communicating uncertain futures. Coincidentally today I was also sent a copy of David Spiegelhalter’s paper on “Visualizing Uncertainty About the Future”. Both are well-worth reading.
It made me think about my own efforts to communicate future uncertainty through graphics."
2014,11,11,IJF review papers,https://robjhyndman.com/hyndsight/ijf-review-papers/,Review papers are extremely useful for new researchers such as PhD students or when you want to learn about a new research field. The International Journal of Forecasting produced a whole review issue in 2006 and it contains some of the most highly cited papers we have ever published. Now beginning with the latest issue of the journal we have started publishing occasional review articles on selected areas of forecasting. The first two articles are:
2014,11,7,Seasonal periods,https://robjhyndman.com/hyndsight/seasonal-periods/,"I get questions about this almost every week. Here is an example from a recent comment on this blog:
 I have two large time series data. One is separated by seconds intervals and the other by minutes. The length of each time series is 180 days. I’m using R (3.1.1) for forecasting the data. I’d like to know the value of the “frequency” argument in the ts() function in R for each data set."
2014,11,5,ABS seasonal adjustment update,https://robjhyndman.com/hyndsight/abs-seasonal-adjustment-3/,"Since my last post on the seasonal adjustment problems at the Australian Bureau of Statistics I&rsquo;ve been working closely with people within the ABS to help them resolve the problems in time for tomorrow&rsquo;s release of the October unemployment figures.
Now that the ABS has put out a statement about the problem I thought it would be useful to explain the underlying methodology for those who are interested.The Labour Force Survey The unemployment rate is derived from the monthly Labour Force Survey."
2014,10,31,Jobs at Amazon,https://robjhyndman.com/hyndsight/jobs-at-amazon/,"I do not normally post job adverts but this was very specifically targeted to &ldquo;applied time series candidates&rdquo; so I thought it might be of sufficient interest to readers of this blog.Here is an excerpt from an email I received from someone at Amazon:
 Amazon is aggressively recruiting in the data sciences and we have found that applied economists compare quite favorably with the machine learning specialists and statisticians that are sometimes recruited for such roles."
2014,10,22,Prediction intervals too narrow,https://robjhyndman.com/hyndsight/narrow-pi/,Almost all prediction intervals from time series models are too narrow. This is a well-known phenomenon and arises because they do not account for all sources of uncertainty. In my 2002 IJF paper we measured the size of the problem by computing the actual coverage percentage of the prediction intervals on hold-out samples. We found that for ETS models nominal 95% intervals may only provide coverage between 71% and 87%.
2014,10,20,Optimally reconciling forecasts in a hierarchy,https://robjhyndman.com/publications/foresight-hts/,"This is an introduction to our approach to forecast reconciliation without using any matrices. The original research is available here:
 Hyndman Ahmed Athanasopoulos and Shang (CSDA 2011) Athanasopoulos Ahmed and Hyndman (IJF 2009)  The software is available in the hts package for R with some notes on usage in the vignette. There is also a gentle introduction in our forecasting textbook."
2014,10,20,hts with regressors,https://robjhyndman.com/hyndsight/hts-with-regressors/,"The hts package for R allows for forecasting hierarchical and grouped time series data. The idea is to generate forecasts for all series at all levels of aggregation without imposing the aggregation constraints and then to reconcile the forecasts so they satisfy the aggregation constraints. (An introduction to reconciling hierarchical and grouped time series is available in this Foresight paper.)
The base forecasts can be generated using any method with ETS models and ARIMA models provided as options in the forecast."
2014,10,15,Congratulations to Dr Souhaib Ben Taieb,https://robjhyndman.com/hyndsight/souhaib/,"Souhaib Ben Taieb has been awarded his doctorate at the Université libre de Bruxelles and so he is now officially Dr Ben Taieb! Although Souhaib lives in Brussels and was a student at the Université libre de Bruxelles I co-supervised his doctorate (along with Professor Gianluca Bontempi). Souhaib is the 19th PhD student of mine to graduate.
His thesis was on &ldquo;Machine learning strategies for multi-step-ahead time series forecasting&rdquo; and is now available online."
2014,10,10,Explaining the ABS unemployment fluctuations,https://robjhyndman.com/hyndsight/abs-seasonal-adjustment-2/,"Although the Guardian claimed yesterday that I had explained “what went wrong” in the July and August unemployment figures I made no attempt to do so as I had no information about the problems. Instead I just explained a little about the purpose of seasonal adjustment.
However today I learned a little more about the ABS unemployment data problems including what may be the explanation for the fluctuations. This explanation was offered by Westpac’s chief economist Bill Evans (see here for a video of him explaining the issue)."
2014,10,9,Seasonal adjustment in the news,https://robjhyndman.com/hyndsight/abs-seasonal-adjustment/,"It&rsquo;s not every day that seasonal adjustment makes the front page of the newspapers but it has today with the ABS saying that the recent seasonally adjusted unemployment data would be revised.
I was interviewed about the underlying concepts for the Guardian in this piece.
Further comment from me about users paying for the ABS data is here."
2014,10,7,Connect with local employers,https://robjhyndman.com/hyndsight/connect-with-local-employers/,I keep telling students that there are lots of jobs in data science (including statistics) and they often tell me they can&rsquo;t find them advertised. As usual you do have to do some networking and one of the best ways of doing it is via a Data Science Meetup. Many cities now have them including Melbourne Sydney London etc. It is the perfect opportunity to meet with local employers many of which are hiring due to the huge expansion in the use of data analysis in business (aka business analytics).
2014,10,6,IIF Sponsored Workshops,https://robjhyndman.com/hyndsight/iif-workshops/,The International Institute of Forecasters sponsors workshops every year each of which focuses on a specific theme. The purpose of these workshops is to facilitate small informal meetings where experts in a particular field of forecasting can discuss forecasting problems research and solutions. Over the years our workshops have covered topics from Predicting Rare Events ICT Forecasting and most recently Singular Spectrum Analysis. Often these workshops are associated with a special issue of the International Journal of Forecasting.
2014,10,6,TBATS with regressors,https://robjhyndman.com/hyndsight/tbats-with-regressors/,"I&rsquo;ve received a few emails about including regression variables (i.e. covariates) in TBATS models. As TBATS models are related to ETS models tbats() is unlikely to ever include covariates as explained here. It won&rsquo;t actually complain if you include an xreg argument but it will ignore it.
When I want to include covariates in a time series model I tend to use auto.arima() with covariates included via the xreg argument. If the time series has multiple seasonal periods I use Fourier terms as additional covariates."
2014,9,23,Forecasting: principles and practice (UWA course),https://robjhyndman.com/seminars/uwa/,"Workshop to be held on 23-25 September 2014.
Venue: The University Club University of Western Australia Nedlands WA.
Requirements: a laptop with R installed along with the fpp package and its dependencies. We will also use the hts and vars package on the third day.
Textbook [Hyndman and Athanasopoulos (2014)
Forecasting: principles and practice OTexts: Melbourne Australia.
Program  Introduction to forecasting [Slides R code Lab solutions] Forecasting tools [Slides R code Lab solutions] Exponential smoothing I [Slides R code Lab solutions] Exponential smoothing II [Slides R code Lab solutions] Time series decomposition and cross-validation [Slides R code Lab solutions] Transformations stationarity and differencing [Slides R code Lab solutions] Non-seasonal ARIMA models [Slides R code Lab solutions] Seasonal ARIMA models [Slides R code Lab solutions] State space models [Slides R code Lab solutions] Dynamic regression [Slides R code Lab solutions] Hierarchical forecasting [Slides R code Lab solutions] Advanced methods [Slides R code Lab solutions]  Course Notes"
2014,9,21,FPP now available as a downloadable e-book,https://robjhyndman.com/hyndsight/fpp-e-book/,"My forecasting textbook with George Athanasopoulos is already available online (for free) and in print via Amazon (for under $40). Now we have made it available as a downloadable e-book via Google Books (for $15.55). The Google Books version is identical to the print version on Amazon (apart from a few typos that have been fixed).
To use the e-book version on an iPad or Android tablet you need to have the Google Books app installed [iPad Android]."
2014,9,8,Tim Harford on forecasting,https://robjhyndman.com/hyndsight/tim-harford-on-forecasting/,A few weeks ago I had a Skype chat with Tim Harford the &ldquo;Undercover Economist&rdquo; for Britain&rsquo;s Financial Times. He was working on an article for the FT on forecasting and wanted my perspective as an academic forecaster. I mostly talked about what makes some things more predictable than others as discussed in this blog post. In the end his article headed in a different direction so I don&rsquo;t get quoted but it is still a good read!
2014,9,8,Generating quantile forecasts in R,https://robjhyndman.com/hyndsight/quantile-forecasts-in-r/,"From today’s email:
 I have just finished reading a copy of ‘Forecasting:Principles and Practice’ and I have found the book really interesting. I have particularly enjoyed the case studies and focus on practical applications.
After finishing the book I have joined a forecasting competition to put what I’ve learnt to the test. I do have a couple of queries about the forecasting outputs required. The output required is a quantile forecast is this the same as prediction intervals?"
2014,9,3,Resources for the FPP book,https://robjhyndman.com/hyndsight/fpp-resources/,"The FPP resources page has recently been updated with several new additions including
  R code for all examples in the book. This was already available within each chapter but the examples have been collected into one file per chapter to save copying and pasting the various code fragments.
  Slides from a course on Predictive Analytics from the University of Sydney.
  Slides from a course on Economic Forecasting from the University of Hawaii."
2014,9,1,A new candidate for worst figure,https://robjhyndman.com/hyndsight/worst-figure/,"Today I read a paper that had been submitted to the IJF which included the following figure

along with several similar plots. (Click for a larger version.) I haven&rsquo;t seen anything this bad for a long time. In fact I think I would find it very difficult to reproduce using R or even Excel (which is particularly adept at bad graphics).
A few years ago I produced &ldquo;Twenty rules for good graphics&rdquo;."
2014,9,1,Outdoor fungal spores are associated with child asthma hospitalisations - a case-crossover study,https://robjhyndman.com/publications/fungal-asthma/,"Introduction Asthma can be exacerbated by exposure to various fungal spores and Human Rhinovirus [HRV] but current understanding of the importance of fungal exposure to child asthma hospitalisations is limited. Moreover the interaction between HRV and fungal spore exposure on admission has not been examined.
Aim To investigate the role of outdoor fungal spores in child asthma hospitalisations and if HRV modifies any such effect.
Methods We conducted a case-crossover study of 644 child asthma hospitalisations in Melbourne Australia (2009–11)."
2014,8,24,Forecasting with R in WA,https://robjhyndman.com/hyndsight/forecasting-with-r-in-wa/,"On 23-25 September I will be running a 3-day workshop in Perth on &ldquo;Forecasting: principles and practice&rdquo; mostly based on my book of the same name.
Workshop participants will be assumed to be familiar with basic statistical tools such as multiple regression but no knowledge of time series or forecasting will be assumed. Some prior experience in R is highly desirable.
Venue: The University Club University of Western Australia Nedlands WA."
2014,8,22,biblatex for statisticians,https://robjhyndman.com/hyndsight/biblatex-for-statisticians/,I am now using biblatex for all my bibliographic work as it seems to have developed enough to be stable and reliable. The big advantage of biblatex is that it is easy to format the bibliography to conform to specific journal or publisher styles. It is also possible to have structured bibliographies (e.g. divided into sections: books papers R packages etc.) Here is my default setting which should be suitable for almost all statistics and econometrics journals.
2014,8,18,GEFCom 2014 energy forecasting competition is underway,https://robjhyndman.com/hyndsight/gefcom-2014/,"GEFCom 2014 is the most advanced energy forecasting competition ever organized both in terms of the data involved and in terms of the way the forecasts will be evaluated.
So everyone interested in energy forecasting should head over to the competition webpage and start forecasting: www.gefcom.org.
This time the competition is hosted on CrowdANALYTIX rather than Kaggle.
Highlights of GEFCom2014:
  An upgraded edition from GEFCom2012
  Four tracks: electric load electricity price wind power and solar power forecasting."
2014,8,12,Visit of Di Cook,https://robjhyndman.com/hyndsight/visit-of-di-cook/,"Next week Professor Di Cook from Iowa State University is visiting my research group at Monash University. Di is a world leader in data visualization and is especially well-known for her work on interactive graphics and the XGobi and GGobi software. See her book with Deb Swayne for details.
For those wanting to hear her speak read on.Research seminar She will be giving a seminar at 2pm on Monday 18 August at the Monash Clayton campus (Rm E457 Menzies Building 11)."
2014,8,12,What not to say in a job interview,https://robjhyndman.com/hyndsight/what-not-to-say-in-a-job-interview/,"I&rsquo;ve interviewed a few people for jobs at Monash University and there&rsquo;s always someone who comes out with something surprising. Here are some real examples.For a post-doctoral research position:
  Q: What would you say were your major weaknesses?
  A: I don&rsquo;t have any.
  Q: Really? You can&rsquo;t think of anything that you could work on new skills you could develop anything at all that you might be able to improve?"
2014,8,11,Minimal reproducible examples,https://robjhyndman.com/hyndsight/minimal-reproducible-examples/,I occasionally get emails from people thinking they have found a bug in one of my R packages and I usually have to reply asking them to provide a minimal reproducible example (MRE). This post is to provide instructions on how to create a MRE.Bug reports on github not email First if you think there is a bug please don&rsquo;t send me emails. Instead use the bug-reporting facility on github. All eight of my R packages that are on CRAN have pre-release versions on github.
2014,8,1,Efficient identification of the Pareto optimal set,https://robjhyndman.com/publications/epic/,In this paper we focus on expensive multiobjective optimization problems and propose a method to predict an approximation of the Pareto optimal set using classification of sampled decision vectors as dominated or nondominated. The performance of our method called EPIC is demonstrated on a set of benchmark problems used in the multiobjective optimization literature and compared with state-of-the-art methods ParEGO and PAL. The initial results are promising and encourage further research in this direction.
2014,7,26,Student forecasting awards from the IIF,https://robjhyndman.com/hyndsight/iif-awards/,"At the IIF annual board meeting last month in Rotterdam I suggested that we provide awards to the top students studying forecasting at university level around the world to the tune of $100 plus IIF membership for a year. I&rsquo;m delighted that the idea met with enthusiasm and that the awards are now available. Even better my second year forecasting subject has been approved for an award.
The IIF have agreed to fund awards for 20 forecasting courses to start with."
2014,7,24,Coherent population forecasting using R,https://robjhyndman.com/hyndsight/coherent-population-forecasting/,"This is an example of how to use the demography package in R for stochastic population forecasting with coherent components. It is based on the papers by Hyndman and Booth (IJF 2008) and Hyndman Booth and Yasmeen (Demography 2013). I will use Australian data from 1950 to 2009 and forecast the next 50 years.
In demography &ldquo;coherent&rdquo; forecasts are where male and females (or other sub-groups) do not diverge over time."
2014,7,23,Plotting the characteristic roots for ARIMA models,https://robjhyndman.com/hyndsight/arma-roots/,When modelling data with ARIMA models it is sometimes useful to plot the inverse characteristic roots. The following functions will compute and plot the inverse roots for any fitted ARIMA model (including seasonal models).# Compute AR roots arroots &lt;- function(object) { if(!(&quot;Arima&quot; %in% class(object)) &amp; !(&quot;ar&quot; %in% class(object))) stop(&quot;object must be of class Arima or ar&quot;) if(&quot;Arima&quot; %in% class(object)) parvec &lt;- object$model$phi else parvec &lt;- object$ar if(length(parvec) &gt; 0) { last.
2014,7,21,I am not an econometrician,https://robjhyndman.com/hyndsight/statistics-vs-econometrics/,I am a statistician but I have worked in a department of predominantly econometricians for the past 17 years. It is a little like an Australian visiting the United States. Initially it seems that we talk the same language do the same sorts of things and have a very similar culture. But the longer you stay there the more you realise there are differences that run deep and affect the way you see the world.
2014,7,15,Variations on rolling forecasts,https://robjhyndman.com/hyndsight/rolling-forecasts/,Rolling forecasts are commonly used to compare time series models. Here are a few of the ways they can be computed using R. I will use ARIMA models as a vehicle of illustration but the code can easily be adapted to other univariate time series models.One-step forecasts without re-estimation The simplest approach is to estimate the model on a single set of training data and then compute one-step forecasts on the remaining test data.
2014,7,1,Fast computation of reconciled forecasts in hierarchical and grouped time series,https://robjhyndman.com/seminars/hgts-2/,International Symposium on Forecasting Rotterdam.
2014,6,24,Functional time series with applications in demography,https://robjhyndman.com/seminars/fts-berlin/,"This is a short course given at Humboldt University Berlin 24-25 June 2014.
Venue: LvB Library Room 401 Spandauerstr. 1 10178 Berlin
Time: 24 June 2014 09:30 - 12:30 and 14:00 - 17:00 25 June 2014 09:30 - 11:30
Functional time series are curves that are observed sequentially in time one curve being observed in each time period. In demography examples include curves formed by annual death rates as a function of age or annual fertility rates as a function of age."
2014,6,17,Challenges in forecasting peak electricity demand,https://robjhyndman.com/seminars/swiss-energy-forum/,"I am giving a two-part seminar at the Energy Forum Valais/Wallis Switzerland on 17 June 2014. (English brochure)
Abstract: Electricity demand forecasting plays an important role in short-term load allocation and long-term planning for future generation facilities and transmission augmentation. It is a challenging problem because of the different uncertainties including underlying population growth changing technology economic conditions prevailing weather conditions (and the timing of those conditions) as well as the general randomness inherent in individual usage."
2014,6,15,Varian on big data,https://robjhyndman.com/hyndsight/varian-2014/,"Last week my research group discussed Hal Varian&rsquo;s interesting new paper on &ldquo;Big data: new tricks for econometrics&rdquo; Journal of Economic Perspectives 28(2): 3-28.
It&rsquo;s a nice introduction to trees bagging and forests plus a very brief entree to the LASSO and the elastic net and to slab and spike regression. Not enough to be able to use them but ok if you&rsquo;ve no idea what they are. It was more disappointing on boosting (completely ignoring the fact that boosting can be applied in a regression context as well as a classification context) and his comments on causality seemed curiously naive."
2014,6,15,Specifying complicated groups of time series in hts,https://robjhyndman.com/hyndsight/gts/,"With the latest version of the hts package for R it is now possible to specify rather complicated grouping structures relatively easily.
All aggregation structures can be represented as hierarchies or as cross-products of hierarchies. For example a hierarchical time series may be based on geography: country state region store. Often there is also a separate product hierarchy: product groups product types packet size. Forecasts of all the different types of aggregation are required; e."
2014,6,14,European talks. June-July 2014,https://robjhyndman.com/hyndsight/europe2014/,"For the next month I am travelling in Europe and will be giving the following talks.
17 June. Challenges in forecasting peak electricity demand. Energy Forum Sierre Valais/Wallis Switzerland.
20 June. Common functional principal component models for mortality forecasting. International Workshop on Functional and Operatorial Statistics. Stresa Italy.
24-25 June. Functional time series with applications in demography. Humboldt University Berlin.
1 July. Fast computation of reconciled forecasts in hierarchical and grouped time series."
2014,6,11,Creating a handout from beamer slides,https://robjhyndman.com/hyndsight/beamer-handout/,"I&rsquo;m about to head off on a speaking tour to Europe (more on that in another post) and one of my hosts has asked for my powerpoint slides so they can print them. They have made two false assumptions: (1) that I use powerpoint; (2) that my slides are static so they can be printed.
Instead I produced a cut-down version of my beamer slides leaving out some of the animations and other features that will not print easily."
2014,6,5,"Low-dimensional decomposition, smoothing and forecasting of sparse functional data",https://robjhyndman.com/publications/ropes/,We propose a new generic method ROPES (Regularized Optimization for Prediction and Estimation with Sparse data) for decomposing smoothing and forecasting two-dimensional sparse data. In some ways ROPES is similar to Ridge Regression the LASSO Principal Component Analysis (PCA) and Maximum-Margin Matrix Factorisation (MMMF). Using this new approach we propose a practical method of forecasting mortality rates as well as a new method for interpolating and extrapolating sparse longitudinal data. We also show how to calculate prediction intervals for the resulting estimates.
2014,5,30,State space models,https://robjhyndman.com/seminars/abs-state-space-models/,"Exponential smoothing
 Slides R examples Lab session Solutions to lab session    Structural models
 Slides R examples Lab session Solutions to lab session    ARIMA and RegARMA models and dlm
 Slides R examples    (Updated: 2 June 2014)"
2014,5,26,Data science market places,https://robjhyndman.com/hyndsight/marketplaces/,Some new websites are being established offering &ldquo;market places&rdquo; for data science. Two I&rsquo;ve come across recently are Experfy and SnapAnalytx.Experfy provides a way for companies to find statisticians and other data scientists either for short-term consultancies or to fill full-time positions. They describe their &ldquo;providers&rdquo; as &ldquo;Data Engineers Data Scientists Data Mining Experts Data Analyst/Modelers Big Data Solutions Architects Visualization Designers Statisticians Applied Physicists Mathematicians Econometricians and Bioinformaticians.&rdquo; Data scientists can sign up as &ldquo;Providers&rdquo; companies can sign up as &ldquo;Clients&rdquo;.
2014,5,24,Common functional principal component models for mortality forecasting,https://robjhyndman.com/publications/cfpc-iwfos/,"We explore models for forecasting groups of functional time series data that exploit common features in the data. Our models involve fitting common (or partially common) functional principal component models and forecasting the coefficients using univariate time series methods. We illustrate our approach by forecasting age-specific mortality rates for males and females in Australia.
Slides for talk"
2014,5,23,Structural breaks,https://robjhyndman.com/hyndsight/structural-breaks/,"I&rsquo;m tired of reading about tests for structural breaks and here&rsquo;s why.
A structural break occurs when we see a sudden change in a time series or a relationship between two time series. Econometricians love papers on structural breaks and apparently believe in them. Personally I tend to take a different view of the world. I think a more realistic view is that most things change slowly over time and only occasionally with sudden discontinuous change."
2014,5,22,Monash Electricity Forecasting Model,https://robjhyndman.com/publications/mefm/,The model we developed for peak electricity demand forecasting in Hyndman and Fan (2010) is now widely used in practice around Australia and has undergone many improvements and developments. This document describes the current version of the model. It will be updated from time to time as the model continues to be modified and improved.
2014,5,19,To explain or predict?,https://robjhyndman.com/hyndsight/to-explain-or-predict/,"Last week my research group discussed Galit Shmueli&rsquo;s paper &ldquo;To explain or to predict?&rdquo; Statistical Science 25(3) 289-310. (See her website for further materials.) This is a paper everyone doing statistics and econometrics should read as it helps to clarify a distinction that is often blurred. In the discussion the following issues were covered amongst other things.
  The AIC is better suited to model selection for prediction as it is asymptotically equivalent to leave-one-out cross-validation in regression or one-step-cross-validation in time series."
2014,5,13,Questions on the business analytics jobs,https://robjhyndman.com/hyndsight/business-analytics-jobs-questions/,I&rsquo;ve received a few questions on the business analytics jobs advertised last week. I think it is best if I answer them here so other potential candidates can have the same information. I will add to this post if I receive more questions.1. What are your expectations in terms of outputs (KPIs)? Typically a person at Level B (Lecturer) in our department would be producing at least one refereed article in a good scholarly journal per year.
2014,5,8,ARIMA models with long lags,https://robjhyndman.com/hyndsight/arima-models-with-long-lags/,"Today&rsquo;s email question:
 I work within a government budget office and sometimes have to forecast fairly simple time series several quarters into the future. Auto.arima() works great and I often get something along the lines of: ARIMA(001)(110)[12] with drift as the lowest AICc.
  However my boss (who does not use R) takes issue with low-order AR and MA because &ldquo;you&rsquo;re essentially using forecasted data to make your forecast."
2014,5,4,New jobs in business analytics at Monash,https://robjhyndman.com/hyndsight/monash-business-analytics/,We have an exciting new initiative at Monash University with some new positions in business analytics. This is part of a plan to strengthen our research and teaching in the data science/computational statistics area. We are hoping to make multiple appointments at junior and senior levels. These are five-year appointments but we hope that the positions will continue after that if we can secure suitable funding.What is business analytics? You can think of &ldquo;business analytics&rdquo; as the application of data analysis to business problems.
2014,5,2,Great papers to read,https://robjhyndman.com/hyndsight/great-papers/,"My research group meets every two weeks. It is always fun to talk about general research issues and new tools and tips we have discovered. We also use some of the time to discuss a paper that I choose for them. Today we discussed Breiman&rsquo;s classic (2001) two cultures paper &mdash; something every statistician should read including the discussion.
I select papers that I want every member of research team to be familiar with."
2014,4,28,"Past, present, and future of statistical science",https://robjhyndman.com/hyndsight/ppfss/,"This is the title of a wonderful new book that has just been released courtesy of the Committee of Presidents of Statistical Societies.
It can be freely downloaded from the COPSS website or a hard copy can be purchased on Amazon (for only a little over 10c per page which is not bad compared to other statistics books).
The book consists of 52 chapters spanning 622 pages. The full table of contents below shows its scope and the list of authors (a veritable who&rsquo;s who in statistics)."
2014,4,24,Publishing an R package in the Journal of Statistical Software,https://robjhyndman.com/hyndsight/jss-rpackages/,I&rsquo;ve been an editor of JSS for the last few years and as a result I tend to get email from people asking me about publishing papers describing R packages in JSS. So for all those wondering here are some general comments.JSS prefers to publish papers about packages where the package is on CRAN and has been there long enough to have matured (i.e. obvious bugs ironed out and a few active users).
2014,4,22,Seven forecasting blogs,https://robjhyndman.com/hyndsight/seven-forecasting-blogs/,"There are several other blogs on forecasting that readers might be interested in. Here are seven worth following:
  No Hesitations by Francis Diebold (Professor of Economics University of Pennsylvania). Diebold needs no introduction to forecasters. He primarily covers forecasting in economics and finance but also xkcd cartoons graphics research issues etc.
  Econometrics Beat by Dave Giles. Dave is a professor of economics at the University of Victoria (Canada) formerly from my own department at Monash University (Australia) and a native New Zealander."
2014,4,16,Errors on percentage errors,https://robjhyndman.com/hyndsight/smape/,"The MAPE (mean absolute percentage error) is a popular measure for forecast accuracy and is defined as $$ \text{MAPE} = 100\text{mean}(|y_t - \hat{y}_t|/|y_t|) $$ where $y_t$ denotes an observation and $\hat{y}_t$ denotes its forecast and the mean is taken over $t$.
Armstrong (1985 p.348) was the first (to my knowledge) to point out the asymmetry of the MAPE saying that &ldquo;it has a bias favoring estimates that are below the actual values&rdquo;."
2014,4,14,Generating tables in LaTeX,https://robjhyndman.com/hyndsight/generating-tables-in-latex/,"Typing tables in LaTeX can get messy but there are some good tools to simplify the process. One I discovered this week is tablesgenerator.com a web-based tool for generating LaTeX tables. It also allows the table to saved in other formats including HTML and Markdown. The interface is simple but it does most things. For complicated tables some additional formatting may be necessary.
Similar functionality is available via plugins in Excel OpenOffice and Libreoffice &mdash; useful if the data for the table is already stored in a spreadsheet."
2014,4,9,My forecasting book now on Amazon,https://robjhyndman.com/hyndsight/fpp-amazon/,"For all those people asking me how to obtain a print version of my book &ldquo;Forecasting: principles and practice&rdquo; with George Athanasopoulos you now can.

Order on Amazon.com
Order on Amazon.co.uk
Order on Amazon.fr
The online book will continue to be freely available. The print version of the book is intended to help fund the development of the OTexts platform.
The price is US$45 £27 or €35.
Compare that to $195 for my previous forecasting textbook $150 for Fildes and Ord or $182 for Gonzalez-Rivera."
2014,4,7,Job at Center for Open Science,https://robjhyndman.com/hyndsight/cos-job/,"This looks like an interesting job.
 Dear Dr. Hyndman
  I write from the Center for Open Science a non-profit organization based in Charlottesville Virginia in the United States which is dedicated to improving the alignment between scientific values and scientific practices. We are dedicated to open source and open science.
  We are reaching out to you to find out if you know anyone who might be interested in our Statistical and Methodological Consultant position."
2014,4,6,Interpreting noise,https://robjhyndman.com/hyndsight/interpreting-noise/,"When watching the TV news or reading newspaper commentary I am frequently amazed at the attempts people make to interpret random noise.
For example the latest tiny fluctuation in the share price of a major company is attributed to the CEO being ill. When the exchange rate goes up the TV finance commentator confidently announces that it is a reaction to Chinese building contracts. No one ever says &ldquo;The unemployment rate has dropped by 0."
2014,4,4,Getting a LaTeX system set up,https://robjhyndman.com/hyndsight/latex-setup/,"Today I was teaching the honours students in econometrics and economics about LaTeX. Here are some brief instructions on how to set up a LaTeX system on different operating systems.MS-Windows  Download and run the setup program for MikTeX. Choose the “basic” system. Download and run the installer program for TeXstudio.  Then run TeXstudio and start typing.
Mac OS  Download and install MacTeX.  Then run TeXshop and start typing."
2014,4,1,A gradient boosting approach to the Kaggle load forecasting competition,https://robjhyndman.com/publications/kaggleloadforecasting/,We describe and analyse the approach used by Team TinTin (Souhaib Ben Taieb and Rob J Hyndman) in the Load Forecasting track of the Kaggle Global Energy Forecasting Competition 2012. The competition involved a hierarchical load forecasting problem for a US utility with 20 geographical zones. The available data consisted of the hourly loads for the 20 zones and hourly temperatures from 11 weather stations for four and a half years.
2014,3,18,Cover of my forecasting textbook,https://robjhyndman.com/hyndsight/fpp-cover/,"We now have a cover for the print version of my forecasting book with George Athanasopoulos.

It should be on Amazon in a couple of weeks. The book is also freely available online.
The cover was produced by Scarlett Rugers who I can happily recommend to anyone wanting a book cover designed."
2014,3,17,Fast computation of cross-validation in linear models,https://robjhyndman.com/hyndsight/loocv-linear-models/,"The leave-one-out cross-validation statistic is given by $$ \text{CV} = \frac{1}{N} \sum_{i=1}^N e_{[i]}^2 $$ where ${e_{[i]} = y_{i} - \hat{y}_{[i]}} $ the observations are given by $y_{1}\dotsy_{N}$ and $\hat{y}_{[i]}$ is the predicted value obtained when the model is estimated with the $i\text{th}$ case deleted. This is also sometimes known as the PRESS (Prediction Residual Sum of Squares) statistic.
It turns out that for linear models we do not actually have to estimate the model $N$ times once for each omitted case."
2014,3,14,Probabilistic forecasting by Gneiting and Katzfuss (2014),https://robjhyndman.com/hyndsight/gneiting/,The IJF is introducing occasional review papers on areas of forecasting. We did a whole issue in 2006 reviewing 25 years of research since the International Institute of Forecasters was established. Since then there has been a lot of new work in application areas such as call center forecasting and electricity price forecasting. In addition there are areas we did not cover in 2006 including new product forecasting and forecasting in finance.
2014,3,12,Testing for trend in ARIMA models,https://robjhyndman.com/hyndsight/arima-trends/,"Today&rsquo;s email brought this one:
 I was wondering if I could get your opinion on a particular problem that I have run into during the reviewing process of an article.
  Basically I have an analysis where I am looking at a couple of time-series and I wanted to know if over time there was an upward trend in the series. Inspection of the raw data suggests there is but we want some statistical evidence for this."
2014,3,12,Unit root tests and ARIMA models,https://robjhyndman.com/hyndsight/unit-root-tests/,"An email I received today:
 I have a small problem. I have a time series called x :
   If I use the default values of auto.arima(x) the best model is an ARIMA(100)     However I tried the function ndiffs(x test=&ldquo;adf&rdquo;) and ndiffs(x test=&ldquo;kpss&rdquo;) as the KPSS test seems to be the default value and the number of difference is 0 for the kpss test (consistent with the results of auto."
2014,3,10,Using old versions of R packages,https://robjhyndman.com/hyndsight/old-r-packages/,"I received this email yesterday:
 I have been using your ‘forecast’ package for more than a year now. I was on R version 2.15 until last week but I am having issues with lubridate package hence decided to update R version to R 3.0.1. In our organization even getting an open source application require us to go through a whole lot of approval processes. I asked for R 3.0.1 before I get approval for 3."
2014,3,7,IJF news,https://robjhyndman.com/hyndsight/ijf-news/,This is a short piece I wrote for the next issue of the Oracle newsletter produced by the International Institute of Forecasters. Special section topics We continue to publish special sections on selected topics. Because of the change in the way regular papers are now handled we do not publish whole special issues any more. Rather each issue has regular papers at the front and if there are any special sections they appear at the back.
2014,3,6,Highlighting the web,https://robjhyndman.com/hyndsight/highlighting-the-web/,Users of my new online forecasting book have asked about having a facility for personal highlighting of selected sections as students often do with print books. We have plans to make this a built-in part of the platform but for now it is possible to do it using a simple browser extension. This approach allows any website to be highlighted so is even more useful than if we only had the facility on OTexts.
2014,3,4,Forecasting weekly data,https://robjhyndman.com/hyndsight/forecasting-weekly-data/,"This is another situation where Fourier terms are useful for handling the seasonality. Not only is the seasonal period rather long it is non-integer (averaging 365.25/7 = 52.18). So ARIMA and ETS models do not tend to give good results even with a period of 52 as an approximation.
Regression with ARIMA errors The simplest approach is a regression with ARIMA errors. Here is an example using weekly data on US finished motor gasoline products supplied (in thousands of barrels per day) from February 1991 to May 2005."
2014,3,4,Fitting models to short time series,https://robjhyndman.com/hyndsight/short-time-series/,"Following my post on fitting models to long time series I thought I&rsquo;d tackle the opposite problem which is more common in business environments.
I often get asked how few data points can be used to fit a time series model. As with almost all sample size questions there is no easy answer. It depends on the number of model parameters to be estimated and the amount of randomness in the data."
2014,3,1,Fitting models to long time series,https://robjhyndman.com/hyndsight/long-time-series/,"I received this email today:
 I recall you made this very insightful remark somewhere that fitting a standard arima model with too much data ie. a very long time series is a bad idea.
  Can you elaborate why?
  I can see the issue with noise which compounds the ML estimation as the series gets too long. But is there anything else?
 I&rsquo;m not sure where I made a comment about this but it is true that ARIMA models don&rsquo;t work well for very long time series."
2014,2,25,The forecast mean after back-transformation,https://robjhyndman.com/hyndsight/backtransforming/,Many functions in the forecast package for R will allow a Box-Cox transformation. The models are fitted to the transformed data and the forecasts and prediction intervals are back-transformed. This preserves the coverage of the prediction intervals and the back-transformed point forecast can be considered the median of the forecast densities (assuming the forecast densities on the transformed scale are symmetric). For many purposes this is acceptable but occasionally the mean forecast is required.
2014,2,23,Statistical politicians,https://robjhyndman.com/hyndsight/statistical-politicians/,"Last week we had the pleasure of Professor Stephen Pollock (University of Leicester) visiting our Department best known in academic circles for his work on time series filtering (see his papers and his excellent book). But he has another career as a member of the UK House of Lords (under the name Viscount Hanworth &ndash; he is a hereditary peer).
It made me wonder how many other politicians have PhDs (or equivalent) in statistics or at least in mathematics."
2014,2,21,Forecasting within limits,https://robjhyndman.com/hyndsight/forecasting-within-limits/,It is common to want forecasts to be positive or to require them to be within some specified range \([ab]\). Both of these situations are relatively easy to handle using transformations.Positive forecasts To impose a positivity constraint simply work on the log scale. With the forecast package in R this can be handled by specifying the Box-Cox parameter \(\lambda=0\). For example consider the real price of a dozen eggs (1900-1993; in cents):
2014,2,20,Backcasting in R,https://robjhyndman.com/hyndsight/backcasting/,Sometimes it is useful to “backcast” a time series — that is forecast in reverse time. Although there are no in-built R functions to do this it is very easy to implement. Suppose x is our time series and we want to backcast for \(h\) periods. Here is some code that should work for most univariate time series. The example is non-seasonal but the code will also work with seasonal data.
2014,2,19,Global energy forecasting competitions,https://robjhyndman.com/hyndsight/gefcom2014/,"The 2012 GEFcom competition was a great success with several new innovative forecasting methods introduced. These have been published in the IJF as follows:  Hong Pinson and Fan. Global Energy Forecasting Competition 2012
  Charleton and Singleton. A refined parametric model for short term load forecasting
  Lloyd. GEFCom2012 hierarchical load forecasting: Gradient boosting machines and Gaussian processes
  Nedelec Cugliari and Goude: GEFCom2012: Electric load forecasting and backcasting with semi-parametric models"
2014,2,13,Automatic time series forecasting,https://robjhyndman.com/seminars/granada/,Talk presented at the conference &ldquo;New Trends on Intelligent Systems and Soft Computing 2014&rdquo; University of Granada Spain. 13-14 February 2014.Abstract Many applications require a large number of time series to be forecast completely automatically. For example manufacturing companies often require weekly forecasts of demand for thousands of products at dozens of locations in order to plan distribution and maintain suitable inventory stocks. In these circumstances it is not feasible for time series models to be developed for each series by an experienced analyst.
2014,2,12,Hierarchical forecasting with hts v4.0,https://robjhyndman.com/hyndsight/hts4/,A new version of my hts package for R is now on CRAN. It was completely re-written from scratch. Not a single line of code survived. There are some minor syntax changes but the biggest change is speed and scope. This version is many times faster than the previous version and can handle hundreds of thousands of time series without complaining. The speed-up is due to some new research I am doing with Alan Lee (University of Auckland).
2014,2,8,Detecting seasonality,https://robjhyndman.com/hyndsight/detecting-seasonality/,"I occasionally get email asking how to detect whether seasonality is present in a data set. Sometimes the period of the potential seasonality is known but in other cases it is not.
I’ve discussed before how to estimate an unknown seasonal period and how to measure the strength of the seasonality. In this post I want to look at testing if a series is seasonal when the potential period is known (e."
2014,2,5,Interview for the Capital of Statistics,https://robjhyndman.com/hyndsight/cos-interview/,"Earo Wang recently interviewed me for the Chinese website Capital of Statistics. The English transcript of the intervew is on Earo&rsquo;s personal website.
This is the third interview I&rsquo;ve done in the last 18 months. The others were for:
  Data Mining Research. Republished in Amstat News.
  DecisionStats."
2014,2,4,Top papers in the International Journal of Forecasting,https://robjhyndman.com/hyndsight/ijf-top-papers/,Every year or so Elsevier asks me to nominate five International Journal of Forecasting papers from the last two years to highlight in their marketing materials as &ldquo;Editor&rsquo;s Choice&rdquo;. I try to select papers across a broad range of subjects and I take into account citations and downloads as well as my own impression of the paper. That tends to bias my selection a little towards older papers as they have had more time to accumulate citations.
2014,2,3,Computational Actuarial Science with R,https://robjhyndman.com/hyndsight/caswithr/,I recently co-authored a chapter on &ldquo;Prospective Life Tables&rdquo; for this book edited by Arthur Charpentier. R code to reproduce the figures and to complete the exercises for our chapter is now available on github. Code for the other chapters should also be available soon. The book can be pre-ordered on Amazon.
2014,2,2,Monash Econometrics in the top 10,https://robjhyndman.com/hyndsight/monash-top10/,"Dave Giles pointed out on his blog yesterday that my department is currently ranked in the top 10 in the world for econometrics according to IDEAS. We are also ranked 13th in the world in forecasting. Since IDEAS only covers the economics literature the forecasting rank does not take account of our work in other areas such as demographic forecasting and electricity demand forecasting.
These rankings are only a rough indication of quality but it is nice to see the department being recognized."
2014,1,31,Automatic time series forecasting in Granada,https://robjhyndman.com/hyndsight/granada-workshop/,"In two weeks I am presenting a workshop at the University of Granada (Spain) on Automatic Time Series Forecasting.
Unlike most of my talks this is not intended to be primarily about my own research. Rather it is to provide a state-of-the-art overview of the topic (at a level suitable for Masters students in Computer Science). I thought I&rsquo;d provide some historical perspective on the development of automatic time series forecasting plus give some comments on the current best practices."
2014,1,30,Free books on statistical learning,https://robjhyndman.com/hyndsight/free-books-on-statistical-learning/,Hastie Tibshirani and Friedman&rsquo;s Elements of Statistical Learning first appeared in 2001 and is already a classic. It is my go-to book when I need a quick refresher on a machine learning algorithm. I like it because it is written using the language and perspective of statistics and provides a very useful entry point into the literature of machine learning which has its own terminology for statistical concepts. A free downloadable pdf version is available on the website.
2014,1,28,Online collaborative writing,https://robjhyndman.com/hyndsight/online-collaborative-writing/,Everyone who has written a paper with another author will know it can be tricky making sure you don&rsquo;t end up with two versions that need to be merged. The good news is that the days of sending updated drafts by email backwards and forwards is finally over (having lasted all of 25 years &ndash; I can barely imagine writing papers before email).LaTeX solutions There has been a lot of activity in the development of online LaTeX tools over the last few years.
2014,1,27,New in forecast 5.0,https://robjhyndman.com/hyndsight/forecast5/,"Last week version 5.0 of the forecast package for R was released. There are a few new functions and changes made to the package which is why I increased the version number to 5.0. Thanks to Earo Wang for helping with this new version.
Handling missing values and outliers Data cleaning is often the first step that data scientists and analysts take to ensure statistical modelling is supported by good data."
2014,1,24,Thoughts on the Ljung-Box test,https://robjhyndman.com/hyndsight/ljung-box-test/,It is common to use a Ljung-Box test to check that the residuals from a time series model resemble white noise. However there is very little practical advice around about how to choose the number of lags for the test.
2014,1,22,Looking for a new post-doc,https://robjhyndman.com/hyndsight/postdoc/,"We are looking for a new post-doctoral research fellow to work on the project &ldquo;Macroeconomic Forecasting in a Big Data World&rdquo;. Details are given at the link below
jobs.monash.edu.au/jobDetails.asp?sJobIDs=519824
This is a two year position funded by the Australian Research Council and working with me George Athanasopoulos Farshid Vahid and Anastasios Panagiotelis. We are looking for someone with a PhD in econometrics statistics or machine learning who is well-trained in computationally intensive methods and who has a background in at least one of time series analysis macroeconomic modelling or Bayesian econometrics."
2014,1,21,Estimating a nonlinear time series model in R,https://robjhyndman.com/hyndsight/nlts/,"There are quite a few R packages available for nonlinear time series analysis but sometimes you need to code your own models. Here is a simple example to show how it can be done.
The model is a first order threshold autoregression:
 $$ y_t = \begin{cases} \alpha y_{t-1} + e_t & \text{if $y_{t-1} \le r$}\\ \beta y_{t-1} + \gamma e_t & \text{if $y_{t-1}r$} \end{cases} $$  where $e_t$ is a Gaussian white noise series with variance $\sigma^2$."
2014,1,9,Boosting multi-step autoregressive forecasts,https://robjhyndman.com/publications/boostingar/,Multi-step forecasts can be produced recursively by iterating a one-step model or directly using a specific model for each horizon. Choosing between these two strategies is not an easy task since it involves a trade-off between bias and estimation variance over the forecast horizon. Using a nonlinear machine learning model makes the tradeoff even more difficult. To address this issue we propose a new forecasting strategy which boosts traditional recursive linear forecasts with a direct strategy using a boosting autoregression procedure at each horizon.
2014,1,1,Prospective life tables,https://robjhyndman.com/publications/prospective-life-tables/,
2013,12,22,Judgmental forecasting experiment,https://robjhyndman.com/hyndsight/judgmental-forecasting-experiment/,The Centre for Forecasting at Lancaster University is conducting some research on judgmental forecasting and model selection. They hope to compare the performance of judgmental model selection with statistical model selection in order to learn how to best design forecasting support systems. They would like forecasting students practitioners and researchers to participate and are offering £50 Amazon Gift Cards as prizes. Here is a brief description from Fotios Petropoulos: We are inviting you to participate in a web-based judgmental forecasting exercise.
2013,11,25,How to get your paper rejected quickly,https://robjhyndman.com/hyndsight/quick-rejection/,"I sent this rejection letter this morning about a paper submitted to the International Journal of Forecasting.
 Dear XXXXX.
  I am writing to you regarding manuscript ????? entitled &ldquo;xxxxxxxxxxxx&rdquo; which you submitted to the International Journal of Forecasting.
  It so happens that I am aware that this paper was previously reviewed for the YYYYYYY journal. It seems that you have not bothered to make any of the changes recommended by the reviewers of your submission to YYYYYYY."
2013,10,31,"Nonparametric and semiparametric response surface methodology: a review of designs, models and optimization techniques",https://robjhyndman.com/publications/rsm-review/,Abstract: Since the introduction of Response Surface Methodology in the 1950s there have been many developments with the aim of expanding the range of applications of the methodology. Various new design modeling and optimization techniques have been introduced for coping with unknown input-output relationships costly or time-consuming experimental studies and irregular experimental regions (e.g. non-cubic or non-spherical regions induced by constraints in the input variables). Such developments may involve many different research areas simultaneously (e.
2013,10,14,Probabilistic Energy Forecasting,https://robjhyndman.com/hyndsight/probabilistic-energy-forecasting/,The International Journal of Forecasting is calling for papers on probabilistic energy forecasting. Here are the details (taken from Tao Hong&rsquo;s blog). In today&rsquo;s competitive and dynamic environment more and more decision making processes in the energy industry are relying on probabilistic forecasts. The applications of probabilistic energy forecasts spread across planning and operations of the entire energy value chain. We are seeking papers from researchers working on the areas of probabilistic energy forecasting.
2013,10,11,Coherent mortality forecasting using functional time series,https://robjhyndman.com/seminars/coherent/,A talk given today at Macquarie University Sydney.
2013,10,10,Forecasting hierarchical time series,https://robjhyndman.com/seminars/hts-3/,Talk given at University of Sydney today.
2013,10,9,Robert G Brown (1923-2013),https://robjhyndman.com/hyndsight/rgbrown/,"Robert Goodell Brown was the father of exponential smoothing. He died last week at the age of 90. While I never met him I was indebted to him for exponential smoothing and his practical and insightful books.
Today I received this email from King Harrison III advising of his death. Twenty years ago I attended the ISF 93 conference in Pittsburgh which honored Bob Brown on his 70th birthday and his contributions to the forecasting world."
2013,9,17,Forecasting with daily data,https://robjhyndman.com/hyndsight/dailydata/,"I&rsquo;ve had several emails recently asking how to forecast daily data in R. Unless the time series is very long the easiest approach is to simply set the frequency attribute to 7.
y &lt;- ts(x frequency=7) Then any of the usual time series forecasting methods should produce reasonable forecasts. For example
library(forecast) fit &lt;- ets(y) fc &lt;- forecast(fit) plot(fc) When the time series is long enough to take in more than a year then it may be necessary to allow for annual seasonality as well as weekly seasonality."
2013,9,11,MAXIMA research centre at Monash Uni,https://robjhyndman.com/hyndsight/maxima/,"The &ldquo;Monash Academy for Cross and Interdisciplinary Mathematical Applications&rdquo; (MAXIMA) is a new research centre that aims to maximise the potential of mathematics to deliver impact to society. It will be led by Kate Smith-Miles. I will also be involved along with several other mathematicians at Monash. Our mission at MAXIMA is to find solutions to 21st century problems by dismantling mathematical barriers.
MAXIMA will be launched on 25 September at a public lecture on &ldquo;The Role of Embedded Optimization in Smart Systems and Products&rdquo;."
2013,7,12,Reflections on UseR! 2013,https://robjhyndman.com/hyndsight/user2013/,"This week I&rsquo;ve been at the R Users conference in Albacete Spain. These conferences are a little unusual in that they are not really about research unlike most conferences I attend. They provide a place for people to discuss and exchange ideas on how R can be used.
Here are some thoughts and highlights of the conference in no particular order.  Håvard Rue spoke on Bayesian computing with INLA and the R-INLA package."
2013,7,10,Asking good questions,https://robjhyndman.com/hyndsight/questions/,"I&rsquo;m currently attending my third conference in three weeks. So I&rsquo;ve heard a lot of talks and I&rsquo;ve heard a lot of questions asked after the talks. In this guest post Eran Raviv reflects on what makes a good question after a talk.
 A few weeks back I attended the excellent ISF conference. In one of the sessions the presenter was talking about a state-of-the-art method to prevent model overfitting an issue with an ever-growing importance in this era of increased computational power and data storage capabilities."
2013,7,3,Facts and fallacies of the AIC,https://robjhyndman.com/hyndsight/aic/,Akaike&rsquo;s Information Criterion (AIC) is a very useful model selection tool but it is not as well understood as it should be. I frequently read papers or hear talks which demonstrate misunderstandings or misuse of this important tool. The following points should clarify some aspects of the AIC and hopefully reduce its misuse.  The AIC is a penalized likelihood and so it requires the likelihood to be maximized before it can be calculated.
2013,7,3,R tools for hierarchical time series,https://robjhyndman.com/seminars/hts-2/,"Talk given at EURO/INFORMS Rome 1 July 2013
And at UseR! 2013 Albacete Spain 10 July 2013."
2013,6,29,End of Google Reader,https://robjhyndman.com/hyndsight/feedly/,"If you are reading this in Google Reader or you are reading in an RSS reader that uses Google Reader as a back-end then you probably need this reminder. Everyone else can stop reading now.Google Reader will cease to be available on 1 July and you will lose all your feeds and other data if you have not switched before then.
I have switched to using Feedly and it is a very convenient replacement."
2013,6,26,Future ISFs,https://robjhyndman.com/hyndsight/future-isfs/,"The next few locations for the International Symposium on Forecasting have been announced:
  2014: Rotterdam The Netherlands
  2015: Riverside California USA
  2016: Santander Spain
  2017: Cairns Australia
  The ISF is easily the best forecasting conference and is held every year in different locations. The 2013 conference finished today in Seoul South Korea.
I&rsquo;m especially delighted that it is coming back to Australia in 2017."
2013,6,24,Forecasting without forecasters,https://robjhyndman.com/seminars/forecasting-without-forecasters/,A keynote talk given at the International Symposium on Forecasting Seoul South Korea. 25 June 2013.
2013,5,25,Managing research ideas,https://robjhyndman.com/hyndsight/managing-research-ideas/,"I received this email today:
 Dear Professor Hyndman I was wondering if you could maybe give me some advice on how to organize your research process. I am able to search the literature on a certain topic and identify where there is a question to work with. My main difficult is to organize my paper annotations in order to help me to guide my research process i.e how to manage the information gathered in those papers to compose and structure a document which can represent the research developed so far."
2013,5,17,IJF quality indicators,https://robjhyndman.com/hyndsight/ijf-quality-indicators/,"I often receive email asking about IJF quality indicators. Here is one I received today.
 Dear Professor Hyndman
  I recently had a paper published in IJF entitled &ldquo;xxxxxxxxxxxx&rdquo;. I am very pleased with the publication and consider IJF to be an excellent outlet for my work in time-series econometrics.
  I have an unusual request but I hope you will consider responding. My research is judged by non-economists and IJF is not on their list of &ldquo;quality&rdquo; journals."
2013,5,15,Forecasting annual totals from monthly data,https://robjhyndman.com/hyndsight/forecasting-annual-totals/,"This question was posed on crossvalidated.com:
 I have a monthly time series (for 2009-2012 non-stationary with seasonality). I can use ARIMA (or ETS) to obtain point and interval forecasts for each month of 2013 but I am interested in forecasting the total for the whole year including prediction intervals. Is there an easy way in R to obtain interval forecasts for the total for 2013?
 I&rsquo;ve come across this problem before in my consulting work although I don&rsquo;t think I&rsquo;ve ever published my solution."
2013,5,6,Establishing priority,https://robjhyndman.com/hyndsight/establishing-priority/,The nature of research is that other people are probably working on similar ideas to you and it is possible that someone will beat you to publishing them.When I was working on my PhD I discovered another PhD thesis by Iris Yeung at UKC with almost exactly the same title as mine and published a year earlier. In those days a copy of a thesis had to be printed from microfiche and then posted by snail mail.
2013,4,21,My new forecasting book is finally finished,https://robjhyndman.com/hyndsight/fpp-2/,"My new online forecasting book (written with George Athanasopoulos) is now completed. I previously described it on this blog nearly a year ago.
In reality an online book is never complete and we plan to continually update it. But it is now at the point where it is suitable for course work and contains exercises and references.
We hope that users (especially other lecturers) will submit materials such as slides and exercises that can be shared on the website."
2013,3,31,George E P Box (1919-2013),https://robjhyndman.com/hyndsight/gepbox/,"Last Thursday (28 March 2013) George Box passed away at the age of 93. He was one of the great statisticians of the last 100 years and leaves an astonishingly diverse legacy.
When I teach forecasting to my second year commerce students we cover Box-Cox transformations Box-Pierce and Ljung-Box tests and Box-Jenkins modelling and my students wonder if it is the same Box in all cases. It is. And we don&rsquo;t even go near his work on response surface modelling design of experiments quality control or random number generation."
2013,3,12,The difference between prediction intervals and confidence intervals,https://robjhyndman.com/hyndsight/intervals/,"Prediction intervals and confidence intervals are not the same thing. Unfortunately the terms are often confused and I am often frequently correcting the error in students' papers and articles I am reviewing or editing.
A prediction interval is an interval associated with a random variable yet to be observed with a specified probability of the random variable lying within the interval. For example I might give an 80% interval for the forecast of GDP in 2014."
2013,2,28,ETS models now in EViews 8,https://robjhyndman.com/hyndsight/eviews8/,"The ETS modelling framework developed in my 2002 IJF paper (with Koehler Snyder and Grose) and in my 2008 Springer book (with Koehler Ord and Snyder) is now available in EViews 8. I had no idea they were even working on it so it was quite a surprise to be told that EViews now includes ETS models.Here is the blurb from the release notes:
 EViews 8 now offers support for exponential smoothing using the dynamic nonlinear model framework of Hyndman Koehler et al."
2013,2,22,Removing white space around R figures,https://robjhyndman.com/hyndsight/crop-r-figures/,"When I want to insert figures generated in R into a LaTeX document it looks better if I first remove the white space around the figure. Unfortunately R does not make this easy as the graphs are generated to look good on a screen not in a document.
There are two things that can be done to fix this problem.First you can reduce the white space generated by R. I use the following function when saving figures in R."
2013,2,15,Forecasting conferences,https://robjhyndman.com/hyndsight/forecasting-conferences/,This year there are no fewer than three forecasting conferences planned for June and July 2013. As well as the annual International Symposium on Forecasting there is WIPFOR (Workshop on Industry &amp; Practices for FORecasting) to be held in Clamart (near Paris) in June and a forecasting stream at the EURO2013 conference in Rome in early July. Some details follow taken from emails sent to me recently.WIPFOR (Clamart France 5-7 June 2013) We would like to bring to your attention the Second Workshop on Industry &amp; Practices for FORecasting (WIPFOR) with an emphasis on Modeling and Stochastic Learning for Forecasting in High Dimension.
2013,2,14,Hyndsight,https://robjhyndman.com/hyndsight/hyndsight/,"Originally I wrote this blog for my own PhD students and I covered issues to do with research. I called it &ldquo;Research tips&rdquo; because that is what it was meant to be.
However over time I&rsquo;ve started covering other things of interest to me and the readership has grown way beyond what I ever expected. So I decided it was time to acknowledge the change of focus with a change of name."
2013,2,13,Out-of-sample one-step forecasts,https://robjhyndman.com/hyndsight/out-of-sample-one-step-forecasts/,"It is common to fit a model using training data and then to evaluate its performance on a test data set. When the data are time series it is useful to compute one-step forecasts on the test data. For some reason this is much more commonly done by people trained in machine learning rather than statistics.
If you are using the forecast package in R it is easily done with ETS and ARIMA models."
2013,2,11,Statistical consulting in Australia,https://robjhyndman.com/hyndsight/consulting-groups/,"There must be dozens of statistical consulting businesses and organizations in Australia each specializing in different areas.
I do some consulting work myself mostly in the forecasting area but sometimes in other areas of applied statistics including expert witness work in court cases. Email me if you have a project you would like me to take on. However I often refer potential clients to other statistical consulting groups as I only have a limited amount of time I can spend on consulting projects."
2013,2,6,Man vs wild data,https://robjhyndman.com/seminars/man-vs-wild-data/,"Keynote address. Young Statisticians Conference 2013.
Abstract: For 25 years I have been an intrepid statistical consultant tackling the wild frontiers of real data real problems and real time constraints. I have faced problems ranging from linguistics to river beds from making paper plates to selling pies at the MCG from tax office audits to surveys about the colour purple. University education helps prepare you to be a statistical consultant in the same way that Google maps helps prepare you to cross the Simpson Desert."
2013,2,1,Coherent mortality forecasting: the product-ratio method with functional time series models,https://robjhyndman.com/publications/coherentfdm/,When independence is assumed forecasts of mortality for subpopulations are almost always divergent in the long term. We propose a method for coherent forecasting of mortality rates for two or more subpopulations based on functional principal components models of simple and interpretable functions of rates. The product-ratio functional forecasting method models and forecasts the geometric mean of subpopulation rates and the ratio of subpopulation rates to product rates. Coherence is imposed by constraining the forecast ratio function through stationary time series models.
2013,1,7,Batch forecasting in R,https://robjhyndman.com/hyndsight/batch-forecasting/,"I sometimes get asked about forecasting many time series automatically. Here is a recent email for example:
 I have looked but cannot find any info on generating forecasts on multiple data sets in sequence. I have been using analysis services for sql server to generate fitted time series but it is too much of a black box (or I don’t know enough to tweak/manage the inputs). In short what package should I research that will allow me to load data generate a forecast (presumably best fit) export the forecast then repeat for a few thousand items."
2013,1,1,A change of editors,https://robjhyndman.com/publications/a-change-of-editors/,
2012,12,21,Man vs Wild Data,https://robjhyndman.com/hyndsight/ysc2013/,"I&rsquo;m speaking on this topic at the Young Statisticians Conference 7-8 February 2013.
If you&rsquo;re a young statistician and live in Australia please book in. It promises to be a great couple of days. Early registrations close on 2 January.
Abstract for my talk:
For 25 years I have been an intrepid statistical consultant tackling the wild frontiers of real data real problems and real time constraints. I have faced problems ranging from linguistics to river beds from making paper plates to selling pies at the MCG from tax office audits to surveys about the colour purple."
2012,12,3,New in forecast 4.0,https://robjhyndman.com/hyndsight/forecast4/,A few days ago I released version 4.0 of the forecast package for R. There were quite a few changes and new features so I thought it deserved a new version number. I keep a list of changes in the Changelog for the package but I doubt that many people look at it. So for the record here are the most important changes to the forecast package made since v3.
2012,11,20,"SimpleR: tips, tricks and tools",https://robjhyndman.com/seminars/simpler/,"Melbourne R Users' Group
Deloitte Level 11 550 Bourke Street Melbourne
Examples"
2012,10,31,Makefiles for R/LaTeX projects,https://robjhyndman.com/hyndsight/makefiles/,"Updated: 21 November 2012
Make is a marvellous tool used by programmers to build software but it can be used for much more than that. I use make whenever I have a large project involving R files and LaTeX files which means I use it for almost all of the papers I write and almost of the consulting reports I produce.If you are using a Mac or Linux you will already have make installed."
2012,10,29,YSC2013 funding support,https://robjhyndman.com/hyndsight/ysc2013-funding-support/,"Young Victorian statisticians should be attending the YSC conference in Melbourne in February 2013. It promises to be a great event and the speaker line-up looks first-class (apart from one dodgy keynote speaker).
If you think you can&rsquo;t afford it there is good news! The local SSA Branch is offering financial support to selected young statisticians. Applicants must be student members of the Victorian Branch (to join see www.statsoc.org.au/join.htm). Successful applicants will be given free conference registration."
2012,10,23,LaTeX loops,https://robjhyndman.com/hyndsight/latex-loops/,"Today I was writing a report which included 20 figures with the names demandplot1.pdf demandplot2.pdf &hellip; demandplot20.pdf and all with similar captions. Clearly a loop was required. After all LaTeX is a programming language so we should be able to take advantage of its capabilities.I found the forloop package which handled the problem perfectly. My first attempt looked like this:
\newcommand{\demandplot}[1]{\begin{figure}\centering \includegraphics[width=\textwidth]{./figs/demandplot#1.pdf} \caption{Hourly demand (GW) for zone #1.} \end{figure}} \newcounter{loop} \forloop{loop}{1}{\value{loop}&lt;21}{\demandplot{\arabic{loop}}}  The magic happens in the last line providing a loop from 1 to 20 generating the commands \demandplot{1} \demandplot{2} &hellip; \demandplot{20}."
2012,10,7,The Young Stats Communication Challenge,https://robjhyndman.com/hyndsight/ysc2013comp/,"The Australian Young Statisticians Conference (Feb 2013) is organizing a communication competition. They invite all early-career statisticians (studying or within 5 years of graduation) to produce a short (3-5 minute) video for the ABS YSC2013 Video Competition or a static infographic for the ABS YSC2013 Infographic Competition.
Both competitions have a 1st prize of $500 and 2nd prize of $250.
Entries close 16th November and winners will be notified by mid-December."
2012,10,3,Forecasting research grants,https://robjhyndman.com/hyndsight/iifsas/,The International Institute of Forecasters and SAS have an annual research grant scheme that has been offered for the past ten years. The amounts offered are small (a total of $10K per year usually split between 2 or 3 projects) but it might be useful for young researchers wanting a bit of funding to help with their forecasting research. The deadline for 2012 has just been extended to 26 October which is a sure sign that they don&rsquo;t have enough applications yet.
2012,9,18,Why are some things easier to forecast than others?,https://robjhyndman.com/hyndsight/hardforecasts/,Forecasters are often met with skepticism. Almost every time I tell someone that I work in forecasting they say something about forecasting the stock market or forecasting the weather usually suggesting that such forecasts are hopelessly inaccurate. In fact forecasts of the weather are amazingly accurate given the complexity of the system while anyone claiming to forecast the stock market deserves skepticism. So what is the difference between these two types of forecasts and can we say anything about what can be reasonably be forecast and what can&rsquo;t?
2012,9,3,Multivariate time series modelling and forecasting workshop,https://robjhyndman.com/hyndsight/workshop2013/,"We are planning a workshop on multivariate time series modelling and forecasting to be held at Monash University on 18-19 February 2013.
Call for papers: all submissions should be sent in pdf format to george.athanasopoulos@monash.edu by 20 November 2012.
Keynote speakers   Professor Helmut Lütkepohl (DIW Berlin)
  Professor Massimiliano Marcellino (European University Institute Florence)
  Associate Professor Andrew Patton (Duke University Durham NC)
  Professor George Kapetanios (Queen Mary University of London)"
2012,9,1,Recursive and direct multi-step forecasting: the best of both worlds,https://robjhyndman.com/publications/rectify/,We propose a new forecasting strategy called rectify that seeks to combine the best properties of both the recursive and direct forecasting strategies. The rationale behind the rectify strategy is to begin with biased recursive forecasts and adjust them so they are unbiased and have smaller error. We use linear and nonlinear simulated time series to investigate the performance of the rectify strategy and compare the results with those from the recursive and the direct strategies.
2012,8,28,COMPSTAT2012,https://robjhyndman.com/hyndsight/compstat2012/,"This week I&rsquo;m in Cyprus attending the COMPSTAT2012 conference. There&rsquo;s been the usual interesting collection of talks and interactions with other researchers. But I was struck by two side comments in talks this morning that I&rsquo;d like to mention.
Stephen Pollock: Don&rsquo;t imagine your model is the truth Actually Stephen said something like &ldquo;economists (or was it econometricians?) have a bad habit of imagining their models are true&rdquo;. He gave the example of people asking whether GDP &ldquo;has a unit root&rdquo;?"
2012,8,19,Flat forecasts,https://robjhyndman.com/hyndsight/flat-forecasts/,"About once a week someone will tell me there is a bug in my forecast package for R because it gives forecasts that are the same for all future horizons. To save answering the same question repeatedly here is my response.
A point forecast is (usually) the mean of the distribution of a future observation in the time series conditional on the past observations of the time series. It is possible even likely in some circumstances that the future observations will have the same mean and then the forecast function is flat."
2012,8,9,Interviews,https://robjhyndman.com/hyndsight/interviews/,"I&rsquo;ve been interviewed twice in the last year:
  For DecisionStats 9 August 2012.
  For Data Mining Research 21 October 2011. Republished in Amstat News 1 December 2011.
  Some readers of this blog might find them interesting. I said a few things in today&rsquo;s interview that I don&rsquo;t think I&rsquo;ve previously said publicly."
2012,8,9,Blogs about research,https://robjhyndman.com/hyndsight/blogs-about-research/,"If you find this blog helpful (or even if you don&rsquo;t but you&rsquo;re interested in blogs on research issues and tools) there are a few other blogs about doing research that you might find useful. Here are a few that I read.
  Patter &ndash; Pat Thomson.
  The Thesis Whisperer &ndash; Inger Mewburn.
  The Research Whisperer &ndash; several RMIT researchers.
  the (research) supervisor&rsquo;s friend &ndash; Geof Hill."
2012,8,3,Read the literature,https://robjhyndman.com/hyndsight/read-the-literature/,"I&rsquo;ve just finished another reviewer report for a journal and yet again I&rsquo;ve had to make comments about reading the literature. It&rsquo;s not difficult. Before you write a paper read what other people have done. A simple search on Google scholar will usually do the trick. And before you submit a paper check again that you haven&rsquo;t missed anything important.
The paper I reviewed today did not cite a single reference from either of the two most active research groups in the area in the last ten years."
2012,8,2,Put your pre-prints online,https://robjhyndman.com/hyndsight/preprints/,"I have argued previously that research papers should be posted online at the same time as they are submitted to a journal. Sometimes people claim that journals don&rsquo;t allow it which is nonsense. Almost every journal allows it and many also allow the published version of a paper to appear on your personal website.
Today I discovered a new tool (thanks to the IMU newsletter) which makes it easy to check a journal&rsquo;s policy on this."
2012,8,1,Bare bones beamer,https://robjhyndman.com/hyndsight/beamer/,"Beamer is far and away the most popular software for presentations amongst researchers in mathematics and statistics. Most conference and seminar talks I attend these days use beamer. Unfortunately they all look much the same. I think people find beamer themes too hard to modify easily so a small number of templates get shared around. Even the otherwise wonderful LaTeX Templates site has no beamer examples.
The beamer user guide explains how to make changes but it is not for the faint-hearted (although it is a fantastic resource once you have some expertise)."
2012,7,31,Forecasting the Olympics,https://robjhyndman.com/hyndsight/olympics/,"Forecasting sporting events is a growing research area. The International Journal of Forecasting even had a special issue on sports forecasting a couple of years ago.
The London 2012 Olympics has attracted a few forecasters trying to predict medal counts world records etc. Here are some of the articles I&rsquo;ve seen.
 Which Olympic records get shattered? Nate Silver New York Times. Statisticians predict the number of Olympic records that will fall at London 2012 Physics arXiv blog."
2012,6,25,A case-crossover design to examine the role of aeroallergens and respiratory viruses on childhood asthma exacerbations requiring hospitalisation: The MAPCAH study,https://robjhyndman.com/publications/mapcah/,"Background: Few case-control studies of time dependent environmental exposures and respiratory outcomes have been performed. Small sample sizes pose modeling challenges for estimating interactions. In contrast case cross-over studies are well suited where control selection and responses are low time consuming and costly.
Objective: To demonstrate the feasibility and validity of a case crossover study of children admitted to hospital for asthma to examine interacting effects of time varying environmental exposures."
2012,6,19,Advances in automatic time series forecasting,https://robjhyndman.com/seminars/automaticforecasting/,Invited talk Australian Statistical Conference Adelaide 10 July 2012. COMPSTAT 2012 Cyprus 29 August 2012. Seminar Lancaster University 10 September 2012.  Many applications require a large number of time series to be forecast completely automatically. For example manufacturing companies often require weekly forecasts of demand for thousands of products at dozens of locations in order to plan distribution and maintain suitable inventory stocks. In population forecasting there are often a few hundred time series to be forecast representing various components that make up the population dynamics.
2012,6,6,Constants and ARIMA models in R,https://robjhyndman.com/hyndsight/arimaconstants/,"This post is from my new book Forecasting: principles and practice available freely online at OTexts.org/fpp/.
 A non-seasonal ARIMA model can be written as \begin{equation}\label{eq:c} (1-\phi_1B - \cdots - \phi_p B^p)(1-B)^d y_t = c + (1 + \theta_1 B + \cdots + \theta_q B^q)e_t \end{equation} or equivalently as \begin{equation}\label{eq:mu} (1-\phi_1B - \cdots - \phi_p B^p)(1-B)^d (y_t - \mu t^d/d!) = (1 + \theta_1 B + \cdots + \theta_q B^q)e_t \end{equation} where $B$ is the backshift operator $c = \mu(1-\phi_1 - \cdots - \phi_p )$ and $\mu$ is the mean of $(1-B)^d y_t$."
2012,5,23,My new forecasting textbook,https://robjhyndman.com/hyndsight/fpp/,"After years of saying that I was going to write a book to replace Makridakis Wheelwright and Hyndman (1998) I&rsquo;m finally ready to make an announcement!
My new book is Forecasting: principles and practice co-authored with George Athanasopoulos. It is available online and free-of-charge. We have written about 2/3 of the book so far (all of which is already available online) and we plan to finish it by the end of 2012."
2012,5,15,Blog aggregators,https://robjhyndman.com/hyndsight/blog-aggregators/,"A very useful way of keeping up with blogs in a particular area is to subscribe to a blog aggregator. These will syndicate posts from a large number of blogs and provide links back to the original sources. So you only need to subscribe once to get all the good stuff in that area.
There are now several blog aggregators available that might be of interest to readers here. And this blog is now syndicated on several other sites including those listed below."
2012,5,8,Seeking help,https://robjhyndman.com/hyndsight/seeking-help/,Every day I receive emails or comments on this blog asking for help with R forecasting LaTeX possible research topics how to install software or some other thing I&rsquo;m supposed to know something about. Unfortunately I cannot provide a one-man help service to the rest of the world. I used to reply to all the requests explaining where to go for help but I stopped replying a while ago as it took too much time to do even that.
2012,5,2,Measuring time series characteristics,https://robjhyndman.com/hyndsight/tscharacteristics/,"A few years ago I was working on a project where we measured various characteristics of a time series and used the information to determine what forecasting method to apply or how to cluster the time series into meaningful groups. The two main papers to come out of that project were:
Wang Smith and Hyndman (2006) Characteristic-​​based clustering for time series data. Data Mining and Knowledge Discovery 13(3) 335-364."
2012,4,2,Google scholar metrics,https://robjhyndman.com/hyndsight/google-scholar-metrics/,"Google has produced another great tool for researchers this time providing some metrics on journal citations. Google Scholar Metrics allows you to search on journals rather than articles and to see the average or median h-index for each journal.
For example a search on &ldquo;forecasting&rdquo; yields the following results:

The h-index is the largest number $h$ such that at least $h$ articles in that publication were cited at least $h$ times each."
2012,3,4,Data visualization,https://robjhyndman.com/hyndsight/data-visualization/,"For those who have not read the seminal works of Tufte and Cleveland please hang your heads in shame. To salvage some sense of self-worth you can then head over to Solomon Messing&rsquo;s blog where he is starting a series on data visualization based on the principles developed by Tufte and Cleveland (with R examples).
The classics are also worth reading and remain relevant despite the 20 or 30 years that have elapsed since they appeared."
2012,2,28,Exponential smoothing and regressors,https://robjhyndman.com/hyndsight/ets-regressors/,I have thought quite a lot about including regressors (i.e. covariates) in exponential smoothing (ETS) models and I have done it a couple of times in my published work. See my 2008 exponential smoothing book (chapter 9) and my 2008 Tourism Management paper. However there are some theoretical issues with these approaches which have come to light through the research of Ahmad Farid Osman one of our PhD students at Monash University.
2012,2,22,Academia StackExchange,https://robjhyndman.com/hyndsight/academia/,"There&rsquo;s a new StackExchange site that might be useful to readers: Academia. It is a Q&amp;A site for academics and those enrolled in higher education.
The draft FAQ says it will cover:
  Life as a graduate student postdoctoral researcher university professor
  Transitioning from undergraduate to graduate researcher
  Inner workings of research departments
  Requirements and expectations of academicians
  Judging from the first 89 questions this is going to be extremely helpful especially for PhD students."
2012,2,16,Mailing lists,https://robjhyndman.com/hyndsight/mailing-lists/,"Staying in touch with other researchers is important. You need to know about up-coming conferences seminars jobs etc. To this end it is worth finding a few key email lists to join. A long list of lists in econometrics and statistics is provided by EconometricLinks. Browse through it to find topics of interest. No doubt researchers in other disciplines have their own lists but I don&rsquo;t know about them.
For those in Australia and New Zealand there are two local lists that every statistician and econometrician should be on:"
2012,2,13,Table design,https://robjhyndman.com/hyndsight/tables/,Almost every research paper and thesis in statistics contains at least some tables yet students are rarely taught how to make good tables. While the principles of good graphics are slowly becoming part of a statistical education (although not an econometrics education!) the principles of good tables are often ignored. Perhaps people think they are obvious although the results I see in papers and theses suggest otherwise.Lately the topic seems to have been getting some much-needed attention and the following resources may be useful.
2012,2,1,Short-term load forecasting based on a semi-parametric additive model,https://robjhyndman.com/publications/stlf/,Short-term load forecasting is an essential instrument in power system planning operation and control. Many operating decisions are based on load forecasts such as dispatch scheduling of generating capacity reliability analysis and maintenance planning for the generators. Overestimation of electricity demand will cause a conservative operation which leads to the start-up of too many units or excessive energy purchase thereby supplying an unnecessary level of reserve. On the contrary underestimation may result in a risky operation with insufficient preparation of spinning reserve causing the system to operate in a vulnerable region to the disturbance.
2012,1,31,Following authors on Google Scholar,https://robjhyndman.com/hyndsight/googlescholar2/,"A great new feature has been added to Google Scholar Citations. For those authors who have set up a citations page it is now possible to get email alerts for any new articles they publish or for any new citations of their articles. So you can track citations to your own work this way and stay up-to-date with key authors in your field.
Setting up a Google Citations page is super-easy and was already worth doing."
2012,1,29,Forecasts of COPD mortality in Australia: 2006-2025,https://robjhyndman.com/publications/copdaustralia/,"Background: Chronic Obstructive Pulmonary Disease (COPD) is currently the fifth leading cause of death in Australia and there are marked differences in mortality trends between men and women. In this study we have sought to model and forecast age related changes in COPD mortality over time for men and women separately over the period 2006–2025.
Methods: Annual COPD death rates in Australia from 1922 to 2005 for age groups (50–54 55–59 60–64 65–69 70–74 75–79 80–84 85+) were used."
2012,1,20,Refereeing a journal article,https://robjhyndman.com/hyndsight/refereeing2/,"I&rsquo;ve written briefly on this before. For an excellent and more detailed discussion of what is involved there is a series of excellent posts on Pat Thomson&rsquo;s blog:
  Refereeing a journal article part 1: reading
  Refereeing a journal article part 2: making a recommendation
  Refereeing a journal article part 3: writing the feedback
  If every reviewer followed her advice my life as an editor would be much easier and the quality of research would be improved."
2012,1,18,Internet surveys,https://robjhyndman.com/hyndsight/surveys/,"I received the following email today:
 I am preparing a thesis &hellip; I need to conduct the widest possible poll and it occurred to me that perhaps you could guide me toward an internet-based way in which this can be done easily. I have a ten-question questionnaire prepared that I wish to have an random sample of the population respond to. I have no budget for this so I hope you can suggest a way in which a good number of responses can be harvested using blogs or sites you may be aware of."
2011,12,31,Forecasting time series with complex seasonal patterns using exponential smoothing,https://robjhyndman.com/publications/complex-seasonality/,A new innovations state space modeling framework incorporating Box-Cox transformations Fourier series with time varying coefficients and ARMA error correction is introduced for forecasting complex seasonal time series that cannot be handled using existing forecasting models. Such complex time series include time series with multiple seasonal periods high frequency seasonality non-integer seasonality and dual-calendar effects. Our new modelling framework provides an alternative to existing exponential smoothing models and is shown to have many advantages.
2011,12,23,Are we getting better at forecasting?,https://robjhyndman.com/hyndsight/bostonglobe/,"I was interviewed recently for the Boston Globe. The interview was by email and I thought it might be useful to post here.Here are the questions from the journalist.
 Are we better at predicting future events than we used to be? Or are there obstacles inherent to the endeavor that prevent us from ever really being able to do it accurately? If we are better then what allowed it? Computational power?"
2011,12,19,Organizing travel,https://robjhyndman.com/hyndsight/travel/,Whether travelling to a seminar or conference or just having a holiday using a travel organizer can make the process simpler and easier. A good travel organizer keeps all your travel details (flights hotels car rentals meetings weather forecasts etc.) organized and synced to whatever devices you use (two computers an iPad and an iPhone in my case).I am aware of four travel organizers that do this: TripIt TripCase WorldMate and Wipolo.
2011,12,16,Forecasting time series using R,https://robjhyndman.com/hyndsight/melburntalk/,"I gave this talk on Forecasting time series using R for the Melbourne Users of R Network (MelbURN) on Thursday 27 October 2011.
Slides Examples
Abstract I look at the various facilities for time series forecasting available in R concentrating on the forecast package. This package implements several automatic methods for forecasting time series including forecasts from ARIMA models ARFIMA models and exponential smoothing models. I also look more generally at how to go about forecasting non-seasonal data seasonal data seasonal data with high frequency and seasonal data with multiple frequencies."
2011,12,14,Cyclic and seasonal time series,https://robjhyndman.com/hyndsight/cyclicts/,These terms get confused all the time (e.g. this question on CrossValidated.com) and so I thought it might be helpful to try to summarize the distinction and some of the associated models.Definitions A seasonal pattern exists when a series is influenced by seasonal factors (e.g. the quarter of the year the month or day of the week). Seasonality is always of a fixed and known period. Hence seasonal time series are sometimes called periodic time series.
2011,11,30,The art of R programming,https://robjhyndman.com/hyndsight/matloff/,This is a gem of a book. It will become the book I give PhD students when they are learning how to write good R code. That is if I ever see it again. I had hoped to write a review of it but I haven&rsquo;t seen it since it arrived in the mail a couple of weeks ago because a research student or research assistant has always had it on loan.
2011,11,28,Kaggle on TV,https://robjhyndman.com/hyndsight/kaggle-on-tv/,"It is good to see forecasting algorithms getting some mainstream exposure on ABC Catalyst.
Update: See also this great talk by Jeremy Howard a data scientist from Melbourne and now part of Kaggle."
2011,11,26,Researcher portals,https://robjhyndman.com/hyndsight/portals/,A researcher portal is a website that attempts to list all the publications of a given researcher. Some portals also allow sharing papers interacting with other researchers calculating citation statistics etc. Every researcher wants their work read and cited so these websites can be useful tools for getting your work noticed. They can also function as a de facto home page if you don&rsquo;t already have a personal website. Conversely they can be a good way to find new work by researchers in your field.
2011,11,11,What you wish you knew before you started a PhD,https://robjhyndman.com/hyndsight/wishlist/,I asked my research group recently what they wished they had learned before they started work on a PhD. Here are some of the responses.  More mathematics. Particular topics they named included real analysis functional analysis measure theory algebra linear algebra. That would have been my response also. I still wish I knew more mathematics than I do. I did quite a lot of mathematics as an undergraduate but every year I need to learn some more.
2011,10,27,Forecasting time series using R,https://robjhyndman.com/seminars/melbournerug/,"Melbourne R Users' Group Thursday October 27 2011 6:00 PM Deloitte Level 11 (Culture Room) 550 Bourke Street Melbourne
I will look at the various facilities for time series forecasting available in R concentrating on the forecast package. This package implements several automatic methods for forecasting time series including forecasts from ARIMA models ARFIMA models and exponential smoothing models. I will also look more generally at how to go about forecasting non-seasonal data seasonal data seasonal data with high frequency and seasonal data with multiple frequencies."
2011,10,3,Forecasting electricity demand distributions using a semiparametric additive model,https://robjhyndman.com/seminars/electricity-forecasting/,"Talk given at
 University of Melbourne 11 October 2011. University of Adelaide 16 March 2012 Monash University 16 May 2012 La Trobe University 24 May 2012 EDF Paris. 4 September 2012 University of New South Wales 1 November 2012  Abstract: Electricity demand forecasting plays an important role in short-term load allocation and long-term planning for future generation facilities and transmission augmentation. Planners must adopt a probabilistic view of potential peak demand levels therefore density forecasts (providing estimates of the full probability distributions of the possible future values of the demand) are more helpful than point forecasts and are necessary for utilities to evaluate and hedge the financial risk accrued by demand variability and forecasting uncertainty."
2011,9,25,Help for forecasting practitioners,https://robjhyndman.com/hyndsight/forecasting-help/,I often get email from forecasters wanting assistance. As much as I&rsquo;d like to provide a free forecasting advice service to the world that&rsquo;s not what I&rsquo;m paid to do and I choose to spend my unpaid time on other things. However there are some very helpful resources available for forecasting practitioners.First every practicing forecaster should be reading Foresight. It is far and away the best journal or magazine for forecast practitioners.
2011,8,30,The scourge of the academic publishers,https://robjhyndman.com/hyndsight/scourge/,Academic publishing is built on an old model where publishers were needed to print and distribute journals to libraries. Under this system it makes sense that the journals are distributed by publishing companies who charge fees for their work. On the other hand the academics who write for the journals the peer reviewers and (almost all) editors have always contributed their time and expertise without cost. Essentially they are being paid by universities and other research organizations to do this work.
2011,8,26,Time series cross-validation: an R example,https://robjhyndman.com/hyndsight/tscvexample/,I was recently asked how to implement time series cross-validation in R. Time series people would normally call this &ldquo;forecast evaluation with a rolling origin&rdquo; or something similar but it is the natural and obvious analogue to leave-one-out cross-validation for cross-sectional data so I prefer to call it &ldquo;time series cross-validation&rdquo;.Here is some example code applying time series CV and comparing 1-step 2-step &hellip; 12-step forecasts using the Mean Absolute Error (MAE).
2011,8,26,Major changes to the forecast package,https://robjhyndman.com/hyndsight/forecast3/,The forecast package for R has undergone a major upgrade and I&rsquo;ve given it version number 3 as a result. Some of these changes were suggestions from the forecasting workshop I ran in Switzerland a couple of months ago and some have been on the drawing board for a long time. Here are the main changes in version 3 plus a few earlier additions that I thought deserved a mention.
2011,8,24,Crowd sourcing forecasts,https://robjhyndman.com/hyndsight/crowd-sourcing-forecasts/,"Forecasting Ace is looking for participants to develop improved methods for predicting future events and outcomes. Their goal is to develop methods for aggregating many individual judgments in a manner that yields more accurate predictions than any one person or small group alone could provide. Potential applications of the system include forecasting economic conditions political changes technological development and medical breakthroughs.
They are currently seeking to recruit individuals who have interest (and/or expertise) in any of a wide range of current affairs including but not limited to politics economics technology military affairs social trends and science and technology."
2011,8,16,Learn Machine Learning at Stanford for free,https://robjhyndman.com/hyndsight/stanford-ml/,Andrew Ng&rsquo;s machine learning course at Stanford is being offered free to anyone online in the (northern) fall of 2011. I&rsquo;ve seen some of the notes from this course and it looks to be an excellent broad introduction to machine learning and data mining. For example support vector machines neural networks kernels clustering dimension reduction etc.Statisticians should know something about this area (just as computer scientists working in machine learning should know some statistical modelling) and this would be a great way to learn it.
2011,8,12,Beware of junk journals and publishers,https://robjhyndman.com/hyndsight/junkjournals/,"Today I received the following email:
 Dear Professor
 Antarctica Journal of Mathematics Archimedes Journal of Mathematics Bessel Journal of Mathematics Cayley Journal of Mathematics Diophantus Journal of Mathematics  We are charging only $3 per page which is very cheap when compared to some money oriented journals.
Further we request you to withdraw your paper from other journals keeping in view of high page charges.
 You can submit your research papers to our online journals."
2011,7,25,Recommended survey papers,https://robjhyndman.com/hyndsight/surveypapers/,Survey articles are particularly helpful in getting a foothold in a new research area or in looking for important papers that you may have overlooked. Whatever area of research you are in look out for survey papers and journals dedicated to publishing survey papers.The journal Statistics Surveys is relatively new (volume 1 in 2007) and provides very helpful survey articles of areas of statistical research. It is sponsored by the American Statistical Association the Bernoulli Society the Institute of Mathematical Statistics and by the Statistical Society of Canada.
2011,7,21,Social networking for researchers,https://robjhyndman.com/hyndsight/socialnetworks/,It would be nice to have a place to share ideas links comments in a very informal way with others involved in research in statistical methodology and data science. CrossValidated.com is great for specific questions but is not suitable for commenting on papers or sharing ideas and links.Although I have a facebook account this seems the wrong place to carry out a research discussion because my family and friends are generally not interested and it is messy to restrict posts to groups of friends.
2011,7,16,Investigating the influence of synoptic-scale circulation on air quality using self-organizing maps and generalized additive modelling,https://robjhyndman.com/publications/synoptic-gams/,The influence of synoptic-scale circulations on air quality is an area of increasing interest to air quality management in regards to future climate change. This study presents an analysis where the dominant synoptic &lsquo;types&rsquo; over the region of Melbourne Australia are determined and linked to regional air quality. First a self-organising map (SOM) is used to generate a time series of synoptic charts that classify the annual daily circulation affecting Melbourne into 20 different synoptic types.
2011,7,14,Point and interval forecasts of mortality rates and life expectancy: a comparison of ten principal component methods,https://robjhyndman.com/publications/mortality-forecast-comparison/,Abstract: Using the age- and sex-specific data of 14 developed countries we compare the point and interval forecast accuracy and bias of ten principal component methods for forecasting mortality rates and life expectancy. The ten methods are variants and extensions of the Lee-Carter method. Based on one-step forecast errors the weighted Hyndman-Ullah method provides the most accurate point forecasts of mortality rates and the Lee-Miller method is the least biased. For the accuracy and bias of life expectancy the weighted Hyndman-Ullah method performs the best for female mortality and the Lee-Miller method for male mortality.
2011,7,14,Method for optimizing coating properties based on an evolutionary algorithm approach,https://robjhyndman.com/publications/emma/,In industry as well as many areas of scientific research data collected often contain a number of responses of interest for a chosen set of exploratory variables. Optimization of such multivariable multiresponse systems is a challenge well suited to genetic algorithms as global optimization tools. One such example is the optimization of coating surfaces with the required absolute and relative sensitivity for detecting analytes using devices such as sensor arrays. High-throughput synthesis and screening methods can be used to accelerate materials discovery and optimization; however an important practical consideration for successful optimization of materials for arrays and other applications is the ability to generate adequate information from a minimum number of experiments.
2011,6,22,Giving a useR! talk,https://robjhyndman.com/publications/usertalk/,Giving a useR! talk at the international R user conference is a balancing act in which you have to try to impart some new ideas provide sufficient background and keep the audience interested all in a very short period of time.
2011,6,15,Evaluating extreme quantile forecasts,https://robjhyndman.com/seminars/maep/,Talk to be given at the International Symposium on Forecasting Prague 26–29 June 2011.
2011,5,29,Comparing HoltWinters() and ets(),https://robjhyndman.com/hyndsight/estimation2/,"I received this email today:
 I have a question about the ets() function in R which I am trying to use for Holt-Winters exponential smoothing. My problem is that I am getting very different estimates of the alpha beta and gamma parameters using ets() compared to HoltWinters() and I can&rsquo;t figure out why.
 This is a common question so I thought the answer might be of sufficient interest to post here."
2011,4,29,Tourism forecasting: an introduction,https://robjhyndman.com/publications/tourism-forecasting-an-introduction/,Introduction to the special issue on Tourism Forecasting.
2011,3,31,The price elasticity of electricity demand in South Australia,https://robjhyndman.com/publications/electricity-price-elasticity/,In this paper the price elasticity of electricity demand representing the sensitivity of customer demand to the price of electricity has been estimated for South Australia. We first undertake a review of the scholarly literature regarding electricity price elasticity for different regions and systems. Then we perform an empirical evaluation of the historic South Australian price elasticity focussing on the relationship between price and demand quantiles at each half-hour of the day.
2011,3,29,I'm switching to TeXstudio,https://robjhyndman.com/hyndsight/texstudio/,I&rsquo;ve happily used WinEdt for all my LaTeX editing for about 15 years and I&rsquo;ve encouraged my whole research team to use it. But I&rsquo;m tired of problems with WinEdt that take up my time.I regularly have requests for help from one of my research team because something in WinEdt is not working properly &mdash; such as pdf synchronization problems or it is using an old version of MikTeX that no longer updates or that it has switched to using another pdf viewer without warning.
2011,3,25,Looking after your supervisor,https://robjhyndman.com/hyndsight/looking-after-your-supervisor/,Some good advice here: The care and maintenance of your adviser.
2011,3,15,Optimal combination forecasts for hierarchical time series,https://robjhyndman.com/publications/hierarchical/,"In many applications there are multiple time series that are hierarchically organized and can be aggregated at several different levels in groups based on products geography or some other features. We call these &ldquo;hierarchical time series''. They are commonly forecast using either a &ldquo;bottom-up'' or a &ldquo;top-down'' method.
In this paper we propose a new approach to hierarchical forecasting which provides optimal forecasts that are better than forecasts produced by either a top-down or a bottom-up approach."
2011,3,14,Ten rules for data analysis,https://robjhyndman.com/hyndsight/ten-rules-for-data-analysis/,Peter Kennedy was an associate editor of the International Journal of Forecasting and a superb applied econometrician. He died unexpectedly in August 2010. He was best known for his excellent book A Guide to Econometrics as well as his &ldquo;Ten Commandments of Applied Econometrics&rdquo;. He provided a variation on his ten commandments in advice to his students in the form of the following ten rules:  Use common sense (and economic theory)
2011,3,14,Statistical tests for variable selection,https://robjhyndman.com/hyndsight/tests2/,"I received an email today with the following comment:
 I’m using ARIMA with Intervention detection and was planning to use your package to identify my initial ARIMA model for later iteration however I found that sometimes the auto.arima function returns a model where AR/MA coefficients are not significant. So my question is: Is there a way to filter the search for ARIMA models that only have significant coefficients. I can remove the non-significant coefficients but I think it would be better to search for those models that only have significant coefficients."
2011,3,10,Improved interval estimation of long run response from a dynamic linear model: a highest density region approach,https://robjhyndman.com/publications/dlm-hdr/,This paper proposes a new method of interval estimation for the long run response (or elasticity) parameter from a general linear dynamic model. We employ the bias-corrected bootstrap in which small sample biases associated with the parameter estimators are adjusted in two stages of the bootstrap. As a means of bias-correction we use alternative analytic and bootstrap methods. To take atypical properties of the long run elasticity estimator into account the highest density region (HDR) method is adopted for the construction of confidence intervals.
2011,3,1,RStudio: just what I've been looking for,https://robjhyndman.com/hyndsight/rstudio/,"For many years I used RWinEdt as my text editor for R code but when WinEdt 6.0 came out RWinEdt stopped working. So I&rsquo;ve been looking for something to replace it. I&rsquo;ve tried Tinn-R NppToR Eclipse with StatET and a couple of other editors but nothing was quite right.
Then yesterday out of the blue RStudio was announced and it looks fantastic! A screenshot is given below.

The window contains a smart editor with code completion and tabbing console workspace with viewable objects plotting panel with history etc."
2011,2,17,Nonparametric time series forecasting with dynamic updating,https://robjhyndman.com/publications/dynamic-updating/,Abstract We present a nonparametric method to forecast a seasonal univariate time series and propose four dynamic updating methods to improve point forecast accuracy. Our methods consider a seasonal univariate time series as a functional time series. We propose first to reduce the dimensionality by applying functional principal component analysis to the historical observations and then to use univariate time series forecasting and functional principal component regression techniques. When data in the most recent year are partially observed we improve point forecast accuracy using dynamic updating methods.
2011,2,9,The value of feedback in forecasting competitions,https://robjhyndman.com/publications/kaggle/,In this paper we challenge the traditional design used for forecasting competitions. We implement an online competition with a public leaderboard that provides instant feedback to competitors who are allowed to revise and resubmit forecasts. The results show that feedback significantly improves forecasting accuracy.
2011,1,14,"Lies, damn lies and statistics",https://robjhyndman.com/hyndsight/lewandowsky/,There&rsquo;s a nice article with this title by Stephan Lewandowsky on the ABC website today exploring the difference between anecdotes and data and the dangers of cherry-picking evidence.
2011,1,11,Six places left for the forecasting workshop,https://robjhyndman.com/hyndsight/swissworkshop2011a/,There are six places left for the forecasting workshop I am giving in Switzerland in June. If you were thinking of going book in fast!
2011,1,11,Authorship ethics,https://robjhyndman.com/hyndsight/authorship/,"With the constant pressure on academics to publish research papers there is a temptation for research groups to include &ldquo;coauthors&rdquo; who have not really made any contribution to the manuscript. This seems more prevalent in some fields (e.g. the health sciences) than others.
Occasionally I am asked to add an author to a paper that has already been accepted for publication in the International Journal of Forecasting. I am very reluctant to do this as it is hard to imagine how someone could be left off a paper while it goes through several revisions only to be remembered after the paper is accepted."
2011,1,9,Becoming a referee,https://robjhyndman.com/hyndsight/refereeing/,"I regularly get emails from people wanting to be referees for the International Journal of Forecasting usually with an accompanying CV. This is not how the process works.
Referees are almost always selected because they have previously written papers on a similar topic to the manuscript under review. If you want to be a referee then write good papers and get them published in scholarly journals. Very quickly you will be invited to referee papers in the same journals."
2011,1,9,Tips for academic talks,https://robjhyndman.com/hyndsight/talks/,"There is a nice post on Matt Might&rsquo;s blog entitled &ldquo;10 tips on how to give an academic talk&rdquo;. Check it out.
He recommends the following two books by Joey Asher.
See also my article on &ldquo;Giving an academic talk&rdquo;."
2011,1,1,The tourism forecasting competition,https://robjhyndman.com/publications/the-tourism-forecasting-competition/,We evaluate the performance of various methods for forecasting tourism demand. The data used include 366 monthly series 427 quarterly series and 518 yearly series all supplied to us by tourism bodies or by academics from previous tourism forecasting studies. The forecasting methods implemented in the competition are univariate and multivariate time series approaches and econometric models. This forecasting competition differs from previous competitions in several ways: (i) we concentrate only on tourism demand data; (ii) we include approaches with explanatory variables; (iii) we evaluate the forecast interval coverage as well as point forecast accuracy; (iv) we observe the effect of temporal aggregation on forecasting accuracy; and (v) we consider the mean absolute scaled error as an alternative forecasting accuracy measure.
2011,1,1,Quantifying the influence of local meteorology on air quality using generalized additive modelling,https://robjhyndman.com/publications/local-gams/,Quantifying the observed relationships between local meteorology and air pollution provides air quality managers with a knowledge base that may prove useful in understanding how climate change may potentially impact air quality. This paper presents the estimated response of ozone (O3) particulate matter ≤ 10 μm (PM10) and nitrogen dioxide (NO2) to individual local meteorological variables in Melbourne Australia over the period of 1999 to 2006. The relationships have been quantified after controlling for long-term trends seasonality weekly emissions spatial variation and temporal persistence using the framework of a generalized additive modelling (GAM).
2010,12,23,In praise of Dropbox,https://robjhyndman.com/hyndsight/dropbox/,"Every couple of years a new technology has a big impact on how I work. Gmail was one. My iPhone was another. And I rank Dropbox in the same category.
I get three huge benefits in using Dropbox:
  All my files are backed up online. The house can burn down and I know I can still get my files. Also if I&rsquo;m away from my desktop or laptop I can still access my files on my iPhone."
2010,12,22,CrossValidated Journal Club,https://robjhyndman.com/hyndsight/cvjournalclub/,"Journal Clubs are a great way to learn new research ideas and to keep up with the literature. The idea is that a group of people get together every week or so to discuss a paper of joint interest. This can happen within your own research group or department or virtually online.
There is now a virtual journal club operating in conjunction with CrossValidated.com. The first paper discussed was on text data mining."
2010,12,10,Hamming on research,https://robjhyndman.com/hyndsight/hamming/,Richard Hamming was an excellent mathematician who worked at the interface of mathematics and computer science. In 1986 he gave a wonderful talk entitled You and Your Research. Derek Smith on the AMS Graduate Student blog reminded me of it today. If you haven&rsquo;t read it previously stop work immediately and read it now.
2010,12,6,"Forecasting workshop: Switzerland, June 2011",https://robjhyndman.com/hyndsight/swissworkshop2011/,I will be running a workshop on Statistical Forecasting: Principles and Practice in Switzerland 20-22 June 2011. Check out the venue: Waldhotel Doldenhorn Kandersteg! So if you fancy a trip to the beautiful Swiss Alps next June read on&hellip;Outline Forecasting is required in many situations: deciding whether to build another power generation plant in the next five years requires forecasts of future demand; scheduling staff in a call centre next week requires forecasts of call volume; stocking an inventory requires forecasts of stock requirements.
2010,11,30,Data visualization videos,https://robjhyndman.com/hyndsight/data-visualization-videos/,"Probably everyone has seen Hans Rosling&rsquo;s famous TED talk by now. If not here it is:
  I recently came across a couple of other exceptional talks on data visualization:
Hans Rosling again: &ldquo;Let my dataset change your mindset&rdquo;. If only all statistics lecturers were this dynamic!
  David McCandless: &ldquo;The beauty of data visualization&rdquo;. Not so exciting as Hans but some great examples.
  And here&rsquo;s an hour-length documentary hosted by Hans Rosling called &ldquo;The Joy of Stats&rdquo;."
2010,11,30,Initializing the Holt-Winters method,https://robjhyndman.com/hyndsight/hw-initialization/,"The Holt-Winters method is a popular and effective approach to forecasting seasonal time series. But different implementations will give different forecasts depending on how the method is initialized and how the smoothing parameters are selected. In this post I will discuss various initialization methods.
Suppose the time series is denoted by $y_1\dotsy_n$ and the seasonal period is $m$ (e.g. $m=12$ for monthly data). Let $\hat{y}_{t+h|t}$ be the $h$-step forecast made using data to time $t$."
2010,11,27,A LaTeX template for a CV,https://robjhyndman.com/hyndsight/cv/,"Every researcher needs a Curriculum Vitae (Latin for &ldquo;course of life&rdquo;) or CV. You will need it for job applications for annual performance appraisal and just for keeping track of your publications. A CV typically contains lists of achievements including qualifications publications presentations awards plus teaching experience.
I&rsquo;ve created a LaTeX style for a CV to make it easy to produce something that looks good and is easy to maintain. You will need an up-to-date implementation of LaTeX because I&rsquo;m using the wonderful biblatex package (more on that in another post) which has only just become available as part of MikTeX and TeXLive."
2010,11,24,Tourism forecasting competition ends,https://robjhyndman.com/hyndsight/tfc2/,"And the winners are &hellip; Jeremy Howard and Lee C Baker. (See my earlier post for information about the competition.)
Jeremy describes his approach to seasonal time series in a blog post on Kaggle.com. Lee described his approach to annual time series in an earlier post.
A few lessons that come out of this:
  For data from a single industry using a global trend (i.e. estimated across all series) can be useful."
2010,11,20,Free open-source forecasting using R,https://robjhyndman.com/publications/r-foresight/,Summary  R is a free open source statistical computing environment which can be used for forecasting. It is available at www.r-project.org. Advantages of R include its (zero) price the large number of user-contributed packages its production-quality graphics and the possibility of extending it by linking fast compiled C/C++/Fortran code. Disadvantages of R include a steep learning curve and a certain slowness when dealing with truly massive amounts of data. R can be used to advantage by forecasters with a technical background who are comfortable with the command line and who may want to use R for additional Business Intelligence analyses and graphics beyond forecasting.
2010,11,15,The vector innovations structural time series framework: a simple approach to multivariate forecasting,https://robjhyndman.com/publications/vists/,The vector innovations structural time series framework is proposed as a way of modelling a set of related time series. Like all multivariate approaches the aim is to exploit potential inter-series dependencies to improve the fit and forecasts. The model is based around an unobserved vector of components representing features such as the level and slope of each time series. Equations that describe the evolution of these components through time are used to represent the inter-temporal dependencies.
2010,11,10,"Forecast estimation, evaluation and transformation",https://robjhyndman.com/hyndsight/forecastmse/,"I&rsquo;ve had a few emails lately about forecast evaluation and estimation criteria. Here is one I received today along with some comments.
 I have a rather simple question regarding the use of MSE as opposed to MAD and MAPE. If the parameters of a time series model are estimated by minimizing MSE why do we evaluate the model using some other metric e.g. MAD and MAPE. I could see that MAPE is not scale dependent."
2010,11,4,CrossValidated launched!,https://robjhyndman.com/hyndsight/crossvalidated/,"The CrossValidated Q&amp;A site is now out of beta and the new design and site name is live.

New design The new design looks great thanks to Jin Yang our designer-in-residence. Note the normal density icon for accepted answers and the site icon depicting a 5-fold cross-validation (light green for the test set and dark green for the training set). There is a faint background graphic in the header and footer from a program that tracks and plots a person&rsquo;s mouse movement."
2010,10,26,Different results from different software,https://robjhyndman.com/hyndsight/estimation/,"I&rsquo;ve had a few questions on this topic lately. Here is an email received today:
 I use Eviews to estimate time series but I have been checking out R recently and your Forecast package.
I cannot understand why 2 similar equations in Eviews and R are giving different estimated output. Your insights will be invaluable for my work.
The equations are:
 R: Arima(log(fee) order=c(110) seasonal=list(order=c(100) period=4) include.drift=TRUE) Eviews: dlog(fee) c ar(1) sar(1) Even with include."
2010,10,23,Getting started with XeLaTeX,https://robjhyndman.com/hyndsight/xelatex/,"By now most LaTeX users have probably heard of XeLaTeX if only because it is an option in the latest versions of the standard LaTeX editors such as TeXnicCenter WinEdt and TeXWorks. But most LaTeXers have probably not yet become XeLaTeXers. Why should you?
XeLaTeX is essentially a replacement for pdfLaTeX. It was primarily developed to enable better font handling especially non-Roman scripts. If you want to write in Telugu then XeLaTeX is going to make your life much easier."
2010,10,22,How to avoid annoying a referee,https://robjhyndman.com/hyndsight/how-to-avoid-annoying-a-referee/,"It&rsquo;s not a good idea to annoy the referees of your paper. They make recommendations to the editor about your work and it is best to keep them happy. There is an interesting discussion on stats.stackexchange.com on this subject. This inspired my own list below.
 Explain what you&rsquo;ve done clearly avoiding unnecessary jargon. Don&rsquo;t claim your paper contributes more than it actually does. (I refereed a paper this week where the author claimed to have invented principal component analysis!"
2010,10,19,Happy World Statistics Day!,https://robjhyndman.com/hyndsight/wsd/,"The United Nations has declared today &ldquo;World Statistics Day&rdquo;. I&rsquo;ve no idea what that means or why we need a WSD. Perhaps it is because the date is 20.10.2010 (except in North America where it is 10.20.2010). But then what happens from 2013 to 2099? And do we just forget the whole idea after 3112?
In any case if we are going to have a WSD let&rsquo;s use it to do something useful."
2010,10,17,Do something else,https://robjhyndman.com/hyndsight/do-something-else/,I&rsquo;ve written about taking a break from research before. Along the same lines there is some good advice on &ldquo;The Importance of Making Time for &ldquo;Real World&rdquo; Activities in Grad School&rdquo; over at the excellent AMS Graduate Student Blog.
2010,10,13,Animated plots in R and LaTeX,https://robjhyndman.com/hyndsight/animations/,"I like to use animated plots in my talks on functional time series partly because it is the only way to really see what is going on with changes in the shapes of curves over time and also because audiences love them! Here is how it is done.
For LaTeX you need to create every frame as a separate graphics file. Here is an example. First the R code:"
2010,10,13,Always listen to reviewers,https://robjhyndman.com/hyndsight/always-listen-to-reviewers/,This week I was asked to review a paper that I had seen before. It had been submitted to a journal a few months ago and I had written a detailed report describing some problems with the paper and noting a large number of typos that needed fixing. That journal had rejected the paper the authors had submitted it to a second journal and the paper ended up on my desk again for review.
2010,10,7,Joining an editorial board,https://robjhyndman.com/hyndsight/editorialboards/,"Being on the editorial board of a journal is a lot of work. I&rsquo;m currently Editor-in-Chief of the International Journal of Forecasting and previously I&rsquo;ve been Theory &amp; Methods Editor of the Australian &amp; New Zealand Journal of Statistics. Although it is time-consuming and often goes un-noticed there are some important rewards that make it worthwhile in my opinion.
  You are forced to read carefully a lot of papers in your area of interest."
2010,10,5,Why R is better than Excel for teaching statistics,https://robjhyndman.com/hyndsight/rvsexcel/,"This was the topic of a recent conversation on the Australian and New Zealand R mailing list. Here is an edited list of some of the comments made.
  R is free.
  R is well-documented.
  R runs (really well) on *nix as well as Windows and Mac OS.
  R is open-source. Trust in the R software is evident by its support among distinguished statisticians."
2010,10,4,The ARIMAX model muddle,https://robjhyndman.com/hyndsight/arimax/,There is often confusion about how to include covariates in ARIMA models and the presentation of the subject in various textbooks and in R help files has not helped the confusion. So I thought I&rsquo;d give my take on the issue. To keep it simple I will only describe non-seasonal ARIMA models although the ideas are easily extended to include seasonal terms. I will include only one covariate in the models although it is easy to extend the results to multiple covariates.
2010,10,4,Why every statistician should know about cross-validation,https://robjhyndman.com/hyndsight/crossvalidation/,"Surprisingly many statisticians see cross-validation as something data miners do but not a core statistical technique. I thought it might be helpful to summarize the role of cross-validation in statistics especially as it is proposed that the Q&amp;A site at stats.stackexchange.com should be renamed CrossValidated.com.
Cross-validation is primarily a way of measuring the predictive performance of a statistical model. Every statistician knows that the model fit statistics are not a good guide to how well a model will predict: high $R^2$ does not necessarily mean a good model."
2010,9,30,That syncing feeling,https://robjhyndman.com/hyndsight/syncing/,"Like many people I use more than one computer and I like to have all my files bookmarks and other settings synchronized across my computers. Fortunately that is getting easier as more tools are made available for keeping computers synchronized. So I thought it might be timely to review how to keep computers &ldquo;synced&rdquo;.
 Files: By far the best service is Dropbox. All files within the &ldquo;My Dropbox&rdquo; directory are backed up online and synchronized with any other computer associated with your account."
2010,9,29,Forecasting with long seasonal periods,https://robjhyndman.com/hyndsight/longseasonality/,"I am often asked how to fit an ARIMA or ETS model with data having a long seasonal period such as 365 for daily data or 48 for half-hourly data. Generally seasonal versions of ARIMA and ETS models are designed for shorter periods such as 12 for monthly data or 4 for quarterly data.
The ets() function in the forecast package restricts seasonality to be a maximum period of 24 to allow hourly data but not data with a larger seasonal frequency."
2010,9,20,Tourism forecasting competition results: part one,https://robjhyndman.com/hyndsight/tfc1/,"The first stage of the tourism forecasting competition on kaggle has finished. This stage involved forecasting 518 annual time series. Twenty one teams beat our Theta method benchmark which is a great result and well beyond our expectations. Congratulations to Lee Baker for winning stage one.
I am yet to learn what methods the top teams were using but we hope to write up a paper for the IJF describing the results."
2010,9,16,Your name is your brand,https://robjhyndman.com/hyndsight/names/,"As a researcher you want to become known as an expert in your field. You need people to recognize your name and associate it with your research. Consequently it is important to be consistent in the name you use on publications.
For example I could write under &ldquo;R Hyndman&rdquo; &ldquo;R J Hyndman&rdquo; &ldquo;Rob Hyndman&rdquo; &ldquo;Rob J Hyndman&rdquo; etc. I&rsquo;ve chosen the last of these and I try to use it on all publications."
2010,9,7,Demographic forecasting using functional data analysis,https://robjhyndman.com/seminars/demographic-forecasting/,"University of Wollongong 8 September 2010. Statistical Society of Australia Victorian Branch 28 September 2010. Updated version. September 2012.  Abstract:
Functional time series are curves that are observed sequentially in time. In demography such data arise as the curves formed by annual death rates as a function of age or annual fertility rates as a function of age. I will discuss methods for describing modelling and forecasting such functional time series data."
2010,9,2,How to fail a PhD,https://robjhyndman.com/hyndsight/phdfail/,"I read an interesting post today by Matt Might on &ldquo;10 reasons PhD students fail&rdquo; and I thought it might be helpful to reflect on some of the barriers to PhD completion that I&rsquo;ve seen. Matt&rsquo;s ideas are not all relevant to Australian PhDs so I have come up with my own list below. Here are the seven steps to failure.
1. Wait for your supervisor to tell you what to do A good supervisor will not tell you what to do."
2010,8,31,Econometrics and R,https://robjhyndman.com/hyndsight/econometrics-and-r/,"Econometricians seem to be rather slow to adopt new methods and new technology (compared to other areas of statistics) but slowly the use of R is spreading. I&rsquo;m now receiving requests for references showing how to use R in econometrics and so I thought it might be helpful to post a few suggestions here.
[![](/files/Farnsworth.png)](http://cran.r-project.org/doc/contrib/Farnsworth-EconometricsInR.pdf)  A useful on-line and free resource is ""[Econometrics in R](http://cran.r-project.org/doc/contrib/Farnsworth-EconometricsInR.pdf)"" by Grant Farnsworth. It covers some common econometric methods including heteroskedasticity in regression probit and logit models tobit regression and quantile regression."
2010,8,26,Job advertisements,https://robjhyndman.com/hyndsight/job-advertisements/,"Employers often contact me asking how to find a good statistician econometrician or forecaster for their organization. Students also ask me how to go about finding a job when they finish their degree. This post is for both groups hopefully making it easier for them to pair up appropriately.
First the mainstream media outlets are not usually good places to advertise. It seems that few people read printed newspapers anymore."
2010,8,25,Benchmarks for forecasting,https://robjhyndman.com/hyndsight/benchmarks/,"Every week I reject papers submitted to the International Journal of Forecasting because they present new methods without ever attempting to demonstrate that the new methods are better than existing methods. It is a policy of the journal that every new method must be compared to standard benchmarks and existing methods before the paper will even be considered for publication.
For univariate time series methods it is not difficult. As a minimum comparisons should be made against a naive method and a standard method such as an ARIMA model."
2010,8,17,Phenological change detection while accounting for abrupt and gradual trends in satellite image time series,https://robjhyndman.com/publications/bfast2/,A challenge in phenology studies is understanding what constitutes significant phenological change amidst background variation (e.g. noise) and ecosystem disturbances (e.g. fires). The majority of phenological studies have focussed on extracting critical points in the seasonal growth cycle (e.g. Start-of-spring) without exploiting the full temporal detail. Moreover the high degree of phenological variability between years demonstrates the necessity of distinguishing long term phenological change from temporal variability. Here we evaluate the phenological change detection ability of a method for detecting Breaks For Additive Seasonal and Trend (BFAST).
2010,8,13,Transforming data with zeros,https://robjhyndman.com/hyndsight/transformations/,"I&rsquo;m currently working with a hydrologist and he raised a question that occurs quite frequently with real data &mdash; what do you do when the data look like they need a log transformation but there are zero values?
I asked the question on stats.stackexchange.com and received some useful suggestions. What follows is a summary based on these answers my own experience plus a few papers I discovered that deal with the topic."
2010,8,9,The tourism forecasting competition,https://robjhyndman.com/hyndsight/tourism-forecasting-competition/,Recently I wrote a paper entitled &ldquo;The tourism forecasting competition&rdquo; in which we (i.e. George Athanasopoulos Haiyan Song Doris Wu and I) compared various forecasting methods on a relatively large set of tourism-related time series. The paper has been accepted for publication in the International Journal of Forecasting. (When I submit a paper to the IJF it is always handled by another editor. In this case Mike Clements handled the paper and it went through several revisions before it was finally accepted.
2010,8,6,Twenty rules for good graphics,https://robjhyndman.com/hyndsight/graphics/,One of the things I repeatedly include in referee reports and in my responses to authors who have submitted papers to the International Journal of Forecasting are comments designed to include the quality of the graphics. Recently someone asked on stats.stackexchange.com aboutbest practices for producing plots. So I thought it might be helpful to collate some of the answers given there and add a few comments of my own taken from things I&rsquo;ve written for authors.
2010,8,3,Exploratory graphics for functional data,https://robjhyndman.com/publications/interface2010/,"We survey some graphical tools for visualizing large sets of functional data represented by smooth curves. These graphical tools include the phase-plane plot singular value decomposition plot rainbow plot functional variants of the bagplot and the highest density region boxplot. The latter two techniques utilize the first two robust principal component scores Tukey&rsquo;s halfspace location depth and highest density regions.
The computer code and datasets are collected in the rainbow package for R which is available at the Comprehensive R Archive Network (CRAN)."
2010,7,27,Statistical Analysis StackExchange site now available,https://robjhyndman.com/hyndsight/stats-stackexchange/,"The Q&amp;A site for statistical analysis data mining data visualization and everything else to do with data analysis has finally been launched. Please head over to
stats.StackExchange.com and start asking and answering questions.
Also spread the word to everyone else who may be interested &mdash; work colleagues students etc. The more people who use the site the better it will be. There are already 170 questions 513 answers and 387 users."
2010,7,21,Short-term load forecasting based on a semi-parametric additive model,https://robjhyndman.com/publications/aupec2010/,Short-term load forecasting is an essential instrument in power system planning operation and control. Many operating decisions are based on load forecasts such as dispatch scheduling of generating capacity reliability analysis and maintenance planning for the generators. Overestimation of electricity demand will cause a conservative operation which leads to the start-up of too many units or excessive energy purchase thereby supplying an unnecessary level of reserve. On the contrary underestimation may result in a risky operation with insufficient preparation of spinning reserve causing the system to operate in a vulnerable region to the disturbance.
2010,7,17,More StackExchange sites,https://robjhyndman.com/hyndsight/more-stackexchange-sites/,"The StackExchange site on Statistical Analysis is about to go into private beta testing. This is your last chance to commit if you want to be part of the private beta testing. Don&rsquo;t worry if you miss out &mdash; it will only be a week before it is then open to the public.
There is also a StackExchange site proposal for TeX LaTeX and friends. Presumably that means that most of the LaTeX questions on StackOverflow will then move to this new site."
2010,7,15,The falling standard of English in research,https://robjhyndman.com/hyndsight/english/,"It seems that most journals no longer do any serious copy-editing and the standard of English is falling. Today I was reading an article from the European Journal of Operational Research which is supposedly a good OR journal (current impact factor over 2). Take this for an example from the first page of this paper:
 If the learned patterns are unstable the learning tools would produce inconsistent concepts. To overcome this difficult situation we employed artificial neural networks (ANNs NNs) for helping the learning task."
2010,7,11,Academic citations in the popular press,https://robjhyndman.com/hyndsight/academic-citations-in-the-popular-press/,It is very unusual for a newspaper article to cite an academic paper unless it is in Nature Science or the Lancet. Mostly what we write is too technical and assumes too much background knowledge for it to be accessible to anyone but specialists. So I was pleasantly surprised to find a reference to the International Journal of Forecastingin a recent Wall Street Journalarticle. It is a citation of a 1996 article so in terms of scientific research it is a bit like quoting the Magna Carta but a citation nevertheless.
2010,7,6,Forecasting with Exponential Smoothing: the State Space Approach,https://robjhyndman.com/expsmooth/,"Rob J Hyndman Anne B Koehler J Keith Ord Ralph D Snyder (Springer 2008).
 .verticalhorizontal { display: table-cell; vertical-align: middle; }        
Exponential smoothing methods have been around since the 1950s and are still the most popular forecasting methods used in business and industry. However a modelling framework incorporating stochastic models likelihood calculation prediction intervals and procedures for model selection was not developed until relatively recently."
2010,7,6,Forecasting: methods and applications,https://robjhyndman.com/forecasting/,"This book was published in 1998 and for nearly 20 years I maintained an associated website at this address.
The data sets from the book can be found in the fma package for R.
The solutions to exercises can be downloaded here.
The book is now out-of-date. I recommend my new book entitled Forecasting: principles and practice."
2010,7,6,Musings,https://robjhyndman.com/musings/,"I used to be a Christian and for several years I wrote a blog at this address exploring ideas of faith and belief. I eventually lost my faith as explained in my book Unbelievable. When I reorganized this website there didn&rsquo;t seem much point spending time converting those old posts so they are no longer here.
Some of the posts were reworked into my book including the last one announcing my deconversion."
2010,6,11,Use fake data and real data,https://robjhyndman.com/hyndsight/use-fake-data-and-real-data/,"When developing new statistical methods it is very useful to test them on both fake data (i.e. simulations) and real data.
Testing on fake data is useful because then you know the &ldquo;true&rdquo; answer and can check the procedure under ideal conditions. If your method doesn&rsquo;t work when the data are designed for the task it is unlikely to work in real conditions. Fake data also enables you to test the robustness of your method when the conditions aren&rsquo;t perfect &ndash; for example try adding some nasty outliers and see if the method still works."
2010,6,9,Should you make your working papers public?,https://robjhyndman.com/hyndsight/working-papers/,"There seems to be two points of view on this with different practices in different disciplines.
  Some researchers do not make their work public until after it has been accepted for publication in a journal. Until that time drafts of papers are only circulated to close confidants and usually marked &ldquo;Do not distribute&rdquo;.
  Working papers are published on web sites and in web repositories (such as arXiv or RePEc) as soon as they are finished at about the same time they are submitted to a journal."
2010,6,9,Coherent functional forecasts of mortality rates and life expectancy,https://robjhyndman.com/seminars/isf2010/,Talk to be given at the International Symposium on Forecasting San Diego 20-23 June 2010.
2010,6,2,Update on a StackExchange site for statistical analysis,https://robjhyndman.com/hyndsight/stackexchange2/,"About six weeks ago I proposed that there should be a Stack Exchange site for questions on data analysis statistics data mining machine learning etc. I can finally report that there has been substantial progress on this.
The formal proposal is now at Area 51 where the scope of the new site is being developed and voted on in a democratic way. The site has been in a private beta state for a week or so but is now open for anyone to join in."
2010,5,26,Google scholar alerts,https://robjhyndman.com/hyndsight/google-scholar-alerts/,"A couple of weeks ago Google scholar added a facility to provide email alerts on new articles associated with specific search queries. First do the search then click the envelope at top left of screen. For example here is a search on &ldquo;exponential smoothing&rdquo; since 2000.

Note the envelope at the top marked New! Click it to get the following screen.

Those results show some of the flaws in Google Scholar &ndash; the dates are not always correct (the first paper listed above appeared in 2004) and there are unresolved duplicates."
2010,5,20,Online mathematical resources,https://robjhyndman.com/hyndsight/online-mathematical-resources/,DLMF For nearly 50 years a standard reference in mathematical work has been Abramowitz and Stegun&rsquo;s (1964) Handbook of Mathematical Functions with Formulas Graphs and Mathematical Tables. It has provided a marvellous collection of results and tables that have been indispensable for a generation of mathematicians. I&rsquo;ve used it to look up computationally efficient methods for calculating Bessel functions or gamma functions or to find one of those trigonometric identities I learned in high school and no longer remember.
2010,5,9,Scheduling meetings,https://robjhyndman.com/hyndsight/scheduling-meetings/,"I don&rsquo;t go to many meetings as I find they are largely a waste of time. In fact I have the following poster on my office wall to remind everyone who walks in not to ask me to attend a meeting!
But I&rsquo;m now a chief investigator of an NHMRC grant and I have to meet with other members of the team from time to time. We&rsquo;ve started using Doodle to schedule our meetings and it is so good I thought I should share it."
2010,5,6,Forecasting age-related changes in breast cancer mortality among white and black US women,https://robjhyndman.com/publications/brca-bwus/,The disparity in breast cancer mortality rates among white and black US women is widening with higher mortality rates among black women. We apply functional time series models on age-specific breast cancer mortality rates for each group of women and forecast their mortality curves using exponential smoothing state-space models with damping. The data were obtained from the Surveillance Epidemiology and End Results (SEER) program of the US. Mortality data were obtained from the National Centre for Health Statistics (NCHS) available on the SEER*Stat database.
2010,4,20,A StackExchange site for statistical analysis?,https://robjhyndman.com/hyndsight/stackexchange/,Regular readers of this site will know I&rsquo;m a fan of using Stack Overflow for questions about LaTeX R and other areas of programming. Now the people who produce Stack Overflow are planning on setting up several new sites for asking questions about other topics and are seeking proposals. I have proposed that there should be a site for questions on data analysis statistics data mining machine learning etc.
2010,3,25,Making a poster in beamer,https://robjhyndman.com/hyndsight/beamer-poster/,"This week I made my first poster. Although I&rsquo;ve been an academic for more than 20 years I&rsquo;ve never had to make a poster before. Some of my coauthors have made posters about our joint research and two of them have even won prizes (although I can&rsquo;t take any credit for them). But this week our department is displaying posters from all research staff about our recent work.
Here is my poster (click for pdf version):"
2010,3,21,My standard LaTeX preamble,https://robjhyndman.com/hyndsight/latex-preamble/,When I was a PhD student I found I needed a lot of LaTeX functionality that did not then exist. So I wrote my own package which has served me well for about 20 years. It is called HyTeX.sty (the name being a shameless take-off of LaTeX from Leslie Lamport as well as a homonym of High-Tech). The advantage of having my own package is that almost every file starts with
2010,3,10,Using the command line in Windows,https://robjhyndman.com/hyndsight/using-the-command-line-in-windows/,Jeromy Anglim is a local blogger who covers a lot of the same territory as this blog. His latest post on running command line programs in Windows is particularly helpful.
2010,2,28,"Rainbow plots, bagplots and boxplots for functional data",https://robjhyndman.com/publications/rainbow-fda/,"We propose new tools for visualizing large numbers of functional data in the form of smooth curves or surfaces. The proposed tools include functional versions of the bagplot and boxplot and make use of the first two robust principal component scores Tukey&rsquo;s data depth and highest density regions.
By-products of our graphical displays are outlier detection methods for functional data. We compare these new outlier detection methods with existing methods for detecting outliers in functional data and show that our methods are better able to identify the outliers."
2010,2,15,Top four LaTeX mistakes,https://robjhyndman.com/hyndsight/top-four-latex-mistakes/,There is a nice post today by John Cook on the top four LaTeX mistakes. I see these all the time in draft papers by my students and co-authors.
2010,2,10,Why referee?,https://robjhyndman.com/hyndsight/why-referee/,"There are several reasons why researchers should be willing to provide referee reports.
  You learn a lot. If the paper is in your area then writing a referee report forces you to read it very carefully and engage closely with the research of other people in your field. There&rsquo;s no better way of understand what is going on in your field.
  You get better known by the research leaders in your area."
2010,2,10,Writing a referee report,https://robjhyndman.com/hyndsight/referee-reports/,"As an editor I like to see referee reports comprising three sections:
  A general summary of the paper and the contribution it makes. You need to highlight here what is new and interesting about the paper as well as give a summary in a few sentences.
  The major problems that need addressing. This is probably the most important section of your report where you explain the main problems."
2010,2,7,Functionalization of microarray devices: process optimization using a multiobjective PSO and multiresponse MARS modeling,https://robjhyndman.com/publications/microarray-optimization/,An evolutionary approach for the optimization of microarray coatings produced via sol-gel chemistry is presented. The aim of the methodology is to face the challenging aspects of the problem: high dimensional variable space constraints on the independent variables multiple responses expensive or time-consuming experimental trials expected complexity of the functional relationships between independent and response variables. The proposed approach iteratively select a set of experiments by combining a multiobjective Particle Swarm Optimization (PSO) and a multiresponse Multivariate Adaptive Regression Spines (MARS) model.
2010,2,5,Using functional data analysis models to estimate future time trends of age-specific breast cancer mortality for the United States and England-Wales,https://robjhyndman.com/publications/brca-usew/,"Background: Mortality/incidence predictions are used for planning public health resources and need to accurately reflect age-related changes through time. We present a new forecasting model to estimate future trends in age-related breast cancer mortality for the United States and England-Wales.
Material and methods: We use functional data analysis techniques to model breast cancer mortality-age relationships in the United States from 1950 to 2001 and England-Wales from 1950 to 2003 and estimate 20-year predictions using a new forecasting method."
2010,1,14,Detecting trend and seasonal changes in satellite image time series,https://robjhyndman.com/publications/bfast1/,A wealth of remotely sensed time series covering large areas is now available to the earth science community. Change detection methods are often not capable of detecting land cover changes within time series that are heavily influenced by seasonal climatic variations. Detecting change within the trend and seasonal components of time series enables the detection of different types of changes. Changes occurring in the trend component indicate disturbances (e.g. insect attack) while changes occurring in the seasonal component indicate phenological changes (e.
2010,1,2,Density forecasting for long-term peak electricity demand,https://robjhyndman.com/publications/peak-electricity-demand/,Abstract: Long-term electricity demand forecasting plays an important role in planning for future generation facilities and transmission augmentation. In a long term context planners must adopt a probabilistic view of potential peak demand levels therefore density forecasts (providing estimates of the full probability distributions of the possible future values of the demand) are more helpful than point forecasts and are necessary for utilities to evaluate and hedge the financial risk accrued by demand variability and forecasting uncertainty.
2010,1,1,Business Forecasting Methods,https://robjhyndman.com/publications/iess2/,
2010,1,1,Forecasting Overview,https://robjhyndman.com/publications/iess3/,
2010,1,1,Moving Averages,https://robjhyndman.com/publications/iess1/,
2010,1,1,Encouraging replication and reproducible research,https://robjhyndman.com/publications/replication/,
2010,1,1,Changing of the guard,https://robjhyndman.com/publications/changing-of-the-guard/,
2009,12,27,Using DOIs,https://robjhyndman.com/hyndsight/doi/,"Almost all papers these days have a DOI and it is worth knowing how to use them.
At the top or bottom of the first page of a paper you will see something like this:
doi:10.1016/j.csda.2006.07.028  This is a unique and permanent identifier for the paper known as a &ldquo;Digital Object Identifier&rdquo;. The part before the forward slash (10.1016 in the example above) identifies the naming authority (in this case Elsevier) and the part after the forward slash (j."
2009,12,2,Replications and reproducible research,https://robjhyndman.com/hyndsight/replications/,"Reproducible research One of the best ways to get started with research in a new area is to try to replicate some existing research. In doing so you will usually gain a much better understanding of the topic and you will often discover some problems with the research or develop ideas that will lead to a new research paper.
Unfortunately a lot of papers are not reproducible because the data are not made available or the description of the methods are not detailed enough."
2009,11,25,Exponential smoothing and non-negative data,https://robjhyndman.com/publications/expsmooth-nonnegative/,"The most common forecasting methods in business are based on exponential smoothing and the most common time series in business are inherently non-negative. Therefore it is of interest to consider the properties of the potential stochastic models underlying exponential smoothing when applied to non-negative data. We explore exponential smoothing state space models for non-negative data under various assumptions about the innovations or error process.
We first demonstrate that prediction distributions from some commonly used state space models may have an infinite variance beyond a certain forecasting horizon."
2009,11,24,Learning by video,https://robjhyndman.com/hyndsight/video/,"There are some nice online videos available on various aspects of statistics and mathematics that might be helpful to students trying to learn about new areas.
A search on YouTube will lead to a few fairly basic videos.
 Statistics playlists Mathematics playlists  A better place to go is YouTube EDU which contains material from universities.
Something similar is offered at iTunesU
But the best stuff is on Academic Earth."
2009,11,11,Controlling figure and table placement in LaTeX,https://robjhyndman.com/hyndsight/latex-floats/,"It can be frustrating trying to get your figures and tables to appear where you want them in a LaTeX document. Sometimes they just seem to float off onto another page of their own accord. Here is a collection of tools and ideas that help you get control of those pesky floats.Use the placement options: h t b and p. For example
\begin{figure}[htb]  causes LaTeX to try to fit the float &ldquo;here&rdquo; or at the &ldquo;top&rdquo; of the current page (or the next page) or at the &ldquo;bottom&rdquo; of the current page (or the next page)."
2009,11,9,More on the evils of statistical tests,https://robjhyndman.com/hyndsight/value-of-p-values/,"Check out the two posts by Galit Shmueli over at Bzst on hypothesis tests: one on the value of p-values and another on one-sided tests.
She says &ldquo;Shockingly enough people seem to really want to use p-values even if they don&rsquo;t understand them.&rdquo; That mirrors my experience too. Confidence intervals are much more useful because they provide a measure of the size of an effect rather than testing if it is equal to some prespecified value."
2009,10,15,"""Elements of Statistical Learning"" now online",https://robjhyndman.com/hyndsight/esl2/,"In the past couple of days the authors of several blogs have noted that the wonderful book The Elements of Statistical Learning: Data Mining Inference and Prediction by Hastie Tibshirani and Friedman (2nd ed. 2009) is now available for free download in pdf format.
Of course it is also nice to have a hard copy. Click the image to purchase from Amazon."
2009,10,15,Using personal pronouns in research writing,https://robjhyndman.com/hyndsight/personal-pronouns/,"Should you use &ldquo;I&rdquo; or &ldquo;we&rdquo; or neither in your thesis or paper?
Thoughts on this have changed over the years. Traditionally using personal pronouns like &ldquo;I&rdquo; and &ldquo;we&rdquo; was frowned on. Instead of saying &ldquo;In Section 3 I have compared the results from method X with those of method Y&rdquo; you were expected to write &ldquo;In section 3 the results from method X are compared with those from method Y&rdquo;."
2009,10,14,Attending research seminars,https://robjhyndman.com/hyndsight/attending-research-seminars/,"Most research students don&rsquo;t seem to attend seminars. When asked they usually say the seminars are not on their topic or they don&rsquo;t understand them or they find them boring or some other similar reason. I think this is because students don&rsquo;t understand the purpose of research seminars and have not learned how to listen to them.
Admittedly many research seminars are badly presented and seminar speakers also frequently misunderstand the purpose of the seminar which makes the problem worse."
2009,10,14,Squeezing space with LaTeX,https://robjhyndman.com/hyndsight/squeezing-space-with-latex/,I&rsquo;ve been writing a grant application with a 10-page limit and as usual it is difficult to squeeze everything in. No I can&rsquo;t just change the font as it has to be 12 point with at least 2 cm margins on an A4 page. Fortunately LaTeX is packed full of powerful features that help in squeezing it all in. Here are some of the tips I&rsquo;ve used over the years.
2009,10,1,Converting eps to pdf,https://robjhyndman.com/hyndsight/converting-eps-to-pdf/,"Simply include the package epstopdf. Then when you use pdflatex the eps files will be automatically converted to pdf at compile time. (The conversion only happens the first time you process the file and is skipped if there is already a pdf file with the same name.)
For example:
\documentclass{article} \usepackage{graphicxepstopdf} \begin{document} \includegraphics[width=\textwidth]{fig1} \end{document}  Then even though the only graphics file available is fig1.eps this will still be processed ok using pdflatex or pdftexify."
2009,9,28,The 7 secrets of highly successful PhD students,https://robjhyndman.com/hyndsight/7secrets/,It seems everyone has 7 secrets to success and now someone has hopped on the 7-secrets bandwagon with something for PhD students. Thinkwell is an Australian company offering a seminar and associated work book on &ldquo;The 7 secrets of highly successful PhD students&rdquo;. I bought the book out of curiosity but &ldquo;book&rdquo; is a gross exaggeration &ndash; only eleven pages of fairly simplistic advice. I hope the seminar has more substance.
2009,9,27,Writing an abstract,https://robjhyndman.com/hyndsight/abstracts/,The abstract is probably the most important part of a paper. Many readers will not read anything else so you need to grab their attention and get your main message across as clearly and succinctly as possible. It is not meant to be an introduction to the paper but a summary of the paper. In a single paragraph a reader can learn the purpose of the research your general approach to the problem your main results and the most important conclusions.
2009,9,18,Workflow in R,https://robjhyndman.com/hyndsight/workflow-in-r/,"This came up recently on StackOverflow. One of the answers was particularly helpful and I thought it might be worth mentioning here. The idea presented there is to break the code into four files all stored in your project directory. These four files are to be processed in the following order.
 load.R This file includes all code associated with loading the data. Usually it will be a short file reading in data from files."
2009,9,18,Writing mathematics,https://robjhyndman.com/hyndsight/writing-mathematics/,"Mathematics has its own particular conventions and rules when it comes to writing. There have been several attempts to write them down. The most famous is Halmos&rsquo;s excellent essay &ldquo;How to write mathematics&rdquo;. Other good sources of advice are the following two books:
(I reviewed these two books in the Australian and New Zealand Journal of Statistics a few years ago.)
But if you just want a quick summary I recommend Dave Richeson&rsquo;s blog entry &ldquo;The nuts and bolts of writing mathematics&rdquo;."
2009,9,13,Take a break,https://robjhyndman.com/hyndsight/take-a-break/,"Occasionally the best research is done in long periods of concentrated effort. Allegedly Isaac Newton used to sometimes write for eight hours standing up without a break.
At other times taking a break helps the research process. Think of Archimedes and his Eureka moment. Many of my best ideas come while walking or taking a shower. In fact I once suggested to my head of department that we should have showers installed in every office as it would increase the quality of our research."
2009,9,13,Finding an R function,https://robjhyndman.com/hyndsight/finding-an-r-function/,"Suppose you want a function to fit a neural network. What&rsquo;s the best way to find it? Here are three steps that help to find the elusive function relatively quickly.
First use help.search(&quot;neural&quot;) or the shorthand ??neural. This will search the help files of installed packages for the word &ldquo;neural&rdquo;. Actually fuzzy matching is used so it returns pages that have words similar to &ldquo;neural&rdquo; such as &ldquo;natural&rdquo;. For a stricter search use help."
2009,9,2,Statistics education journals,https://robjhyndman.com/hyndsight/statistics-education-journals/,"In many research universities there can be a tension that arises when great teachers don&rsquo;t publish much. I believe there is a place for excellent teachers who do limited research within a strong research university but their contribution is considerably enhanced if they share their teaching insights. There are at least three reputable research journals for publishing articles on statistics education:
  Journal of Statistics Education
  Statistics Education Research Journal"
2009,9,2,Mathematical research and the internet,https://robjhyndman.com/hyndsight/tao-lecture/,"On Monday night I attended a lecture by Terry Tao on &ldquo;Mathematical research and the internet&rdquo;. Terry is Australia&rsquo;s most famous mathematician our only Field&rsquo;s medalist and one of the most active mathematical bloggers in the world. He has been described as the &ldquo;Mozart of mathematics&rdquo; for his remarkable precocity and prolific output. The slides of his talk are available on his blog site.
It was an interesting talk with excellent slides marred only by the poor sound system and his bad habit of mumbling."
2009,8,27,How good are economic forecasts?,https://robjhyndman.com/hyndsight/how-good-are-economic-forecasts/,I wrote last week that &ldquo;macroeconomic forecasts are little better than shooting blindfold&rdquo;. I don&rsquo;t know if it was connected or not but on the same day a journalist (Richard Pullin) from Reuters phoned me to ask about assessing some economic forecasts. He wanted to compare the accuracy of several economic forecasts for Japan and he wasn&rsquo;t sure how to go about it. I helped him to calculate the MASE for the different forecasts and the results have now been published.
2009,8,26,Research supervision workshop,https://robjhyndman.com/hyndsight/supervision-workshop/,"Today I gave a workshop for supervisors of postgraduate students. Mostly I talked about creating a team environment for postgraduate students rather than the traditional model (at least in statistics and econometrics) of each student working in isolation.
The slides are available here in presentation form or in handout form. Actually these are an edited version of the slides as I accidentally left out a couple of the photographs in the workshop and I&rsquo;ve omitted slides that I didn&rsquo;t end up covering in the workshop."
2009,8,25,Seek help when it's needed,https://robjhyndman.com/hyndsight/seek-help/,I don&rsquo;t think I&rsquo;ve had a research student who did not think about giving up at some point. It was part through my second year when I felt like giving up. I felt I was not going to be able to finish my thesis and that I would be better off throwing in the towel and doing something else. Fortunately I couldn&rsquo;t think of anything better to do plus I hate giving up on anything so I persevered and it turned out ok.
2009,8,24,Why I don't like statistical tests,https://robjhyndman.com/hyndsight/tests/,"It may come as a shock to discover that a statistician does not like statistical tests. Isn&rsquo;t that what statistics is all about? Unfortunately in some disciplines statistical analysis does seem to consist almost entirely of hypothesis testing and therein lies the problem.
The standard practice is to construct a hypothesis test to determine if some attribute of the data is &ldquo;significant&rdquo; or not with the standard p-value threshold of 5%."
2009,8,23,R help on StackOverflow,https://robjhyndman.com/hyndsight/r-help-on-stackoverflow/,"Ever since I began using R about ten years ago the best place to find R help was on the R-help mailing list. But it is time-consuming searching through the archives trying to find something from a long time ago and there is no way to sort out the good advice from the bad advice.
But now there is a new tool and it is very neat. Head over to the R tag on StackOverflow."
2009,8,21,Backing up,https://robjhyndman.com/hyndsight/backing-up/,"Ever since I deleted the only copy of my honours thesis one week before it was due to be handed in I&rsquo;ve been obsessive about backups often to the amusement of my family and colleagues. But every time one of them loses a file or has a hard-disk fail the smiles fade and they ask for advice.
I&rsquo;ve used many systems over the years each one a little better than the last."
2009,8,17,Forecasting the recession,https://robjhyndman.com/hyndsight/forecasting-the-recession/,"Forecasters are under the pump with a recession that many didn&rsquo;t see coming. As I don&rsquo;t do any macroeconomic forecasting I can sit back and smile smugly at some of my colleagues while I work on simpler problems such as forecasting in epidemiology demography and energy demand.
Some of those colleagues are cited in the Wall Street Journal today. The following quotation is interesting:
 The spate of cloudy crystal balls highlighted an uncomfortable reality about telling the future: It is hardest when it is most important."
2009,8,14,Maintaining local LaTeX files,https://robjhyndman.com/hyndsight/localtexmf/,If you use LaTeX then you probably have a bib file — a data base of all the papers and books that you have cited. It is much more efficient to keep one database in one location than have multiple copies of it floating around your hard drive. (Or even worse have different bib files created for different papers.) You might also have a few of your own style files and again it is best to keep these in a central location and not have duplicates all over the place.
2009,8,13,Sight what you cite,https://robjhyndman.com/hyndsight/sight-what-you-cite/,"There seems to be a widespread practice of researchers citing papers they have never even seen let alone read. For example
  Some papers claim to do something new when it has already been done in one of the papers cited.
  Some articles are cited that apparently have little to do with the reason given for the citation or which argue the opposite point of view to what is claimed."
2009,8,5,Songs of Statistics,https://robjhyndman.com/hyndsight/songs-of-statistics/,"If you love statistics (don&rsquo;t we all?) and can write Chinese (which rules me out) you might like to contribute to the Chinese National Bureau of Statistics celebrations of the 60th anniversary of the &ldquo;founding of New China&rdquo;. They are calling for submissions of prose poetry or song which will &ldquo;enhance people&rsquo;s patriotic feelings statistics and confidence&rdquo;. Here is an English translation of the page.
Some further translations are on the WSJ page."
2009,8,3,Writing responses to referee reports,https://robjhyndman.com/hyndsight/responses/,"I&rsquo;ve been spending time writing response letters lately. I&rsquo;ve also been reading lots of response letters from authors wanting their stuff published in the International Journal of Forecasting. I thought it might be useful to collate a few thoughts on the subject.
  No grovelling. I sometimes get response letters that start off with a paragraph of inane and obsequious fawning. The real response only begins after a paragraph of flattery which makes me wonder why I&rsquo;ve never won the Nobel prize."
2009,8,2,Managing a bibliographic database,https://robjhyndman.com/hyndsight/managing-a-bibliographic-database/,"All researchers need to maintain a database of papers they have read cited or simply noted for later reference. For those of us using LaTeX the database is in the BibTeX format and is stored as a simple text file (a bib file) that can be edited using a text editor such as WinEdt.
But it is often easier to edit the file using specialist software. My current favourite tool is JabRef."
2009,7,27,Finding LaTeX symbols,https://robjhyndman.com/hyndsight/finding-latex-symbols/,"All LaTeX users will sometimes need a symbol for which they don&rsquo;t know the command. If you use WinEdt there is a neat drop-down menu of some common symbols that can be helpful but it is necessarily limited. What do you do when you want the male and female symbols?
For several years I&rsquo;ve been using the comprehensive symbol list whenever I need a symbol in LaTeX that I can&rsquo;t recall."
2009,7,23,Forecasting functional time series,https://robjhyndman.com/publications/forecasting-functional-time-series-2/,We propose forecasting functional time series using weighted functional principal component regression and weighted functional partial least squares regression. These approaches allow for smooth functions assign higher weights to more recent data and provide a modeling scheme that is easily adapted to allow for constraints and other information. We illustrate our approaches using age-specific French female mortality rates from 1816 to 2006 and age-specific Australian fertility rates from 1921 to 2006 and show that these weighted methods improve forecast accuracy in comparison to their unweighted counterparts.
2009,7,21,Why Word is a bad choice for academic writing,https://robjhyndman.com/hyndsight/why-word-is-a-bad-choice-for-academic-writing/,For years I&rsquo;ve been telling everyone who would listen that MS-Word may sometimes be useful for short notes or for making a &ldquo;Back in 5 minutes&rdquo; sign to stick on your door but if you want to write a serious document like an academic paper a book or a thesis then you should use a serious tool such as LaTeX. For those who are not yet convinced Ben Klemens has a nice article entitled &ldquo;Why Word is a terrible program&rdquo;.
2009,7,15,Mathematical genealogy,https://robjhyndman.com/hyndsight/mathematical-genealogy/,"Having a PhD student is like having a child. I have had many such &ldquo;children&rdquo; graduate and have another few &ldquo;on the way&rdquo;. (See here for my offspring.)
Going in the other direction here is my family tree (or one branch of it) compiled from the Mathematical Genealogy Project. Each successive person is the doctoral student of the person before him. (No females here!)
     Erhard Weigel Ph."
2009,7,13,Searching the research literature,https://robjhyndman.com/hyndsight/searching-the-research-literature/,"Most students seem to go to Google first. This is not a good strategy. Google Scholar is much better as it filters out all the junk. Scopus is another engine that aims to do a similar thing. It is better organized but not so complete. ISI WOK is also not as complete as Google Scholar but is particularly good at tracking citations.
  Google scholar
  Scopus
  ISI Web of Knowledge"
2009,7,12,Nonparametric time series forecasting with dynamic updating,https://robjhyndman.com/publications/dynamic-updating1/,
2009,7,5,Monitoring processes with changing variances,https://robjhyndman.com/publications/monitoring-processes/,Statistical process control (SPC) has evolved beyond its classical applications in manufacturing to monitoring economic and social phenomena. This extension requires consideration of autocorrelated and possibly non-stationary time series. Less attention has been paid to the possibility that the variance of the process may also change over time. In this paper we use the innovations state space modeling framework to develop conditionally heteroscedastic models. We provide examples to show that the incorrect use of homoscedastic models may lead to erroneous decisions about the nature of the process.
2009,6,25,English academic writing,https://robjhyndman.com/seminars/english-academic-writing/,Presentation to College of Management University of Fuzhou China. 25 June 2009.
2009,6,23,Extreme forecasting,https://robjhyndman.com/seminars/extreme-forecasting/,"Keynote address International Symposium on Forecasting June 2009. Abstract
Extremely bad data extremely poor methods and extremely difficult problems will be used as the basis of some extremely useful lessons. I will describe three cases from my consulting experience and draw some general lessons that are widely applicable.
The first case involved forecasting passenger traffic on an Australian airline. The data showed variations due to school holidays major sporting events competitor activity industrial disputes changes in fare structures and other factors."
2009,6,1,Clive Granger (1934-2009),https://robjhyndman.com/hyndsight/granger/,Sir Clive Granger has died at the age of 74. There are some nice obituaries in the New York Times and the Daily Telegraph. Also his Wikipedia page has some good information. I met Clive on several occasions and he was &ldquo;a scholar and a gentleman&rdquo; a remarkably humble man given his outstanding achievements and someone who was always willing to help young researchers. The world of forecasting will miss him.
2009,5,28,Akram's story,https://robjhyndman.com/hyndsight/akrams-story/,Muhammad Akram was my PhD student a few years ago and has remained a good friend since he moved on. Here is an interview he recently gave about moving to Australia. Thanks Akram for the kind words about me!
2009,5,18,Prediction markets,https://robjhyndman.com/hyndsight/prediction-markets/,Andrew Leigh has a nice piece in today&rsquo;s AFR on forecasting via prediction markets
2009,5,1,Statistical support for HDR students,https://robjhyndman.com/seminars/statistical-support-for-hdr-students/,Presentation to a meeting of Australian Deans and Directors of Graduate Studies 1 May 2009.
2009,4,8,Neil Postman on technological change,https://robjhyndman.com/hyndsight/neil-postman-on-technological-change/,"Neil Postman was Professor of Communication at New York University until his death in 2003. He wrote many wonderfully insightful and thought-provoking articles and books about television education technology and childhood. I recently came across a speech he gave in 1998 on &ldquo;Five things we need to know about technological change&rdquo;. Here is an online transcript. The five things are:
  That we always pay a price for technology; the greater the technology the greater the price."
2009,3,12,Accessing journal articles online,https://robjhyndman.com/hyndsight/accessing-journal-articles-online/,"When searching for research articles online I often find that the article is unavailable unless I go through the Monash library website especially when working from home. Here are two solutions to the problem
  Within Google scholar go to &ldquo;Scholar preferences&rdquo; and under library links search for &ldquo;Monash&rdquo;. Tick the entry &ldquo;Monash University - Check for full text&rdquo;. Then save your preferences (button at bottom of page). Next time you do a Google scholar search a link labelled &ldquo;Check for full text&rdquo; will appear beside each entry."
2009,2,1,Statistician: the dream job,https://robjhyndman.com/hyndsight/statistician-the-dream-job/,"So what is the ultimate job? According to Hal Varian chief economist at Google it is being a statistician (see this interview)
Here he is on YouTube with a longer comment:
  Statistics - Dream Job of the next decade
From a keynote presentation to the 2008 Almaden Institute - &ldquo;Innovating with Information&rdquo;.
The full presentation is available at http://www.almaden.ibm.com/institute/agenda.shtml
Hal Varian makes the argument that with data in huge supply and statisticians in short supply being a statistician has to be the &lsquo;really sexy job for the 2010s&rsquo;."
2009,1,16,Rule induction for forecasting method selection: meta-learning the characteristics of univariate time series,https://robjhyndman.com/publications/forecast-rules/,For univariate forecasting there are various statistical models and computational algorithms available. In real-world exercises too many choices can create difficulties in selecting the most appropriate technique especially for users lacking sufficient knowledge of forecasting. This study focuses on rule induction for forecasting method selection by understanding the nature of historical forecasting data. A novel approach for selecting a forecasting method for univariate time series based on measurable data characteristics is presented that combines elements of data mining meta-learning clustering classification and statistical measurement.
2009,1,16,Hierarchical forecasts for Australian domestic tourism,https://robjhyndman.com/publications/hierarchical-tourism/,In this paper we explore the hierarchical nature of tourism demand time series and produce short-term forecasts for Australian domestic tourism. The data and forecasts are organized in a hierarchy based on disaggregating the data for different geographical regions and for different purposes of travel. We consider five approaches to hierarchical forecasting: two variations of the top-down approach the bottom-up method a newly proposed top-down approach where top-level forecasts are disaggregated according to forecasted proportions of lower level series and a recently proposed optimal combination approach.
2009,1,1,A multivariate innovations state space Beveridge-Nelson decomposition,https://robjhyndman.com/publications/vists-beveridge-nelson/,The Beveridge-Nelson vector innovations structural time series framework is a new formulation that decomposes a set of variables into their permanent and transitory components. The proposed framework is flexible modelling inter-series relationships and common features in a simple manner. In particular it is shown that this new specification is simpler than conventional state space and cointegration approaches. The approach is illustrated using a trivariate data set comprising the GDP of Australia the USA and the UK.
2008,11,16,Forecasting time series with multiple seasonal patterns,https://robjhyndman.com/publications/multiple-seasonal-patterns/,A new approach is proposed for forecasting a time series with multiple seasonal patterns. A state space model is developed for the series using the innovation approach which enables us to develop explicit models for both additive and multiplicative seasonality. Parameter estimates may be obtained using methods from exponential smoothing. The proposed model is used to examine hourly and daily patterns in hourly data for both utility loads and traffic flows.
2008,11,5,Forecasting without significance tests?,https://robjhyndman.com/publications/forecasting-without-significance-tests/,Statistical significance testing has little useful purpose in business forecasting and other tools are to be preferred. For selecting or ranking forecasting methods (especially those based on models) there exist simple but powerful and practical alternative approaches that are not tests in any sense. It is suggested that forecasters place less emphasis on p-values and more emphasis on the predictive ability of models.
2008,10,16,Time series packages on R,https://robjhyndman.com/hyndsight/time-series-packages-on-r/,There is now an official CRAN Task View for Time Series. This will replace my earlier list of time series packages for R and provide a more visible and useful entry point for people wanting to use R for time series analysis. If I have missed anything on the list please let me know.
2008,9,3,LaTeX tips,https://robjhyndman.com/hyndsight/latex-tips/,"While reading students' theses and papers recently I came across various examples of poor latex-ing that I thought would be useful to catalogue.
  Don&rsquo;t set both width and height when using \includegraphics. It distorts the figure. Instead I suggest using \includegraphics[width=\textwidth]{..}
  Be consistent in capitalizing section and subsection headings and titles in the bibliography. My preferences is to use sentence case throughout.
  When using functions such as max min log exp sin cos etc."
2008,8,20,Tracking changes in LaTeX files,https://robjhyndman.com/hyndsight/tracking-changes-in-latex-files/,"When I write a paper it usually goes through many versions before being submitted to a journal. I keep track of the different versions by renaming the file when I&rsquo;m about to make major changes or when I receive a new version from a coauthor. The files are named file1.tex file2.tex etc. where &ldquo;file&rdquo; is replaced by something more meaningful.
It is often useful to be able to compare versions to see what changes have been made especially when working with coauthors."
2008,8,20,Tracking changes in text files,https://robjhyndman.com/hyndsight/tracking-changes-in-text-files/,"A common issue that arises with text files (e.g. R code) is to identify changes that have been made between versions. I usually number my R files as file1.R file2.R etc. (with &ldquo;file&rdquo; replaced by something more meaningful)with the number indicating the version of the file. Version numbers change whenever I send the file to someone else to modify or whenever I make major changes myself.
I often need to know what changes have been made between successive versions."
2008,8,19,Supervision award,https://robjhyndman.com/hyndsight/supervision-award/,Last night I received the Vice-Chancellor&rsquo;s postgraduate supervision award at a function at Government House. I am deeply honoured that my students thought to nominate me for the award. I think I was as surprised as anyone to win and some people have asked me what I did to deserve it. Actually I&rsquo;m not sure that I did deserve it but I can tell you what I told the award committee who chose me.
2008,7,30,LaTeX books,https://robjhyndman.com/hyndsight/latex-books/,"amzn_assoc_placement = ""adunit0""; amzn_assoc_tracking_id = ""otexts-20""; amzn_assoc_ad_mode = ""manual""; amzn_assoc_ad_type = ""smart""; amzn_assoc_marketplace = ""amazon""; amzn_assoc_region = ""US""; amzn_assoc_title = ""LaTeX books""; amzn_assoc_rows = ""6""; amzn_assoc_linkid = ""837c44f97120fac94a76c36f9781f970""; amzn_assoc_asins = ""18471998603642238157032117385602013629961784395145081764131933192379500201529831"";   If you cannot see any books above please turn off your ad-blocker."
2008,7,25,Time management,https://robjhyndman.com/hyndsight/time-management/,"I am frequently asked how I manage my time and how I manage to get so much done. I don&rsquo;t know that my approach is right for everyone but in case it helps here are some comments on how I work.
One of the main traps that people fall into is to do things that are urgent but not necessarily important. They react to the urgency of deadlines and demands of colleagues and end up spending a lot of time on things that don&rsquo;t really matter much."
2008,7,18,Forecasting and the importance of being uncertain,https://robjhyndman.com/seminars/forecasting-and-the-importance-of-being-uncertain/,Indian Institute of Management Calcutta. Melbourne 18 July 2008.
2008,7,16,"Stochastic population forecasts using functional data models for mortality, fertility and migration",https://robjhyndman.com/publications/stochastic-population-forecasts/,Age-sex-specific population forecasts are derived through stochastic population renewal using forecasts of mortality fertility and net migration. Functional data models with time series coefficients are used to model age-specific mortality and fertility rates. As detailed migration data are lacking net migration by age and sex is estimated as the difference between historic annual population data and successive populations one year ahead derived from a projection using fertility and mortality data. This estimate which includes error is also modeled using a functional data model.
2008,7,16,Automatic time series forecasting: the forecast package for R,https://robjhyndman.com/publications/automatic-forecasting/,Automatic forecasts of large numbers of univariate time series are often needed in business and other contexts. We describe two automatic forecasting algorithms that have been implemented in the forecast package for R. The first is based on innovation state space models that underly exponential smoothing methods. The second is based on ARIMA models. The algorithms are applicable to both seasonal and non-seasonal data and are compared and illustrated using four real time series.
2008,6,29,Building R packages for Windows,https://robjhyndman.com/seminars/building-r-packages-for-windows/,"R workshop. Melbourne 29 June 2008. There was an R workshop on 28-29 June just before the Australian Statistical Conference. I put in an appearance on the second day.
Building R packages for Windows
 handout slides"
2008,6,29,Time series and forecasting in R,https://robjhyndman.com/seminars/time-series-and-forecasting-in-r/,"R workshop. Melbourne 29 June 2008. There was an R workshop on 28-29 June just before the Australian Statistical Conference. I put in an appearance on the second day.
Time series and forecasting in R
 handout slides"
2008,6,19,"Bagplots, boxplots and outlier detection for functional data",https://robjhyndman.com/seminars/fboxplot-talk/,Australian Statistics Conference. Melbourne July 2008.  Where: First International Workshop on Functional and Operatorial Statistics Toulouse Where : Australian Statistical Conference Melbourne Australia Speakers: Professor Rob J Hyndman Monash University &amp; Han Lin Shang Monash University  Abstract: We propose some new tools for visualizing functional data and for identifying functional outliers. The proposed tools make use of robust principal component analysis data depth and highest density regions. We compare the proposed outlier detection methods with the existing “functional depth” method and show that our methods have better performance on identifying outliers in French male age-specific mortality data.
2008,6,16,The admissible parameter space for exponential smoothing models,https://robjhyndman.com/publications/the-admissible-parameter-space-for-exponential-smoothing-models/,We discuss the admissible parameter space for some state space models including the models that underly exponential smoothing methods. We find that the usual parameter restrictions (requiring all smoothing parameters to lie between 0 and 1) do not always lead to stable models. We also find that all seasonal exponential smoothing methods are unstable as the underlying state space models are neither reachable nor observable. This instability does not affect the forecasts but does corrupt the state estimates.
2008,6,15,Exponential smoothing and non-negative data,https://robjhyndman.com/seminars/exponential-smoothing-and-non-negative-data/,Where: International Symposium on Forecasting Nice France  Abstract: The most common forecasting methods in business are based on exponential smoothing and the most common time series in business are inherently non-negative. Therefore it is of interest to consider the properties of the potential stochastic models underlying exponential smoothing when applied to non-negative data. We explore nonlinear exponential smoothing state space models for non-negative data under various assumptions about the innovations or error process.
2008,6,6,LaTeX workshop,https://robjhyndman.com/hyndsight/latex-workshop/,"I gave a one-day LaTeX workshop today.
Here is the blurb:
 LaTeX is an extremely powerful markup language for creating structured documents. It is particularly well-suited for documents containing mathematics but can be used for any document. Those whose publications involve a large number of mathematical equations often use LaTeX rather than MS-Word or some other word processing package. It is the standard writing tool for most research in the mathematical sciences."
2008,5,28,Words to avoid,https://robjhyndman.com/hyndsight/words-to-avoid/,"According to Andrew Gelman we should avoid these words in research writing:
  Note that
  Interestingly
  Obviously
  It is clear that
  It is interesting to note that
  very
  quite
  of course
  Notice that
  I agree with him that all of these are overused but that doesn&rsquo;t mean they should be banned. The words &ldquo;very&rdquo; and &ldquo;quite&rdquo; are useful for conveying the strength of a statement."
2008,5,15,"Bagplots, boxplots and outlier detection for functional data",https://robjhyndman.com/publications/bagplots-boxplots-and-outlier-detection-for-functional-data/,We propose some new tools for visualizing functional data and for identifying functional outliers. The proposed tools make use of robust principal component analysis data depth and highest density regions. We compare the proposed outlier detection methods with the existing &ldquo;functional depth&rdquo; method and show that our methods have better performance on identifying outliers in French male age-specific mortality data.
2008,5,7,Giving a research seminar,https://robjhyndman.com/hyndsight/giving-a-research-seminar/,"An expanded version of this post is available in my article on “Giving an academic talk”.
With conference season almost upon us it is timely to discuss what makes a good conference presentation. Here is a suggested structure.
 A motivating example demonstrating the problem you are trying to solve. Explain existing approaches to the problem and their weaknesses. Describe your main contributions. Show how your ideas solve the problem/example you started with."
2008,4,28,Forecasting and time series books,https://robjhyndman.com/hyndsight/forecasting-and-time-series-books/,"People often ask me for recommendations on forecasting books and time series books. So here is list of eight good books to which I often refer.
(Updated 8 November 2017)
amzn_assoc_placement = ""adunit0""; amzn_assoc_tracking_id = ""otexts-20""; amzn_assoc_ad_mode = ""manual""; amzn_assoc_ad_type = ""smart""; amzn_assoc_marketplace = ""amazon""; amzn_assoc_region = ""US""; amzn_assoc_title = ""Forecasting and time series books""; amzn_assoc_rows = ""6""; amzn_assoc_linkid = ""837c44f97120fac94a76c36f9781f970""; amzn_assoc_asins = ""098750710935407191640999064908B004UW0PA4047136164X11186750290792374010144197864X"";   Two are my own books of course (after all I wrote them because I thought I had something to say)."
2008,3,28,R workshop,https://robjhyndman.com/hyndsight/r-workshop/,"There was an R workshop on 28-29 June just before the Australian Statistical Conference. I put in an appearance on the second day giving two talks.
Time series and forecasting in R
 handout slides  Building R packages for Windows
 handout slides"
2008,3,27,Creating a BibTeX file from a Google Library,https://robjhyndman.com/hyndsight/creating-a-bibtex-file-from-a-google-library/,"As you will have seen if you poke around these pages I have a Google Library of books in statistics and forecasting. This is intended to be a complete copy of what is on the shelves in my office (about 400 books) plus books that I would like on my shelves if I had more space.
I find the library useful for keeping track of books that my students and colleagues borrow (I just add a tag containing their name to the book)."
2008,3,26,The maths/stats crisis in Australian education,https://robjhyndman.com/hyndsight/the-mathsstats-crisis-in-australian-education/,"There is a great article in today&rsquo;s Australian by my co-author Peter Hall on the crisis in mathematics &amp; statistics education (with student numbers falling at the same time as the number of jobs is rising).
The most interesting comment is the last paragraph:
 For a nation in the grip of a serious skills shortage in mathematics and statistics another review is not needed. Action is needed. It should be possible to drive university behaviour in the national interest by foreshadowing that the Government&rsquo;s proposed compacts will include a requirement that mathematics and statistics curriculums be offered in science and technology education business and economics courses across the country taught by professionals in mathematics and statistics."
2008,3,13,Dodgy forecasting,https://robjhyndman.com/hyndsight/dodgy-forecasting/,"A few years ago I did some forecasting work for a commonwealth government department and found that they were forecasting a $5 billion budget using the FORECAST command in Excel. Worse they were fitting a regression through only three observations and they were not even the most recent observations.
It seems a similar thing has happened again. The Victorian government is projecting water consumption based on a regression through three observations."
2008,2,24,About Hyndsight,https://robjhyndman.com/hyndsight/about/,I was thinking of writing a book on doing research in statistics. Instead I decided to write a blog covering the same material plus other things that might be of interest to my research team. Topics covered include LaTeX R writing and preparing a thesis writing a journal article submitting an article to a refereed journal how to convince editors to publish your work and writing referee reports. Topics of more specific interest to my research team include forecasting data visualization and functional data and local events such as meetups or statistics conferences.
2008,2,22,Forecasting functional time series,https://robjhyndman.com/seminars/forecasting-functional-time-series/,Where: Australian Frontiers of Science  Abstract: Functional time series are curves that are observed sequentially in time. For example the curve of death rate as a function of age is observed annually. Yield curves in finance (essentially interest rates as a function of the term of investment) are observed each week or each day. Electricity consumption as a function of temperature is observed every month. These are all high dimensional functional data indexed by time.
2008,2,1,Modelling and forecasting Australian domestic tourism,https://robjhyndman.com/publications/aus-domestic-tourism/,In this paper we model and forecast Australian domestic tourism demand. We use a regression framework to estimate important economic relationships for domestic tourism demand. We also identify the impact of world events such as the 2000 Sydney Olympics and the 2002 Bali bombings on Australian domestic tourism. To explore the time series nature of the data we use innovation state space models to forecast the domestic tourism demand. Combining these two frameworks we build innovation state space models with exogenous variables.
2008,1,25,Generation of synthetic sequences of half-hourly temperatures,https://robjhyndman.com/publications/generation-of-synthetic-sequences-of-half-hourly-temperatures/,"We present tools to generate synthetic sequences of half-hourly temperatures with similar statistical characteristics to observed historical data. Temperatures are generated using a combination of daily and half-hourly temperature models which account for intra-day and intra-year seasonality as well as short- and long-term serial correlations. Details of the model estimation are given as well as a description of the synthetic generation.
Keywords: temperature data time series Fourier series ARMA models seasonal block-bootstrap synthetic generation."
2007,11,27,Population forecasting and the importance of being uncertain,https://robjhyndman.com/seminars/population-forecasting-and-the-importance-of-being-uncertain/,Where: Knibbs Lecture Statistical Society of Australia  Abstract: Forecasters had an inauspicious beginning dabbling with divination sheep&rsquo;s livers and vapour-ridden caves in the mountains of Greece. Then there was a time when forecasters could be charged with vagrancy! Their reputations are still tarnished but their tools are rather more effective. Professor Rob Hyndman will argue for the importance of statistical modelling in forecasting and demonstrate the dangers that occur when uncertainty is ignored.
2007,11,16,Indexing in LaTeX,https://robjhyndman.com/hyndsight/indexing-in-latex/,"I&rsquo;m in the final stages of preparing my new exponential smoothing book for publication and have been learning about some LaTeX indexing tools.
The standard subject index is created using the following procedure:
 Include \index{entry} commands wherever you want an index entry. Include \usepackage{makeidx} and \makeindex in the preamble. Put a \printindex command where the index is to appear normally before the \end{document} command.  The details are well-documented in this tutorial (starting on p9)."
2007,11,1,Tables in LaTeX,https://robjhyndman.com/hyndsight/tables-in-latex/,"(Updated May 2017)
Making tables in LaTeX is one of the few areas where LaTeX can be more difficult than a WYSIWYG editor.
Here are some pointers to tools and packages that I have found useful.
  tablesgenerator.com: a web-based tool for generating LaTeX tables.
  Excel2LaTeX: this excel add-in makes it easy to copy a rectangular array of cells in a spreadsheet into a LaTeX document.
  Calc2LaTeX: a similar extension for LibreOffice and OpenOffice."
2007,10,25,Graduation address,https://robjhyndman.com/seminars/graduation-address/,"Mr Chancellor Madam Deputy Vice-Chancellor colleagues guests and especially graduates.
I would like to congratulate all of you who are graduating tonight.
It is a great achievement to have completed a university degree and you should all feel very very proud of your accomplishment. This is one of the six great milestones in your life: the others being birth death marriage parenthood and the day you finally pay off your HECS debt."
2007,9,12,Searching the statistical literature,https://robjhyndman.com/hyndsight/searching-the-statistical-literature/,"Last week Google books introduced a facility whereby users can create their own &ldquo;library&rdquo; containing a subset of books to search. I have set up a library of statistical books to aid in searching the statistical literature.
My Google library is intended to include all the books on my office shelves plus a whole lot more that I would like to buy if I had more money (and more shelf space)."
2007,9,6,Organization and R,https://robjhyndman.com/hyndsight/organization-and-r/,"Many R users seem to get themselves in a bit of a mess with R files and workspaces scattered across different directories. The R files themselves also get messy and hard to follow. So here is some advice on keeping organized with R:
  Try to keep code strictly indented based on the code structure such as loops if statements etc. Every left brace { should be followed by an extra level of indentation which continues until the matching right brace }."
2007,8,31,Debugging in R,https://robjhyndman.com/hyndsight/debugging-in-r/,"Anyone who starts writing serious R code (i.e. code that involves user-written functions) soon finds the need to use debugging tools. There are a few basic tools in R that are worth knowing about.
The function debug() allows one to step through the execution of a function line by line. At any point you can print out values of variables or produce a graph of the results within the function. While debugging you can simply type &ldquo;c&rdquo; to continue to the end of the current section of code (e."
2007,7,16,Measurement of changes in antihypertensive drug utilization following primary care educational interventions,https://robjhyndman.com/publications/measurement-of-changes-in-antihypertensive-drug-utilization-following-primary-care-educational-inte/,"Abstract: Purpose To measure changes in drug utilization following a national general practice education program aimed at improving prescribing for hypertension.
Methods A series of nationally implemented multifaceted educational interventions using social marketing principles focusing on prescribing for hypertension was commenced in October 1999 and repeated in September 2001 and August 2003. The target group was all primary care prescribers in Australia and interventions were both active (voluntary) and passive. Newsletter and prescribing feedback was mailed in October 1999 September 2001 (newsletter only) and August 2003."
2007,7,16,Robust forecasting of mortality and fertility rates: a functional data approach,https://robjhyndman.com/publications/funcfor/,Abstract: A new method is proposed for forecasting age-specific mortality and fertility rates observed over time. This approach allows for smooth functions of age is robust for outlying years due to wars and epidemics and provides a modelling framework that is easily adapted to allow for constraints and other information. Ideas from functional data analysis nonparametric smoothing and robust statistics are combined to form a methodology that is widely applicable to any functional time series data observed discretely and possibly with error.
2007,6,29,Do levels of airborne grass pollen influence asthma hospital admissions?,https://robjhyndman.com/publications/do-levels-of-airborne-grass-pollen-influence-asthma-hospital-admissions/,"Background: The effects of environmental factors and ambient concentrations of grass pollen on allergic asthma are yet to be established.
Objective: We sought to estimate the independent effects of grass pollen concentrations in the air over Melbourne on asthma hospital admissions for the 1992-1993 pollen season.
Methods: Daily grass pollen concentrations were monitored over a 24 hr period at three stations in Melbourne. The outcome variable was defined as all-age asthma hospital admissions with ICD9-493 codes."
2007,6,25,Forecasting medium- and long-term peak electricity demand,https://robjhyndman.com/seminars/forecasting-medium-and-long-term-peak-electricity-demand/,"Where: International Symposium on Forecasting New York  Abstract: Peak electricity demand forecasting is important in medium and long-term planning of electricity supply. Extreme demand often leads to supply failure with consequential business and social disruption. Forecasting extreme demand events is therefore an important problem in energy management.
Electricity demand at a given time is subject to a range of influences including the ambient temperature recent past temperatures time of day day of week holidays economic conditions and so on."
2007,5,29,A state space model for exponential smoothing with group seasonality,https://robjhyndman.com/publications/a-state-space-model-for-exponential-smoothing-with-group-seasonality/,We present an approach to improve forecast accuracy by simultaneously forecasting a group of products that exhibit similar seasonal demand patterns. Better seasonality estimates can be made by using information on all products in a group and using these improved estimates when forecasting at the individual product level. This approach is called the group seasonal indices (GSI) approach and is a generalization of the classical Holt-Winters procedure. This article describes an underlying state space model for this method and presents simulation results that show when it yields more accurate forecasts than Holt-Winters.
2007,4,1,Half-life estimation based on the bias-corrected bootstrap: a highest density region approach,https://robjhyndman.com/publications/half-life-estimation-based-on-the-bias-corrected-bootstrap-a-highest-density-region-approach/,The half-life is defined as the number of periods required for the impulse response to a unit shock to a time series to dissipate by half. It is widely used as a measure of persistence especially in international economics to quantify the degree of mean-reversion of the deviation from an international parity condition. Several studies have proposed bias-corrected point and interval estimation methods. However they have found that the confidence intervals are rather uninformative with their upper bound being either extremely large or infinite.
2007,3,16,Minimum sample size requirements for seasonal forecasting models,https://robjhyndman.com/publications/minimum-sample-size-requirements-for-seasonal-forecasting-models/,How much data do you need to forecast using a seasonal model? The answer depends on the type of model being used and the amount of random variation in the data. We discuss the mathematical limits for estimating various common seasonal forecasting models from data. These limits apply when the amount of random variation is very small. Real data often contain a lot of random variation and then many more observations are required.
2007,2,22,Stochastic population forecasts using functional data models,https://robjhyndman.com/seminars/stochastic-population/,When: 12.00noon Thu 22nd February 2007 Where: Room 213 Richard Berry Building The University of Melbourne When: 2.30pm Fri 1 June 2007 Where: Room 457 Menzies Building Monash University (Clayton)  Abstract: I will present a new approach to age-specific forecasting of population based on separate forecasts of mortality fertility and net migration. Functional data models with time series coefficients will be used to model mortality and fertility rates and net migration.
2007,2,16,Forecasting age-specific breast cancer mortality using functional data models,https://robjhyndman.com/publications/forecasting-age-specific-breast-cancer-mortality-using-functional-data-models/,Accurate estimates of future age-specific incidence and mortality are critical for allocation of resources to breast cancer control programs and evaluation of screening programs. The purpose of this study is to apply functional data analysis techniques to model age-specific breast cancer mortality time trends and forecast entire age-specific mortality functions using a state-space approach. We use annual unadjusted breast cancer mortality rates in Australia from 1921 to 2001 in five year age groups (45 to 85+).
2006,11,16,Another look at measures of forecast accuracy,https://robjhyndman.com/publications/another-look-at-measures-of-forecast-accuracy/,"We discuss and compare measures of accuracy of univariate time series forecasts. The methods used in the M-competition and the M3-competition and many of the measures recommended by previous authors on this topic are found to be degenerate in commonly occurring situations. Instead we propose that the mean absolute scaled error become the standard measure for comparing forecast accuracy across multiple time series.
Keywords: forecast accuracy forecast evaluation forecast error measures M-competition mean absolute scaled error."
2006,10,26,Forecasting and the importance of being uncertain,https://robjhyndman.com/seminars/forecasting-and-the-importance-of-being-uncertain-2/,What: 2006 Belz lecture Statistical Society of Australia (Victorian branch) When: 6.15pm 24 October 2006 Where: Old Geology Theatre 1 University of Melbourne  Forecasters had an inauspicious beginning dabbling with divination sheep&rsquo;s livers and vapour-ridden caves in the mountains of Greece. Then there was a time when forecasters could be charged with vagrancy! Their reputations are still tarnished but their tools are rather more effective. Professor Rob Hyndman will argue for the importance of statistical modelling in forecasting and demonstrate the dangers that occur when uncertainty is ignored.
2006,10,20,Lee-Carter mortality forecasting: a multi-country comparison of variants and extensions,https://robjhyndman.com/publications/lee-carter-mortality-forecasting-a-multi-country-comparison-of-variants-and-extensions/,We compare the short- to medium- term accuracy of five variants or extensions of the Lee-Carter method for mortality forecasting. These include the original Lee-Carter the Lee-Miller and Booth-Maindonald-Smith variants and the more flexible Hyndman-Ullah and De Jong-Tickle extensions. These methods are compared by applying them to sex-specific populations of 10 developed countries using data for 1986-2000 for evaluation. All variants and extensions are more accurate than the original Lee-Carter method for forecasting log death rates by up to 61%.
2006,9,17,Projection pursuit estimator for multivariate conditional densities,https://robjhyndman.com/publications/projection-pursuit-estimator-for-multivariate-conditional-densities/,
2006,9,16,Another look at measures of forecast accuracy for intermittent demand,https://robjhyndman.com/publications/another-look-at-measures-of-forecast-accuracy-for-intermittent-demand/,"Some of the proposed measures of forecast accuracy for intermittent demand can give infinite or undefined values. This makes them unsuitable for general use. I summarize the various measures and demonstrate what can go wrong. Then I describe a new measure (the mean absolute scaled error) which does not have these flaws. I believe it should become the standard measure for comparing forecast accuracy for multiple intermittent-demand series.
Errata: In the series shown in Table 2 the sixth value should be 11 (not 1)."
2006,8,16,A note on the categorization of demand patterns,https://robjhyndman.com/publications/a-note-on-the-categorization-of-demand-patterns/,"We revisit the problem of categorizing demand patterns in order to select the best forecasting method. We improve the categorization scheme of Syntetos Boylan and Croston (2004) by deriving an exact result for the boundary between type and giving a simple approximation to the boundary that is better than that previously published.
Keywords: categorization; forecasting; inventory control; intermittent demand."
2006,7,30,Useful LaTeX links,https://robjhyndman.com/hyndsight/useful-latex-links/,"Learning LaTeX   Introduction to LaTeX: notes and other materials from a one-day workshop.
  Getting started with LaTeX: an excellent online introduction from David Wilkins.
  Great reference book from the Indian TeX Users Group.
  Excellent on-line tutorials from Andy Roberts.
  More tutorials from the Indian TeX Users Group.
  CTAN for finding packages.
  Getting help The first and best place to go is TeX."
2006,7,20,A Bayesian approach to bandwidth selection for multivariate kernel density estimation,https://robjhyndman.com/publications/bandwidth-selection-for-multivariate-kernel-density-estimation-using-mcmc/,Kernel density estimation for multivariate data is an important technique that has a wide range of applications. However it has received significantly less attention than its univariate counterpart. The lower level of interest in multivariate kernel density estimation is mainly due to the increased difficulty in deriving an optimal data-driven bandwidth as the dimension of the data increases. We provide Markov chain Monte Carlo (MCMC) algorithms for estimating optimal bandwidth matrices for multivariate kernel density estimation.
2006,7,16,25 years of time series forecasting,https://robjhyndman.com/publications/25-years-of-time-series-forecasting/,We review the past 25 years of research into time series forecasting. In this silver jubilee issue we naturally highlight results published in journals managed by the International Institute of Forecasters (Journal of Forecasting 1982-1985; International Journal of Forecasting 1985-2005). During this period over one third of all papers published in these journals concerned time series forecasting. We also review highly influential works on time series forecasting that have been published elsewhere during this period.
2006,7,15,Twenty-five years of forecasting,https://robjhyndman.com/publications/ijf-editorial-twenty-five-years-of-forecasting/,
2006,6,26,Automatic time series forecasting,https://robjhyndman.com/seminars/automatic-time-series-forecasting/,UseR! conference Vienna Austria
2006,6,20,Optimal combination forecasts for hierarchical time series,https://robjhyndman.com/seminars/hierarchical2/,"International Symposium on Forecasting Santander Spain
Speakers: Rob J Hyndman and Roman A. Ahmed"
2006,5,16,Characteristic-based clustering for time series data,https://robjhyndman.com/publications/ts-clustering/,With the growing importance of time series clustering research particularly for similarity searches amongst long time series such as those arising in medicine or finance it is critical for us to find a way to resolve the outstanding problems that make most clustering methods impractical under certain circumstances. When the time series is very long some clustering algorithms may fail because the very notation of similarity is dubious in high dimension space; many methods cannot handle missing data when the clustering is based on a distance metric.
2006,5,16,Measuring change in prescription drug utilization in Australia,https://robjhyndman.com/publications/measuring-change-in-prescription-drug-utilization-in-australia/,"Purpose: The National Prescribing Service Ltd (NPS) aims to improve prescribing and use of medicines consistent with evidence-based best practice. This study compares two statistical methods used to determine whether multiple educational interventions influenced antibiotic prescribing in Australia.
Methods Monthly data (July 1996 to June 2003) were obtained from a national administrative claims database. The outcome measures were the median number of antibiotic prescriptions per 1000 consultations for each general practitioner (GP) each month and the mean proportion (across GPs) of each subgroup of antibiotics (e."
2006,5,1,Local linear multivariate regression with variable bandwidth in the presence of heteroscedasticity,https://robjhyndman.com/publications/local-linear-multivariate-regression-with-variable-bandwidth-in-the-presence-of-heteroscedasticity/,We present a local linear estimator with variable bandwidth for multivariate nonparametric regression. We prove its consistency and asymptotic normality in the interior of the observed data and obtain its rates of convergence. This result is used to obtain practical direct plug-in bandwidth selectors for heteroscedastic regression in one and two dimensions. We show that the local linear estimator with variable bandwidth has better goodness-of-fit properties than the local linear estimator with constant bandwidth in the presence of heteroscedasticity.
2006,1,16,The accuracy of television network rating forecasts: the effects of data aggregation and alternative models,https://robjhyndman.com/publications/the-accuracy-of-television-network-rating-forecasts-the-effects-of-data-aggregation-and-alternative-models/,This paper investigates the effect of aggregation in relation to the accuracy of television network rating forecasts. We compare the forecast accuracy of network ratings using population rating models rating models for demographic/behavioural segments and individual viewing behaviour models. Models are fitted using neural networks decision trees and regression. The most accurate forecasts are obtained by aggregating forecasts from segment rating models with neural networks being used to fit these models.
2005,12,16,Sensitivity of the estimated air pollution-respiratory admissions relationship to statistical model,https://robjhyndman.com/publications/sensitivity-of-the-estimated-air-pollution-respiratory-admissions-relationship-to-statistical-model/,"Abstract: Study objective: The objective of this study is to demonstrate the methodological shortcomings of currently available analytical methods for single-city time series data one of the most commonly used ecological study designs in air pollution and respiratory disease research.
Design and Methods: We analyse single city epidemiological time series of daily Chronic Obstructive Pulmonary Disease (COPD) (ICD codes 490-492 494 496) and daily asthma (ICD codes 493) hospital admissions in Melbourne Australia from July 1989 to December 1992."
2005,10,16,Empirical information criteria for time series forecasting model selection,https://robjhyndman.com/publications/empirical-information-criteria-for-time-series-forecasting-model-selection/,In this paper we propose a new Empirical Information Criterion (EIC) for model selection which penalizes the likelihood of the data by a function of the number of parameters in the model. It is designed to be used where there are a large number of time series to be forecast. However a bootstrap version of the EIC can be used where there is a single time series to be forecast. The EIC provides a data-driven model selection tool that can be tuned to the particular forecasting task.
2005,7,16,Stochastic models underlying Croston's method for intermittent demand forecasting,https://robjhyndman.com/publications/croston/,Intermittent demand commonly occurs with inventory data with many time periods having no demand and small demand in the other periods. Croston&rsquo;s method is a widely used procedure for intermittent demand forecasting. However it is an ad~hoc method with no properly formulated underlying stochastic model. In this paper we explore possible models underlying Croston&rsquo;s method and three related methods and we show that any underlying model will be inconsistent with the properties of intermittent demand data.
2005,7,16,"Book Review of ""Data Analysis and Graphics Using R: An Example-based Approach"" (Maindonald and Braun, 2003)",https://robjhyndman.com/publications/maindonald-and-braun-data-analysis-and-graphics-using-r-an-example-based-approach/,
2005,5,22,Dimension reduction for clustering time series using global characteristics,https://robjhyndman.com/publications/dimension-reduction-for-clustering-time-series-using-global-characteristics/,Existing methods for time series clustering rely on the actual data values can become impractical since the methods do not easily handle dataset with high dimensionality missing value or different lengths. In this paper a dimension reduction method is proposed that replaces the raw data with some global measures of time series characteristics. These measures are then clustered using a self-organizing map. The proposed approach has been tested using benchmark time series previously reported for time series clustering and is shown to yield useful and robust clustering.
2005,4,16,Robust forecasting of mortality and fertility rates: a functional data approach,https://robjhyndman.com/publications/robust-forecasting-of-mortality-and-fertility-rates-a-functional-data-approach/,We propose a new method for forecasting age-specific mortality and fertility rates observed over time. We combine ideas from functional data analysis nonparametric smoothing and robust statistics to form a methodology that is widely applicable to any functional time series data and age-specific mortality and fertility in particular. Our approach provides a modelling framework that is easily adapted to allow for constraints and other information. The model used can be considered a generalization of the Lee-Carter model commonly used in mortality and fertility forecasting.
2005,4,2,Time series forecasting: the case for the single source of error state space approach,https://robjhyndman.com/publications/322/,The state space approach to modelling univariate time series is now widely used both in theory and in applications. However the very richness of the framework means that quite different model formulations are possible even when they purport to describe the same phenomena. In this paper we examine the single source of error [SSOE] scheme which has perfectly correlated error components. We then proceed to compare SSOE to the more common version of the state space models for which all the error terms are independent; we refer to this as the multiple source of error [MSOE] scheme.
2005,1,16,Prediction intervals for exponential smoothing using two new classes of state space models,https://robjhyndman.com/publications/prediction-intervals-for-exponential-smoothing-using-two-new-classes-of-state-space-models/,Three general classes of state space models are presented based upon the single source of error formulation. The first class is the standard linear state space model with homoscedastic errors the second retains the linear structure but incorporates a dynamic form of heteroscedasticity and the third allows for non-linear structure in the observation equation as well as heteroscedasticity. These three classes provide stochastic models for a wide variety of exponential smoothing methods.
2005,1,16,Local linear forecasts using cubic smoothing splines,https://robjhyndman.com/publications/splinefcast/,We show how cubic smoothing splines fitted to univariate time series data can be used to obtain local linear forecasts. Our approach is based on a stochastic state space model which allows the use of a likelihood approach for estimating the smoothing parameter and which enables easy construction of prediction intervals. We show that our model is a special case of an ARIMA(022) model and we provide a simple upper bound for the smoothing parameter to ensure an invertible model.
2005,1,15,Editorial,https://robjhyndman.com/publications/ijf-editorial/,
2004,10,16,The interaction between trend and seasonality,https://robjhyndman.com/publications/the-interaction-between-trend-and-seasonality/,A contribution to the discussion of Miller and Williams (2004).
2004,7,16,Nonparametric confidence intervals for receiver operating characteristic curves,https://robjhyndman.com/publications/nonparametric-confidence-intervals-for-receiver-operating-characteristic-curves/,We study methods for constructing confidence intervals and confidence bands for estimators of receiver operating characteristics. Particular emphasis is placed on the way in which smoothing should be implemented when estimating either the characteristic itself or its variance. We show that substantial undersmoothing is necessary if coverage properties are not to be impaired. A theoretical analysis of the problem suggests an empirical plug-in rule for bandwidth choice optimising the coverage accuracy of interval estimators.
2004,5,16,Exponential smoothing models: Means and variances for lead-time demand,https://robjhyndman.com/publications/exponential-smoothing-models-means-and-variances-for-lead-time-demand/,Exponential smoothing is often used to forecast lead-time demand for inventory control. In this paper formulae are provided for calculating means and variances of lead-time demand for a wide variety of exponential smoothing methods. A feature of many of the formulae is that variances as well as the means depend on trends and seasonal effects. Thus these formulae provide the opportunity to implement methods that ensure that safety stocks adjust to changes in trend or changes in season.
2004,1,16,Spline interpolation for demographic variables: the monotonicity problem,https://robjhyndman.com/publications/monotonic-splines-2/,"In demography it is often necessary to obtain a monotonic interpolation of data. A solution to this problem is available using the Hyman filter for cubic splines. However this does not seem to be well-known amongst demographers and no implementation of the procedure is readily available. We remedy these problems by outlining the relevant ideas here and providing a function for the R package.
R code"
2003,7,16,Normative data for the Test of Visual Analysis Skills on an Australian population,https://robjhyndman.com/publications/normative-data-for-the-test-of-visual-analysis-skills-on-an-australian-population/,Purpose: The purpose of this study was to produce normative data for Rosner&rsquo;s Test of Visual Analysis Skills (TVAS). Methods: 886 unselected children aged 5 to 10 years and in the first 4 years of school in Australia were tested to threshold on the TVAS. Percentiles means and standard deviations for each age group were calculated. Results: We found a steady increase in scores with grade and age and a significant difference in scores between each of the ages and grades.
2003,7,15,Statistical jokes,https://robjhyndman.com/hyndsight/statistical-jokes/,"Most of these jokes were posted to Usenet news groups. People who read such things collected them and put them on their web sites. I have shamelessly borrowed them edited them and posted them here for the light relief of other statisticians.
The Biologist the Statistician the Mathematician and the Computer Scientist A biologist a statistician a mathematician and a computer scientist are on a photo-safari in Africa. They drive out into the savannah in their jeep stop and scour the horizon with their binoculars."
2003,4,16,Unmasking the Theta method,https://robjhyndman.com/publications/unmasking-the-theta-method/,"The &ldquo;Theta method&rdquo; of forecasting performed particularly well in the M3-competition and is therefore of interest to forecast practitioners. The description of the method given by Assimakopoulos and Nikolopoulos (2000) involves several pages of algebraic manipulation and is difficult to comprehend. We show that the method can be expressed much more simply; furthermore we show that the forecasts obtained are equivalent to simple exponential smoothing with drift.
Keywords: exponential smoothing forecasting competitions state space models."
2003,2,16,Improved methods for bandwidth selection when estimating ROC curves,https://robjhyndman.com/publications/improved-methods-for-bandwidth-selection-when-estimating-roc-curves/,"The receiver operating characteristic (ROC) curve is used to describe the performance of a diagnostic test which classifies observations into two groups. We introduce new methods for selecting bandwidths when computing kernel estimates of ROC curves. Our techniques allow for interaction between the distributions of each group of observations and gives substantial improvement in MISE over other proposed methods especially when the two distributions are very different.
Keywords: bandwidth selection; binary classification; kernel estimator; ROC curve."
2002,11,16,Mixed model-based hazard estimation,https://robjhyndman.com/publications/mixed-model-based-hazard-estimation/,We propose a new method for estimation of the hazard function from a set of censored failure time data with a view to extending the general approach to more complicated models. The approach is based on a mixed model representation of penalized spline hazard estimators. One payoff is the automation of the smoothing parameter choice through restricted maximum likelihood. Another is the option to use standard mixed model software for automatic hazard estimation.
2002,7,16,Nonparametric estimation and symmetry tests for conditional density functions,https://robjhyndman.com/publications/nonparametric-estimation-and-symmetry-tests-for-conditional-density-functions/,We suggest two new methods for conditional density estimation. The first is based on locally fitting a log-linear model and is in the spirit of recent work on locally parametric techniques in density estimation. The second method is a constrained local polynomial estimator. Both methods always produce non-negative estimators. We propose an algorithm suitable for selecting the two bandwidths for either estimator. We also develop a new bootstrap test for the symmetry of conditional density functions.
2002,7,16,A state space framework for automatic forecasting using exponential smoothing methods,https://robjhyndman.com/publications/hksg/,We provide a new approach to automatic busineswwwwws forecasting based on an extended range of exponential smoothing methods. Each method in our taxonomy of exponential smoothing methods can be shown to be equivalent to the forecasts obtained from a state space model. This allows (1) the easy calculation of the likelihood the AIC and other model selection criteria; (2) the computation of prediction intervals for each method; and (3) random simulation from the underlying state space model.
2002,7,15,Kalman filter,https://robjhyndman.com/publications/kalman-filter/,
2002,7,15,Box-Jenkins modelling,https://robjhyndman.com/publications/box-jenkins-modelling/,
2002,7,15,ARIMA processes,https://robjhyndman.com/publications/arima-processes/,
2002,3,16,Using R to Teach Econometrics,https://robjhyndman.com/publications/using-r-to-teach-econometrics/,R an open-source programming environment for data analysis and graphics has in only a decade grown to become a de-facto standard for statistical analysis against which many popular commercial programs may be measured. The use of R for the teaching of econometric methods is appealing. It provides cutting-edge statistical methods which are by R&rsquo;s open-source nature available immediately. The software is stable available at no cost and exists for a number of platforms including various flavors of Unix and Linux Windows (9x/NT/2000) and the MacOS.
2001,11,16,Cycles and synchrony in the Collared Lemming (Dicrostonyx groenlandicus) in Arctic North America,https://robjhyndman.com/publications/cycles-and-synchrony-in-the-collared-lemming-dicrostonyx-groenlandicus-in-arctic-north-america/,Lemming populations are generally characterised by their cyclic nature yet empirical data to support this are lacking for most species largely because of the time and expense necessary to collect long-term population data. In this study we use the relative frequency of yearly willow scarring by lemmings as an index of lemming abundance allowing us to plot population changes over a 34-year period. Scars were collected from 18 sites in Arctic North America separated by 2-1647 km to investigate local synchrony among separate populations.
2001,10,16,It's time to move from 'what' to 'why',https://robjhyndman.com/publications/its-time-to-move-from-what-to-why/,"(Invited commentary on M3 competition.)
We provide a new approach to automatic business forecasting based on an extended range of exponential smoothing methods. Each method in our taxonomy of exponential smoothing methods can be shown to be equivalent to the forecasts obtained from a state space model. This allows (1) the easy calculation of the likelihood the AIC and other model selection criteria; (2) the computation of prediction intervals for each method; and (3) random simulation from the underlying state space model."
2001,8,16,Data visualization for time series in environmental epidemiology,https://robjhyndman.com/publications/data-visualization-for-time-series-in-environmental-epidemiology/,Data visualization has become an integral part of statistical modelling. Exploratory graphical analysis allows insight into the underlying structure of observations in a data set and graphical methods for diagnostic purposes after model fitting provide insight into the fitted model and its inadequacies. In this paper we present visualization methods for preliminary exploration of time series data and graphical diagnostic methods for modelling relationships between time series data in medicine. We will use exploratory graphical methods to better understand the relationship between a time series response and a number of potential covariates.
2001,7,2,Statistical methodological issues in studies of air pollution and respiratory disease,https://robjhyndman.com/publications/statistical-methodological-issues-in-studies-of-air-pollution-and-respiratory-disease/,Epidemiological studies have consistently shown short term associations between levels of air pollution and respiratory disease in countries of diverse populations geographical locations and varying levels of air pollution and climate. The aims of this paper are: (1) to assess the sensitivity of the observed pollution effects to model specification with particular emphasis on the inclusion of seasonally adjusted covariates; and (2) to study the effect of air pollution on respiratory disease in Melbourne Australia.
2001,6,16,Bandwidth selection for kernel conditional density estimation,https://robjhyndman.com/publications/bandwidth-selection-for-kernel-conditional-density-estimation/,"We consider bandwidth selection for the kernel estimator of conditional density with one explanatory variable. Several bandwidth selection methods are derived ranging from fast rules-of-thumb which assume the underlying densities are known to relatively slow procedures which use the bootstrap. The methods are compared and a practical bandwidth selection strategy which combines the methods is proposed. The methods are compared using two simulation studies and a real data set.
Keywords: density estimation; kernel smoothing; conditioning; bandwidth selection."
2000,11,16,Non-Gaussian conditional linear AR(1) models,https://robjhyndman.com/publications/non-gaussian-conditional-linear-ar1-models/,We give a general formulation of a non-Gaussian conditional linear AR(1) model subsuming most of the non-Gaussian AR(1) models that have appeared in the literature. We derive some general results giving properties for the stationary process mean variance and correlation structure and conditions for stationarity. These results highlight similarities and differences with the Gaussian AR(1) model and unify many separate results appearing in the literature. Examples illustrate the wide range of properties that can appear under the conditional linear autoregressive assumption.
2000,11,16,Residual diagnostic plots for model mis-specification in time series regression,https://robjhyndman.com/publications/residual-diagnostic-plots-for-model-mis-specification-in-time-series-regression/,This paper considers residuals for time series regression. Despite much literature on visual diagnostics for uncorrelated data there is little on the autocorrelated case. In order to examine various aspects of the fitted time series regression model three residuals are considered. The fitted regression model can be checked using orthogonal residuals; the time series error model can be analysed using marginal residuals; and the white noise error component can be tested using conditional residuals.
2000,8,9,Seasonal adjustment methods for the analysis of respiratory disease in environmental epidemiology,https://robjhyndman.com/publications/seasonal-adjustment-methods-for-the-analysis-of-respiratory-disease-in-environmental-epidemiology/,We study the relationship between daily hospital admissions for respiratory disease and various pollutant and climatic variables looking particularly at the effect of seasonal adjustment on the estimated models. Often time series exhibit seasonal behaviour and adequate control for the presence of a seasonal component is essential before one attempts to model the complex pollution-health association. We show that if these factors are not adequately controlled for spurious effects of pollutants and climate on morbidity/mortality can be induced.
2000,7,16,"Book review of ""Nonparametric econometrics"" (Pagan and Ullah, 1999)",https://robjhyndman.com/publications/pagan-and-ullah-nonparametric-econometrics/,
2000,5,16,Generalized additive modelling of mixed distribution Markov models with application to Melbourne's rainfall,https://robjhyndman.com/publications/gam-rainfall/,We consider modelling time series using a generalized additive model with first-order Markov structure and mixed transition density having a discrete component at zero and a continuous component with positive sample space. Such models have application for example in modelling daily occurrence and intensity of rainfall and in modelling the number and size of insurance claims. We show how these methods extend the usual sinusoidal seasonal assumption in standard chain-dependent models by assuming a general smooth pattern of occurrence and intensity over time.
1999,7,16,"Book Review of ""A primer of mathematical writing"" (Krantz, 1997) and ""Handbook of writing for the mathematical sciences"" (Higham, 1998)",https://robjhyndman.com/publications/krantz-a-primer-of-mathematical-writing-higham-handbook-of-writing-for-the-mathematical-sciences/,
1999,7,16,"Book review of ""Statistically speaking: a dictionary of quotations"" (Gaither and Cavazos-Gaither, 1996)",https://robjhyndman.com/publications/gaither-and-cavazos-gaither-statistically-speaking-a-dictionary-of-quotations/,
1999,7,15,"Book review of ""Chance encounters: a first course in data analysis and inference"" (Wild & Seber, 2000)",https://robjhyndman.com/publications/wild-c-j-and-seber-g-a-f-chance-encounters-a-first-course-in-data-analysis-and-inference/,
1999,7,7,Nonparametric additive regression models for binary time series,https://robjhyndman.com/publications/logitar/,I consider models for binary time series starting with autoregression models and then developing generalizations of them which allow nonparametric additive covariates. I show that several apparently different binary AR(1) models are equivalent. Three possible nonparametric additive regression models which allow for autocorrelation are considered; one is a generalization of an ARX model the other two are generalizations of a regression model with AR errors. One of the models is applied to two data sets: IBM stock transactions and Melbourne&rsquo;s rainfall.
1998,7,16,Smoothing non-Gaussian time series with autoregressive structure,https://robjhyndman.com/publications/smoothing-non-gaussian-time-series-with-autoregressive-structure/,We consider nonparametric smoothing for time series which are clearly non-Gaussian and which are subject to an autoregressive random component. This generalizes methods for smoothing Gaussian series with autoregressive errors but in the non-Gaussian case the autoregressive structure is not always additive. The problem can be formulated in a general way to include many common non-Gaussian autoregressive models. The amount of smoothing can be chosen by penalized likelihood methods and we give simulations and parametric bootstrap methods for studying and empirically estimating the penalty function.
1998,7,15,"Book Review of ""Smoothing methods in Statistics"" (Simonoff, 1996)",https://robjhyndman.com/publications/simonoff-smoothing-methods-in-statistics/,
1998,7,15,"Book review of ""Leading personalities in the Statistical Sciences: from the seventeenth century to the present"" (Johnson and Kotz, 1998)",https://robjhyndman.com/publications/johnson-and-kotz-eds-leading-personalities-in-the-statistical-sciences-from-the-seventeenth-century-to-the-present/,
1997,12,16,Nonparametric autocovariance function estimation,https://robjhyndman.com/publications/nonparametric-autocovariance-function-estimation/,"Nonparametric estimators of autocovariance functions for non-stationary time series are developed. The estimators are based on straightforward nonparametric mean function estimation ideas and allow use of any linear smoother (e.g. smoothing spline local polynomial). We study the properties of the estimators and illustrate their usefulness through application to some meteorological and seismic time series.
Keywords: bandwidth; correlated errors; kernel smoothing; local polynomial; nonparametric regression; non-stationary model; time series.
Data: Melbourne maximum temperatures Kobe earthquake seismograph"
1997,7,16,Some properties and generalizations of non-negative Bayesian time series models,https://robjhyndman.com/publications/some-properties-and-generalizations-of-non-negative-bayesian-time-series-models/,We study the most basic Bayesian forecasting model for exponential family time series the power steady model (PSM) of Smith in terms of observable properties of one-step forecast distributions and sample paths. The PSM implies a constraint between location and spread of the forecast distribution. Including a scale parameter in the models does not always give an exact solution free of this problem but it does suggest how to define related models free of the constraint.
1997,1,16,The pricing and trading of options using a hybrid neural network model with historical volatility,https://robjhyndman.com/publications/the-pricing-and-trading-of-options-using-a-hybrid-neural-network-model-with-historical-volatility/,"(Later known as Journal of Computational Intelligence in Finance)
The residuals between conventional option pricing models and market prices have persistent patterns or biases. The &ldquo;hybrid&rdquo; method models the residuals using an artificial neural network. The pricing accuracy of the hybrid method is demonstrated on real data using the Australian All Ordinaries Share Price Index options on futures and is compared with all major competing conventional models. The hybrid method is found to be both statistically and economically superior to the conventional models alone."
1996,11,16,Sample quantiles in statistical packages,https://robjhyndman.com/publications/quantiles/,There are a large number of different definitions used for sample quantiles in statistical computer packages. Often within the same package one definition will be used to compute a quantile explicitly while other definitions may be used when producing a boxplot a probability plot or a QQ-plot. We compare the most commonly implemented sample quantile definitions by writing them in a common notation and investigating their motivation and some of their properties.
1996,7,16,Estimating and visualizing conditional densities,https://robjhyndman.com/publications/estimating-and-visualizing-conditional-densities/,We consider the kernel estimator of conditional density and derive its asymptotic bias variance and mean-square error. Optimal bandwidths (with respect to integrated mean-square error) are found and it is shown that the convergence rate of the density estimator is order n-2/3. We also note that the conditional mean function obtained from the estimator is equivalent to a kernel smoother. Given the undesirable bias properties of kernel smoothers we seek a modified conditional density estimator which has mean equivalent to some other nonparametric regression smoother with better bias properties.
1996,7,16,Computing and graphing highest density regions,https://robjhyndman.com/publications/computing-and-graphing-highest-density-regions/,Many statistical methods involve summarizing a probability distribution by a region of the sample space covering a specified probability. One method of selecting such a region is to require it to contain points of relatively high density. Highest density regions are particularly useful for displaying multimodal distributions and in such cases may consist of several disjoint subsets &mdash; one for each local mode. In this paper I propose a simple method for computing a highest density region from any given (possibly multivariate) density f(x) which is bounded and continuous in x.
1996,7,15,"Book review of ""Kernel smoothing"" (Wand and Jones, 1995)",https://robjhyndman.com/publications/wand-and-jones-kernel-smoothing/,
1996,6,16,A unified view of linear AR(1) models,https://robjhyndman.com/publications/a-unified-view-of-linear-ar1-models/,We review and synthesize the wide range of non-Gaussian first order linear autoregressive models that have appeared in the literature. Models are organized into broad classes to clarify similarities and differences and facilitate application in particular situations. General properties for process mean variance and correlation are derived unifying many separate results appearing in the literature. Examples illustrate the wide range of properties that can appear even under the autoregressive assumption. These results are used in analysing a variety of real data sets illustrating general methods of estimation model diagnostics and model selection.
1995,7,16,Highest density forecast regions for non-linear and non-normal time series models,https://robjhyndman.com/publications/highest-density-forecast-regions-for-non-linear-and-non-normal-time-series-models/,"Many modern time series methods such as those involving non-linear models or non-normal data frequently lead to forecast densities which are asymmetric or multi-modal. The problem of obtaining forecast regions in such cases is discussed and it is proposed that highest density forecast regions be used. A graphical method for presenting the results is discussed.
Keywords: non-linear time series non-normal time series highest density regions forecast intervals threshold models.
R code"
1995,7,15,The use of information technology in the research process,https://robjhyndman.com/publications/the-use-of-information-technology-in-the-research-process/,
1995,7,5,The problem with Sturges' rule for constructing histograms,https://robjhyndman.com/publications/sturges/,Most statistical packages use Sturges' rule (or an extension of it) for selecting the number of classes when constructing a histogram. Sturges' rule is also widely recommended in introductory statistics textbooks. It is known that Sturges' rule leads to oversmoothed histograms but Sturges' derivation of his rule has never been questioned. In this note I point out that the argument leading to Sturges' rule is wrong.
1994,7,16,Approximations and boundary conditions for continuous time threshold autoregressive processes,https://robjhyndman.com/publications/approximations-and-boundary-conditions-for-continuous-time-threshold-autoregressive-processes/,"Continuous time threshold autoregressive (CTAR) processes have been developed in the past few years for modelling non-linear time series observed at irregular intervals. Several approximating processes are given here which are useful for simulation and inference. Each of the approximating processes implicitly defines conditions on the thresholds thus providing greater understanding of the way in which boundary conditions arise.
Keywords: continuous time autoregression threshold autoregression non-linear stochastic differential equations unequally spaced time series."
1993,7,16,Yule-Walker estimates for continuous-time autoregressive models,https://robjhyndman.com/publications/yule-walker-estimates-for-continuous-time-autoregressive-models/,I consider continuous time autoregressive (CAR) processes of order p and develop estimators of the model parameters based on Yule&ndash;Walker type equations. For continuously recorded data it is shown that these estimators are least squares estimators and have the same asymptotic distribution as maximum likelihood estimators. In practice though data can only be observed discretely. For discrete data I consider approximations to the continuous time estimators. It is shown that some of these discrete time estimators are asymptotically biased.
1992,12,17,Continuous-time threshold autoregressive modelling,https://robjhyndman.com/publications/phd/,"This thesis considers continuous time autoregressive processes defined by stochastic differential equations and develops some methods for modelling time series data by such processes.
The first part of the thesis looks at continuous time linear autoregressive (CAR) processes defined by linear stochastic differential equations. These processes are well-understood and there is a large body of literature devoted to their study. I summarise some of the relevant material and develop some further results."
1992,7,16,On continuous-time threshold autoregression,https://robjhyndman.com/publications/on-continuous-time-threshold-autoregression/,The use of non-linear models in time series analysis has expanded rapidly in the last ten years with the development of several useful classes of discrete-time non-linear models. One family of processes which has been found valuable is the class of self-exciting threshold autoregressive (SETAR) models discussed extensively in the books of Tong (1983 1990). In this paper we consider problems of modelling and forecasting with continuous-time threshold autoregressive (CTAR) processes.
1991,2,16,Continuous time threshold autoregressive models,https://robjhyndman.com/publications/continuous-time-threshold-autoregressive-models/,The importance of non-linear models in time series analysis has been recognized increasingly over the past ten years. A number of discrete time non-linear processes have been introduced and found valuable for the modelling of observed series. Among these processes are the discrete time threshold models discussed extensively in the book of Tong (1983). The purpose of this paper is to define a continuous time analogue of the threshold AR(p) process and to discuss some of its properties.
1987,9,15,Calculating the odds,https://robjhyndman.com/publications/calculating-the-odds/,
1,1,1,,https://robjhyndman.com/software/,"CRAN task views  Time Series Anomaly Detection   Selected R packages I’ve coauthored Tidy time series analysis and forecasting      Tidy Temporal Data Frames and Tools: as described in Wang Cook & Hyndman (2020).   Github   CRAN   Monthly downloads:
19184       Diverse Datasets for ‘tsibble’.   Github   CRAN   Monthly downloads:"
1,1,1,,https://robjhyndman.com/teaching/,"ETC3550: Applied Forecasting for Business and Economics Reliable forecasts of business and economic variables must often be obtained against a backdrop of structural change in markets and the economy. This unit introduces methods suitable for forecasting in these circumstances including the decomposition of time series exponential smoothing methods ARIMA modelling and regression with auto-correlated disturbances. Students can expect to enhance their computer skills with exercises using R.
Handbook entry
Textbook: Forecasting: Principles and Practice Hyndman &amp; Athanasopoulos (3rd ed."
1,1,1,About me,https://robjhyndman.com/about/,"The following bio and this photo may be used in media releases without further permission.
 Rob J Hyndman FAA FASSA is Professor of Statistics and Head of the Department of Econometrics and Business Statistics at Monash University. From 2005 to 2018 he was Editor-in-Chief of the International Journal of Forecasting and a Director of the International Institute of Forecasters. Rob is the author of over 200 research papers and 5 books in statistical science."
1,1,1,Handling papers as an IJF associate editor,https://robjhyndman.com/ijf/associate-editor-instructions/,"Using ManuscriptCentral When you log into ManuscriptCentral you will see an &ldquo;Associate Editor Center&rdquo; along with your &ldquo;Reviewer Center&rdquo; and &ldquo;Author Center&rdquo;. These are for different roles that you have with the journal. You will mostly be using the &ldquo;Associate Editor Center&rdquo;.
Please log in every week or two to check on papers you are handling and to handle any new papers you have been allocated. You will receive email notifications about new papers but you won&rsquo;t necessarily receive any notifications about papers where the reviewers are overdue."
1,1,1,Handling papers as an IJF editor,https://robjhyndman.com/ijf/editor-instructions/,"Using ManuscriptCentral When you log into https://mc.manuscriptcentral.com/ijf you will see an &ldquo;Associate Editor Center&rdquo; and a &ldquo;Handling Editor Center&rdquo; along with your &ldquo;Reviewer Center&rdquo; and &ldquo;Author Center&rdquo;. These are for different roles that you have with the journal. You will mostly be using the &ldquo;Handling Editor Center&rdquo;.
Please log in every week to check on papers you are handling and to handle any new papers you have been allocated. You will receive email notifications about new papers but you won&rsquo;t necessarily receive any notifications about papers where the associate editors are overdue."
1,1,1,Handling papers as an IJF guest editor,https://robjhyndman.com/ijf/guest-editor-instructions/,"Using ManuscriptCentral When you log into ManuscriptCentral you will see an &ldquo;Associate Editor Center&rdquo; along with your &ldquo;Reviewer Center&rdquo; and &ldquo;Author Center&rdquo;. These are for different roles that you have with the journal. When acting as a guest editor you will be using the &ldquo;Associate Editor Center&rdquo;.
Please log in every week or two to check on papers you are handling and to handle any new papers you have been allocated."
1,1,1,In the news,https://robjhyndman.com/in-the-news/,"&ldquo;All eyes online&rdquo; &ldquo;Monash Awards&rdquo; 10 June 2021.
  “This is really a marathon.” Meet the statistician helping forecast the spread of COVID-19 Monash Impact 25 May 2021.
  The Australia Academy of Science Elects ACEMS Chief Investigator as New Fellow ACEMS News 25 May 2021.
  Leading Monash University scientists elected Fellows of Australian Academy of Science Monash News 25 May 2021.
  Distinguished statistician elected Fellow of the Australian Academy of Science Monash News 25 May 2021."
1,1,1,Research team,https://robjhyndman.com/research-team/,Potential PhD students: please read Advice to PhD Applicants Potential research fellows: All available positions are filled.  Current research fellows  Puwasala Gamakumara  Stuart Lee  Zhuo Li  Current PhD students  Lakshan Bernard. Analytical tools for future power networks (PhD begun 2020). Fan Cheng. Manifold learning on empirical probability distributions (PhD begun 2019).  Sayani Gupta. Visualization of probability distributions of deconstructed temporal data (PhD begun 2018).
2021,6,25,"Data privacy isn&#8217;t a compliance checkbox, but a competitive advantage",https://www.ibm.com/blogs/journey-to-ai/2021/06/data-privacy-isnt-a-compliance-checkbox-but-a-competitive-advantage/,"In the post-GDPR era data privacy has taken center stage yet again due to digital transformation across the globe.  Governments everywhere are enforcing more robust data protection guidelines to address new digital interactions between enterprises and consumers as well as to increase accountability from enterprises in the use and protection of data. Accordingly there has [&#8230;]
The post Data privacy isn&#8217;t a compliance checkbox but a competitive advantage appeared first on Journey to AI Blog."
2021,6,24,Reducing neonate mortality rates with AI and Edge computing,https://www.ibm.com/blogs/journey-to-ai/2021/06/reducing-neonate-mortality-rates-with-ai-and-edge-computing/,"She sees the concern in their eyes. She hears &#8220;we&#8217;ll take good care of you&#8211;both.&#8221; Then she awakes to learn her child is in intensive care 3 lb. 7 oz. and eight weeks early. Days into his stay he is jaundiced—a complication of sepsis. Thus begins the adventure into the unknown.  –Reflections from a preemie&#8217;s [&#8230;]
The post Reducing neonate mortality rates with AI and Edge computing appeared first on Journey to AI Blog."
2021,6,23,Infuse intelligent automation at scale with IBM Cloud Pak for Data 4.0,https://www.ibm.com/blogs/journey-to-ai/2021/06/infuse-intelligent-automation-at-scale-with-ibm-cloud-pak-for-data-4-0/,"When’s the last time you considered if you’re operating in a truly predictive enterprise furthermore if it’s easy for your data consumers models and apps to access the right data? More often than not the answer is a resounding “not very”. Between the proliferation of data types and sources and tightening regulations data is often [&#8230;]
The post Infuse intelligent automation at scale with IBM Cloud Pak for Data 4.0 appeared first on Journey to AI Blog."
2021,6,17,Trustworthy AI helps Regions Bank better serve customers,https://www.ibm.com/blogs/journey-to-ai/2021/06/trustworthy-ai-helps-regions-bank-better-serve-customers/,"Financial institutions worldwide are feeling the scrutiny from both customers and regulators alike. Perceptions of an institution’s governance practices including its commitment to ethics fairness explainability and transparency of decisions are critical to its standing. No wonder those poised to gain a competitive advantage today want to ensure their AI is fair trustworthy and explainable. A member of the S&#38;P 500 Index Regions Financial Corporation is one of the United States’ largest full-service [&#8230;]
The post Trustworthy AI helps Regions Bank better serve customers appeared first on Journey to AI Blog."
2021,6,16,"Operationalize AI: You built an AI model, now what?",https://www.ibm.com/blogs/journey-to-ai/2021/06/operationalize-ai-you-built-an-ai-model-now-what/,"Global AI Adoption Index 2021 reports the top drivers of AI adoption in organizations are: 1. Advances in AI that make it more accessible (46%); 2. Business needs (46%); and 3. Changing business needs due to COVID-19 (44%). To bring AI models into production businesses are also mitigating the following AI modeling and management issues: [&#8230;]
The post Operationalize AI: You built an AI model now what? appeared first on Journey to AI Blog."
2021,6,14,How bakery company Vaasan used AI to upgrade their planning,https://www.ibm.com/blogs/journey-to-ai/2021/06/how-bakery-company-vaasan-used-ai-to-upgrade-their-planning/,"The Finnish baker Vaasan knows a thing or two about fast delivery. After all the company’s roots date back to year 1849 which makes Vaasan one of the oldest nationwide bakeries in Finland. Vaasan is best known as the producer of Finland’s most popular bread Vaasan Ruispalat. The company has to be fast because the [&#8230;]
The post How bakery company Vaasan used AI to upgrade their planning appeared first on Journey to AI Blog."
2021,6,4,How do you drive exponential growth in the healthcare industry?,https://www.ibm.com/blogs/journey-to-ai/2021/06/how-do-you-drive-exponential-growth-in-the-healthcare-industry/,"The healthcare industry is adapting to changes resulting from the coronavirus pandemic but many complex challenges prevail. How do we anticipate and prevent hospitalization of high-risk patients? How can we reduce the length of stay without compromising quality of care? How do we improve patient experience? How do we obtain the insights needed to drive [&#8230;]
The post How do you drive exponential growth in the healthcare industry? appeared first on Journey to AI Blog."
2021,6,3,IBM Planning Analytics delivers continuous integration with Watson,https://www.ibm.com/blogs/journey-to-ai/2021/06/ibm-planning-analytics-delivers-continuous-integration-with-watson/,"IBM’s Global C-suite study recently validated that data-driven organizations are 178% more likely to outperform their peers in terms of revenue and profitability. It’s no surprise that more and more companies are moving beyond basic Financial Planning and Analysis (FP&#38;A) and toward adopting a mindset of Gartner&#8217;s newly-dubbed &#8220;Extended Planning &#38; Analysis&#8221; (xP&#38;A)—or what we [&#8230;]
The post IBM Planning Analytics delivers continuous integration with Watson appeared first on Journey to AI Blog."
2021,5,28,You could be paying less for software licensing,https://www.ibm.com/blogs/journey-to-ai/2021/05/you-could-be-paying-less-for-software-licensing/,"High licensing and maintenance fees Gartner Inc. defines the TCO for enterprise software as the total cost an organization incurs to use and maintain software technology over time. To calculate TCO companies consider direct costs such as hardware software and administration and indirect costs including human resources project management and downtime. But there’s one key [&#8230;]
The post You could be paying less for software licensing appeared first on Journey to AI Blog."
2021,5,27,Making Data Simple: What does Legacy Powers Legendary mean?,https://www.ibm.com/blogs/journey-to-ai/2021/05/making-data-simple-what-does-legacy-powers-legendary-mean/,The post Making Data Simple: What does Legacy Powers Legendary mean? appeared first on Journey to AI Blog.
2021,5,26,How to Succeed With AI: Do It Backwards,http://feedproxy.google.com/~r/jtonedm/~3/-Elew1DNyck/,Copyright © 2021 https://jtonedm.com MarketingThe single most critical and most neglected aspect of artificial intelligence (AI) projects is problem definition. All too often teams start with data determine what kind of machine learning (ML)/AI insights they can generate and then go off to find someone in the business who can benefit from it. The result? [&#8230;]
2021,3,4,Keep Your Business Logic Out of (Code) Jail,http://feedproxy.google.com/~r/jtonedm/~3/souQ77yEqyg/,Copyright © 2021 https://jtonedm.com James Taylor Our friends at Data Decisioning forwarded an article from The Register recently &#8211; Inflexible prison software says inmates due for release should be kept locked up behind bars. The basic building blocks of this story is that there is a module calculating release dates for prisoners that was clearly [&#8230;]
2021,1,4,Decision-Driven Data Analytics,http://feedproxy.google.com/~r/jtonedm/~3/XzX9UH1-EU8/,Copyright © 2021 https://jtonedm.com James Taylor Bart de Langhe and Stefano Puntoni recently published a great article in the MIT Sloan Management Review called &#8220;Leading With Decision-Driven Data Analytics.&#8221; In contrast to so much of the literature that focuses first on data they focus on decision-making. In fact they go so far as to say [&#8230;]
2020,12,3,Maximizing the ROI of your business rules investment,http://feedproxy.google.com/~r/jtonedm/~3/hApk14JWzCE/,Copyright © 2021 https://jtonedm.com James Taylor We help a lot of clients select install and adopt a Business Rules Management Systems (BRMS). These clients are looking to get automate decision-making with transparency deliver business control of their critical decision-making logic and establish an ability to drive continuous improvement through simulation and impact analysis. Adopted correctly [&#8230;]
2020,10,19,Job Opening at Decision Management Solutions – Delivery Management,http://feedproxy.google.com/~r/jtonedm/~3/1cps1d_dKz0/,Copyright © 2021 https://jtonedm.com James Taylor POSITION FILLED Decision Management Solutions is growing and looking for a Delivery Manager for its projects. The Delivery Manager will be an experienced hybrid agile project manager and will be responsible for managing several concurrent discipline based high visibility projects using agile and fixed milestone methods in a fast-paced [&#8230;]
2020,10,6,Machine Learning Week 2021 Call for Speakers,http://feedproxy.google.com/~r/jtonedm/~3/9I7_tQdYQYk/,Copyright © 2021 https://jtonedm.com James Taylor The team at Machine Learning Week/Predictive Analytics World has announced the schedule for 2021 (virtual conference May 24-28 2021) and issued their call for speakers. This is a great conference and will be a great opportunity to present. As always those with case studies and real experience will be [&#8230;]
2020,10,2,Getting Executive Support for Machine Learning – Backwards,http://feedproxy.google.com/~r/jtonedm/~3/1G5SM28_RE8/,Copyright © 2021 https://jtonedm.com James Taylor Eric Siegel and I had a great discussion about doing Machine Learning BACKWARDS recently &#8211; you can watch the recording below or on our YouTube Channel. Eric if you don&#8217;t know is the&#160;founder of Predictive Analytics World a leading consultant and author of &#8220;Predictive Analytics&#8220;. You can also check [&#8230;]
2020,8,10,Go to war with the data you have,http://feedproxy.google.com/~r/jtonedm/~3/mLRFHCv7QkY/,Copyright © 2021 https://jtonedm.com James Taylor A few months back Scott Adams posted a great Dilbert that I have been meaning to write about for a while (click on the image to see the original). In the strip Dilbert says &#8220;You don&#8217;t go to war with the data you need. You go to war with [&#8230;]
2020,7,16,Analytic Enterprises: Three Critical Success Factors,http://feedproxy.google.com/~r/jtonedm/~3/-eKXobyExb0/,Copyright © 2021 https://jtonedm.com James Taylor Working with companies that are investing in becoming analytic enterprises we have determined that there are three critical success factors. Whether you are focused on business analytics data mining predictive analytics machine learning artificial intelligence or all of the above these factors will be critical. Check out these videos [&#8230;]
2020,6,18,Gartner Top 10 Trends include Decision Intelligence,http://feedproxy.google.com/~r/jtonedm/~3/bqSocPTL7pY/,Copyright © 2021 https://jtonedm.com James Taylor Gartner recently published a piece &#8220;Top 10 Trends in Data and Analytics 2020&#8221; that you can currently get from our friends at ThoughtSpot (registration required). It&#8217;s an interesting report you should definitely check out. My favorite section was the one on Decision Intelligence within which they include the kind [&#8230;]
2021,6,21,Tips for Using Photos in Data Storytelling,https://www.juiceanalytics.com/writing/tips-for-using-photos-in-data-stories,"Data storytelling needs more than a collection of data visualizations. It is about weaving an engaging message that combines data exploration with narrative descriptions and graphics. Photographs in particular can play a vital role to help your data story grab attention and provide emotional engagement.Photos have always been an important element that we integrate into our data storytelling designs. Here’s why:Photos can underscore a theme or message;Photos can bring an element of humanity and emotion to a data story;Photos provide some visual space between information-dense charts and text;Photos can be used to attract and guide the attention of your audience. People will be instantly drawn to the photos first.Now we’ve made it trivially easy for you to add photos to your data stories in Juicebox.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


We’ve integrated into Unsplash which has over 2 million photos and includes a license that allows for free and commercial use of the photos (though you may not sell them). You can also upload your own images when you want to add company logos product images or a picture of a happy customer.How should you think about picking photos to include in your data story? Here are the most important do’s and don’ts:Balance the colors👍You want the colors of your photos to complement the primary color of your data story. If the content is largely white/black/grey a photo with bright colors will bring energy. If you are already using bold colors find a photo that shares the color scheme or is in black &amp; white.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            Connecting colors in the photo to other colors in your design
          
        
      
        
      

    
  


  


👎 You don't want the colors in your photo to compete with the colors used in the rest of your data story. A vivid photo will leave your reader wondering where to place their attention.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            Purples and greens aren’t good friends. Consult your color wheel.
          
        
      
        
      

    
  


  


Match the theme or mood👍The photo should align with the overall message of your data story. Is your theme dark hopeful energetic serious? The right photo will help you set the tone you are looking for.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            A spooky picture to go with the UFO theme.
          
        
      
        
      

    
  


  


👎 You want to avoid dissonance between the story theme and the images. A serious topic is not well suited to a silly image. A data story about people should include photos of people rather than machines or abstract concepts.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            Hooray the kangaroos are excited about your key metrics!
          
        
      
        
      

    
  


  


Avoid on-the-nose-ness👍The best photos will alude to your message or theme without being overly literal. To do so the photos might pick out a specific example or on the other hand express broad feelings or concepts that are relevant to your data story.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            A discussion about architecting your story
          
        
      
        
      

    
  


  


👎 Avoid using photos that literally present the subject of your data story. The photos are more likely to receive groans from your audience as they realize what you've done. They are the ""Dad Jokes"" of design.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            Get it?! Story + Structure.
          
        
      
        
      

    
  


  


Avoid stock photos👍You want to look for photos that bring a sense of authenticity or reality to your data story. Better to find an image of a specific location product customer or activity -- rather than something that feels generic.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            Show the top paid athletes
          
        
      
        
      

    
  


  


👎We’re all tired of the unnaturally happy business people sitting at a conference table. Or the close up image of shaking hands. This is one reason we value our integration with Unsplash which provides photos by photographers with a specific eye for authentic details.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            A generic “athlete”
          
        
      
        
      

    
  


  


Abstraction vs. Detail👍There is a place for abstract photos (when you are setting a mood or feeling) and detailed photos (when you want to show something important for your readers). Consider what the story needs to enhance your message.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            A photo that suggests zooming-in
          
        
      
        
      

    
  


  


👎If you are showing a lot of data a detailed photo may distract the reader from the content you want to be the focus of your message. On the other hand add a detailed photo -- and give it some visual space -- if you want the reader to linger on the image.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            A complex visualization competes for attention with a crowd of people.
          
        
      
        
      

    
  


  


Data Storytelling and Photo Composition


	Juicebox ❤️ Photos"
2021,6,18,Data Storytelling: What's Easy and What's Hard,https://www.juiceanalytics.com/writing/data-storytelling-whats-easy-and-whats-hard,"Putting data on a screen is easy. Making it meaningful is so much harder. Gathering a collection of visualizations and calling it a data story is easy (and inaccurate). Making data-driven narrative that influences people...hard.Here are 25 more lessons we've learned (the hard way) about what's easy and what's hard when it comes to telling stories with data. We also included links to our 🎓 Data Storytelling Lessons where they might help make things a little less hard for you.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            Michał Parzuchowski
          
        
      
        
      

    
  


  


Easy — Picking a good visualization to answer a data question …🎓 How to Choose the Right ChartHard — Discovering the core message of your data story that will move your audience to action …🎓 Story ElementsEasy — Knowing who is your target audienceHard — Knowing what motivates your target audience at a personal level by understanding their everyday frustrations and career goalsEasy — Collecting questions your audience wants to answerHard — Delivering answers your audience can act onEasy — Providing flexibility to slice and dice dataHard — Balancing flexibility with prescriptive guidance to help focus on the most important things …🎓 Explore vs. ExplainEasy — Labeling visualizationsHard — Explaining the intent and meaning of visualizations …🎓 Relatable and SpecificEasy — Choosing dimensions to showHard — Choosing the right metrics to show …🎓 Metrics: Your Story CharactersEasy — Getting an export of the data you needHard — Restructuring data for high-performance analytical queriesEasy — Discovering inconsistencies in your dataHard — Fixing those inconsistenciesEasy — Designing a data story with a fixed data setHard — Designing a data story where the data changes …🎓 Explore vs. ExplainEasy — Categorical dimensionsHard — Dates and timesEasy — Showing data values within expected rangesHard — Dealing with null valuesEasy — Determining formats for data fieldsHard — Writing a human-readable definition of data fieldsEasy — Getting people interested in analytics and visualizationHard — Getting people to use data regularly in their job …🎓 Data Personality ProfilesEasy — Picking theme colorsHard — Using colors judiciously and with meaning …🎓 Color and ContrastEasy — Setting the context for your storyHard — Creating intrigue and suspense to move people past the introduction …🎓 Narrating Data StoriesEasy — Showing selections in a visualizationHard — Carrying those selections through the duration of the storyEasy — Creating a long shaggy data storyHard — Creating a concise meaningful data story …🎓 Story StructureEasy — Adding more dataHard — Cutting out unnecessary dataEasy — Serving one audienceHard — Serving multiple audiences to enable new kinds of discussions …🎓 Data Personality ProfilesEasy — Helping people find insightsHard — Explaining what to do about those insights …How to Ensure Your Actionable Insights Lead to ActionEasy — Explaining data to expertsHard — Explaining data to novices …🎓 Relatable and SpecificEasy — Building a predictive modelHard — Convincing people they should trust your predictive modelEasy — Visual mock-ups with stubbed-in dataHard — Visual mock-ups that support real-world dataEasy — Building a visualization toolHard — Building a data storytelling tool


	Try Juicebox Free"
2021,6,14,How to Ensure Your Actionable Insights Lead to Action,https://www.juiceanalytics.com/writing/how-to-ensure-your-actionable-insights-lead-to-action,"“Actionable insights” is the Holy Grail of analytics. It is the point at which data achieves value when smarter decisions are made and when the hard work of the analytics team pays off. Actionable insights can also be elusive — a perfectly brilliant insight gets ignored or a comprehensive report gathers dusts on a shelf.The myth persists: If you build it they will come. The purity of your insight will compel your audience to action.Of course that’s not how it works. There is rarely an analytical finding that is by itself so obvious clever and compelling that other people in your organization are forced to act. The reality is that an analytic insight needs to be nurtured and sold. Like a new product people need to understand it see how it fits into their life compare it to the status quo and see the case for making change.Your actionable insight needs to connect to the things that are important to your audience. The following diagram shows the eight things that you’ll want to consider to evaluate whether your insight is likely to deliver the action you are hoping for. Consider it a checklist starting from the top (“Attention”) and proceeding clockwise.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


These are the stumbling blocks that we’ve seen as perfectly intelligence and researched insights go to a pre-mature grave. A good insight is a terrible thing to waste.


	Try Juicebox Free"
2021,6,8,7 Must-Have Data Visualization Skills for Data Analysts,https://www.juiceanalytics.com/writing/7-must-have-data-visualization-skills-for-data-analysts,"When I speak to people new to the analytics field they often wonder what skills will make them successful in their career. For all the data science skills tools and boot camps that are available I still find that the missing link for many data analysts is the ability to communicate and convince after they’ve analyzed data. All your hard work is wasted if it doesn’t spur someone to change. The smart folks at MIT agree:“The skill of data storytelling is removing the noise and focusing people's attention on the key insights.”To be a good data storyteller and communicator of data you need a collection of “soft skills” that are overlooked when people think of data analysis. Here are the 7 skills I think are most important:1. Ask the tough questionsThe common trap for data analysts is to become an order taker. Your boss asks a series of off-the-cuff questions and you run off to answer them — only to find that the answers don’t get to the crux of the issue. The cycle continues.A skilled analyst asks tough questions before they start getting into the data. “What actions could we take if you knew that answer?” Nancy Duarte a leader in business storytelling emphasizes the phrase “therefore we need to…” appended to the results of an analysis. Know the purpose of your work and how it will effect change.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


2. Develop audience understandingAs a data analyst you need to get into the shoes of your audience. What are their priorities? What actions can they take in their role? The more you know about the people who will consume your insights the more you’ll be able to shape them in a way that they are useful. Our Data Personality Profile is one way to build this type of understanding. 








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


3. Basic visual design skillsWhen you are sharing your analysis you will impress a lot of people if you apply some basic design principles.Learn how to properly use color and contrast in your charts apply our Simple Font Framework to make your text look great and remove distracting visual elements (e.g. chartjunk). You’ll be considered the data artist 👩🏾‍🎨🎨in many organizations. 








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


4. Edit and simplify to preserve attentionIn your role as a data analyst you will be challenged to get the attention of your manager or peers when you share data. Therefore it is incumbent on you to bring focus and clarity to your message. You’ll want to remove data that is merely interesting and guide attention to the data that is most actionable.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


5. Practice rapid prototypingWhen you first create a report dashboard or data presentation you will inevitably leave some important questions unanswered. Your audience won’t fully understand your message or how to read you data visualizations. That’s ok. Putting data solutions in front of customer and then learning from their reactions is part of the process. Make sure to listen carefully and move quickly to respond.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


6. Gather feedbackDelivering a presentation analysis or report can feel like the finish line. Often it is not. As noted above you may need more cycles of refinement to ensure your hard work is having the impact it deserves.Seek out feedback: Did they understand your metric definitions or the charts you used? Did they interpret the data in the same way as you? The more you ask the more you will learn and grow.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


7. Learn storytelling with dataStorytelling with data is a powerful skill for analysts to connect with their audiences. Our collection of lessons on data storytelling includes guidance around structuring your data stories choosing the right metrics and writing a guided narrative.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


&nbsp;


	Get Your Free Workspace"
2021,5,31,"Google Data Studio Dashboard, Remixed",https://www.juiceanalytics.com/writing/remaking-a-google-data-studio-dashboard,"When Najmah Salam at Notion demonstrated how to build a Google Data Studio dashboard to show email campaign data I had to create the Juicebox version. This dashboard is a common use case for many marketers who want to see how their email efforts are performing and identify what is working.I appreciate that Najmah used a tool they were comfortable with. Here’s how their dashboard came out:








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            https://datastudio.google.com/u/0/reporting/c03801dd-e707-4ca6-a1fe-11daad8d390c/page/FthGC
          
        
      
        
      

    
  


  


Data Studio is eager to fit all the content on one page in the traditional “collage of data visualizations” style.Juicebox offers a different approach more reminiscent of scrollable websites and data journalism. Here’s what I put together:This exercise helps highlight a few things our design team cares about. We want to make presenting data…Easier than a HelloFresh meal kit. We are dead-set on making it dead-simple whether you are calculating measures adding visualizations or laying out your app.More collaborative than Among Us. Google Data Studio shows the data in a largely static view. We want to enable discussions through interaction and exploration. Good data visualization will raise more questions than it answers. Interactivity lets you answer the next level of questions…like Who’s the imposter?!?Better looking than a Telsa Cybertruck. No offense Cybertruck. We want to make it easy to deliver an attractive balanced modern web app not something from a ‘90s Paul Verhoeven film.As portable as an iPhone. What if I could fit 1000 data points in my pocket…or a million. A modern visualization tool should be responsive for viewing on mobile devices. Data Studio is still working on that.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            Email Report Dashboard
          
        
      
        
      

    
  


  


To sum up: Juicebox is like HelloFresh prepared in a Ford F150 Lightning while playing Among Us on your phone.


	Get Your Free Workspace

"
2021,5,26,Data Storytelling Is a Sandwich,https://www.juiceanalytics.com/writing/data-storytelling-is-a-sandwich,"What is made out of layers and becomes more delicious when you put them together?Why…a data story of course! And a sandwich.(This is what happens when you write a blog post after learning your favorite sandwich shop is closing down…we’ll miss you Clawson’s 😫)








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            Sara Cervera via Unsplash
          
        
      
        
      

    
  


  


What are those layers?An effective data story should have four distinct parts not unlike the layers of a sandwich:The filling: Whether deli meat or peanut butter &amp; jelly the filling is the heart of a sandwich. For data stories that filling is the data visualizations.The foundation bread. This is the bedrock on which the story is built. Big picture numbers (a.k.a. Big-Ass Numbers) that set the stage of the story.The finishing bread. This is the final layer that tells us it is time for action.The condiments and toppings. These are the flavor boosters that connect the filling to the bread. In data stories these flavor boosters are often specific examples call-outs or insights that capture the attention of the audience.As a data storytelling artist your job is to get quality ingredients and assemble them in a logical order. Let’s take a bigger bite of this metaphor…The fillingSometimes people make the mistake of thinking that a lonely data visualization is itself a data story. That’s like saying a slice of ham is a ham sandwich. Sure the data visualization is the star of the show. But it needs to be wrapped in a few more layers to make it complete.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            Juicebox
          
        
      
        
      

    
  


  











  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            Karo Kujanpaa via Unsplash
          
        
      
        
      

    
  


  


The foundation breadThe foundation of a data story are the key metrics. They are often shown as ‘Big Ass Numbers’ — a summarized simplified powerful statement about what matters (“metrics are the characters of your story”). These BANs have the potential to deliver an impact if you can communicate them in the right way.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            Juicebox
          
        
      
        
      

    
  


  











  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            Hans Vivek via Unsplash
          
        
      
        
      

    
  


  


The finishing breadWhen you put that slice of bread on the top you know it is time for action. Eating action! Same with a data story. You want to end the story with an action your audience can do something about. 








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            Killer Heat Interactive Tool
          
        
      
        
      

    
  


  











  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            Jordan Nix via Unsplash
          
        
      
        
      

    
  


  


The condiments and toppingsThe bread and fillings fulfill the basic requirements of a sandwich; taking it extraordinary requires condiments and toppings. This is where the sandwich and data storytelling artists distinguish themselves.Specific examples that provide human-sized connections;Narrative text that ties the story together;Design touches with color emojis fonts that make the story irresistible.(By the way here’s a favorite April Fools joke my team played on me a mayo-lover)








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            Juicebox
          
        
      
        
      

    
  


  











  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            Eaters Collective via Unsplash
          
        
      
        
      

    
  


  


Of course your data story/sandwich is only as good as the ingredients you use.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            amirali mirhashemian via Unsplash
          
        
      
        
      

    
  


  





	Try Juicebox Free"
2021,5,19,Keys to Data Fluency: Shared Understanding,https://www.juiceanalytics.com/writing/keys-to-data-fluency-shared-understanding,"Building a Data Fluent organization requires getting everyone on the same page. This common understanding spans everything from cultural expectations to accessing and using data. Below I’ve outlined six areas where creating alignment will have long-term benefit…and where you can go to get started today.&nbsp;








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            Photo by Nicole Baster via Unsplash
          
        
      
        
      

    
  


  


&nbsp;Shared ExpectationsLeaders need to define and communicate how they expect data to be used in the organization. As I’ve written time and again it incumbent on executives to set the standard for data culture.Where to get started: The Data Lodge provides guidance and training to help data leaders build their organizational culture and capabilities.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


Shared SkillsYour team needs the know-how to understand analyze and communicate data. These skills are not always a prevalent as we’d like.Where to get started: DataLiteracy.com provides a growing collection of courses for data skills.Where else to get started: Quanthub offers an adaptive educational platform focused on building data skills in your organization.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


Shared Definitions and TerminologyYou want everyone in your organization to know what is meant by the data being shared. Who qualifies as a lead? What is an active customer? How is revenue calculated? Without arriving at shared definitions and terminology your data discussion will get stuck in fruitless debates.Where to get started: There are many high-tech Master Data Management solutions…not the place to start. Create a shared document where you define: 1) how your key data/metrics are calculated; 2) where this data comes from; 3) how this data might be improved. Link to it when you present data in a dashboard report or data story.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


Shared Data AccessIncreasingly we’ve run into IT teams who have found a way out of the endless back-and-forth data requirements cycle. Instead they generate frequently-updated and thoroughly-tested tables for analysis. Now business users can have the flexibility to create with less risk of misinterpreting data.Where to get started: One of the best tools we’ve found for this is Keboola a flexible platform for connecting to data ETL and providing a data catalog.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


Shared Metrics and GoalsData fluent organizations have a shared set of key metrics and can explain how these metrics link to organizational goals.Where to get started: We  appreciate the simplicity and clarity of Matt Lerner’s Metrics that Matter. His framework provides a roadmap for defining your most important metrics.Where else to get started: TeamOnUp provides guidance and software for aligning around shared goals and defining clear responsibility.    Matt Lerner | Metrics that Matter | BoS USA Online 2020  from Business of Software Conference Shared Data ProductsA data fluent organization leans on a curated set of data products — dashboards reports presentations — for focus and insight.Where to get started: Juicebox is a lightweight and versatile solution for business users to create reports presentations dashboards and data stories. It is designed to make creating and sharing easier than anything else on the planet.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


"
2021,5,17,Data Storytelling and Photo Composition,https://www.juiceanalytics.com/writing/data-storytelling-and-photo-composition,"Photographers are intentional and often instinctual about how they arrange elements to capture and hold the attention of their viewers. Composition is the term for the art of composing an image through framing. While there are “rules of thumb” for how to compose photographs (or any visual expression) visual artists would just as quickly say that great composition is beyond rules subjective and a natural ability. This recognition in real life of a rhythm of surfaces lines and values is for me the essence of photography; composition should be a constant of preoccupation being a simultaneous coalition—an organic coordination of visual elements. —Henri Cartier-BressonData Storytelling is of course a visual medium. And as a new medium we are well served to build on the shoulders of giants. What are a few lessons we can draw from photography to compose our data stories? Here are a few:Leading LinesIn photographs: “The eye of the viewer will make its way through the frame of the photograph. The path is not always predicable but how you arrange objects in the photograph or how you frame the scene can serve as a guide for the eye’s (hopefully) pleasing journey through your image—a journey that allows the viewer to understand the meaning of your photograph.” Todd Vorenkamp








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            Image by&nbsp;Pierre Metivier.
          
        
      
        
      

    
  


  


In data stories: As an author your role is to guide the reader through the information. You can help people navigate through pointers leading text and numbering steps. Give them the signposts to know where to start and where to go next.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            Top 20 Best Data Storytelling Examples
          
        
      
        
      

    
  


  


Foreground the SubjectIn photographs: Photos are inherently two-dimensional. By composing a distinct foreground and background the artist is able to create depth and interest.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            https://petapixel.com/2016/09/14/20-composition-techniques-will-improve-photos/
          
        
      
        
      

    
  


  


In data stories: It is critical to set the context of your story up-front (foregrounding) then draw your reader into the depths of the story gradually. In this example a data story about Cicadas grabs attention with a title image before leading into the data exploration.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            Top 20 Best Data Storytelling Examples
          
        
      
        
      

    
  


  


Balance Symmetry and PatternsIn photographs: People are naturally drawn to patterns symmetry and balance. It provides a sense of harmony that allows the viewer to linger on an image.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            https://petapixel.com/2016/09/14/20-composition-techniques-will-improve-photos/
          
        
      
        
      

    
  


  


In data stories: The analytical tool on the right provides a sense of balance by centering the text and providing the same number of choices for the two chart axes. For analytical interfaces providing a balanced UI can alleviate some of the user’s inherent fear of complexity. 








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            Lessons on Data Storytelling
          
        
      
        
      

    
  


  


SimplicityIn photographs: “Cut out all unnecessary details to keep keep the viewer's attention focused on the subject.” Photographymad








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            Image by Hien Nguyen.
          
        
      
        
      

    
  


  


In data stories: Focus on the core message or key question you want your audience to understand. Remove data and content that will distract from this message.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            Top 20 Best Data Storytelling Examples
          
        
      
        
      

    
  


  


Negative SpaceIn photographs: By giving visual “breathing room” around the main subject you make it easier for the viewer to engage with the subject.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            https://petapixel.com/2016/09/14/20-composition-techniques-will-improve-photos/
          
        
      
        
      

    
  


  


In data stories: Traditional dashboards and report attempt to fill-up the entire screen to show as much data as possible at once. Providing negative space will give your readers the opportunity to see what is most important and focus their attention. 








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            Lessons on Data Storytelling
          
        
      
        
      

    
  


  


&nbsp;


	Try Juicebox Free"
2021,5,14,Interview with SourceForge: Bringing Data-driven Decisions to a Broad Audience,https://www.juiceanalytics.com/writing/interview-with-sourceforge-bringing-data-driven-decisions-and-communications-to-a-broad-audience,"I recently spoke with the team at SourceForge a leading platform for the distribution and discovery of software solutions. The interview ended up summarizing our journey as a company to transform how people communicate with data. Here’s the transcript:SourceForge: You have said that the challenges faced by the analytics industry are more social than technical. What did you mean by that?We’ve been in the analytics space for nearly two decades. The technology has advanced particularly in advanced analytics but the same problems persist. There is still a lack of engagement with data by many people in organizations. People have discomfort with using data to drive everyday decisions. That stuff doesn’t get solved through more features. And for many leaders there is a feeling of frustration for all the money they have spent on data projects. Where is the payback? How long are they going to have to wait? We can’t climb our way out of these problems by always betting on the machines to do more.For all the advances in artificial intelligence and machine learning it seems to me that the people-side of analytics continues to be neglected. When I say people-side I mean: what skills do everyday information workers need to be successful? How does the culture of an organization need to change to embrace using data? How do we meet people where they are to help them become more data fluent?SourceForge: Given that why have you focused your company on building yet another tool?








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


Fair point. It is because creating change in the workplace is often the intersection of new behavior and new easier ways to enable those behaviors. Take Slack and how they transformed workplace communication. Our email inboxes were exploding and adding more features to email clients wasn’t solving the problem. Slack came along and re-thought how to make it easier for people to collaborate in teams.That’s how we think about Juicebox. There needs to be a fresh approach to how people take spreadsheets of data and turn it into something useful. The new approach needs to put people first not by making it more complex or feature bloated.SourceForge: But there are a lot of tools for visualizing data. Why did you feel like the world needed another one?The world certainly doesn’t need another dashboard-creating tool that’s for sure. Nor do we need something to try to replace the visual analytics behemoths Tableau and PowerBI. Those tools are essentially Excel on steroids. More capable. More visual. And more complicated.What these tools don’t focus on is how do we make sure the data gets communicated effectively. That’s the missing link. We sometimes call it the last mile of analytics. What has been missing is a solution that provides an easy accessible bridge between people who work with data and the minds of the decision-makers who should understand that data.With Juicebox we created a solution that is lightweight and accessible to everyone. It is easy to learn easy to get started. The everyday information worker doesn’t want to have to get an advanced certification to be able to visualize present and share data in their organization. They need something radically simpler. But also something radically more powerful than the Excel and PowerPoint that they are currently using to present data.That’s where Juicebox fits in. We experienced first-hand the frustration people feel. We set out to deliver a better mousetrap for communicating data.SourceForge: Let’s talk about those people. What have they struggled with and how does Juicebox help them?ZG: I believe there is a silent majority of people in the workplace who want to do more with data but don’t yet have the skills or tools.Think of it like all the want-to-be cooks who admire recipes online but find it too much effort to gather all the ingredients and learn to make the meal. For these people meal prep solutions came along like Blue Apron and HelloFresh. Suddenly anyone could whip up a darn good at-home meal. What did it take? It took some guidance some simplifying of the recipes and more convenience.It’s exactly the same for data in the workplace. Sure people have Excel and maybe access to a powerful analytics BI platform…but that doesn’t make it easy. With Juicebox we want to make it easy for these people to whip up something delicious with their data.&nbsp;








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


&nbsp;SourceForge: You are also the author of a book called ‘Data Fluency’ in which you present a path toward more effective use of data in organizations. How does your product fit into this framework?We wrote that book because it was clear that many organizations were struggling to really unlock the power of their data. I’m not talking about hiring more data scientists or applying machine learning models. They just want to know what is most important define key metrics see trends and find insights they could act on. It is the world of small data that still has so much untapped potential. We saw that the issues were about mindset and skillset not technology.In our book we propose four pillars that an organization needs to build to become data fluent. The pillars are: data consumers that are data literate; data authors that know how to communicate effectively; an organizational data-driven culture; and an ecosystem for designing and sharing what we call data products.Juicebox is a key that can help unlock some of these challenges. It gives data authors the most user-friendly solution for communicating and it serves as an integral part of a data product ecosystem. More than ever we believe that we need to make data a medium for communication. Juicebox is one piece of the puzzle.&nbsp;








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


&nbsp;&nbsp;


	Try Juicebox Free"
2021,5,7,14 Best Data Storytelling Tools 2021,https://www.juiceanalytics.com/writing/best-data-storytelling-solutions,"Data storytelling is quickly becoming a popular mode for presenting data. It combines text and graphics with data visualizations to guide an audience. Traditionally people have used tools like PowerPoint and Excel as well as traditional dashboard and business intelligence platforms to communicate in this way. But these solutions are limited in their ability to balance the explanatory and exploratory elements of an effective data story. We are seeing a new category of tool emerge: the data storytelling platform. It emphasizes features such as human-friendly visualizations integration of text and visuals narrative flow connected stories easy-to-learn authoring and effortless sharing.The demand for better data storytelling is being met by a growing collection of data storytelling tools. We evaluated tools that resembled the description above leaving out more technical tools visualization libraries and old-school dashboard/report tools. In the end we identified four unique categories:Guided AnalysisStand-alone VisualizationsData Storytelling as a FeatureDesign over DataStories with WordsGuided AnalysisThese solutions combine exploratory data visualization with explanatory text and graphical elements. The interactive data storytelling applications created by these platforms are intended as an alternative to traditional dashboards and reports.JuiceboxJuicebox combines modern data journalism style with exploratory visualizations that are automatically connected to enable analysis. A focus on easy authoring makes Juicebox the only tool in this category that is accessible to non-technical or non-analyst users. In their words: Create beautiful data visualizations that make you look like a proStrengths: Lightweight easy editing professional web design automatically connected visualizations.Cost: Free plan (up to 3 users). Team plan is $49/month for 5 editors 15 viewers.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


Toucan TocoToucan Toco is one of the earliest solutions for data storytelling. This platform targets enterprise buyers and has a unique approach to presenting data stories. Sharing annotation and drill-in story views give you a chance to communicate a comprehensive overview of a topic.In their words: Communicate actionable insights at scale using Toucan’s built-in no-code framework for storytelling.Strengths: Dashboard-style layout; user management features; sharing via presentation-mode for sharing.Cost: Annual subscription. Reach out for a quote.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


NugitNugit has flown under-the-radar for some customers but represents one of the most complete data storytelling solutions on the market. Attractive design combined with powerful text features make this a solution worth watching.In their words: A better way to share data with colleagues and customers. Automated tools for creating data stories on web and email.Strengths: Live API integrations report/email automation automated natural language generation infographic-style graphics.Cost: Not available.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


Stand-alone VisualizationThese visualization solutions offer flexibility and beautiful design to build individual visualizations. The end-product is generally intended to be embedded in a webpage often as part of an online article.FlourishFlourish has built a loyal customer base by delivering creative and beautifully-designed visualizations. They are well-known for their racing bar-chart but have many other visual options.In their words: Easily turn your data into stunning charts maps and interactive stories.Strengths: Animated visualization easy embedding fine-grain configuration of visualiations. Cost: Free tier. Paid plans start at $69/mo.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


RAWGraphsRAWGraphs is one of the quickest easiest ways to create advanced visualizations. An open source project with a long history this tool provides a simple step-by-step process to create downloadable images for embedding in webpages.In their words: The missing link between spreadsheets and data visualization.Strengths: Open source lightweight editing advanced visualizations data doesn’t leave your browser.Cost: Free.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


DatawrapperDatawrapper is a popular tool for data journalist around the world. With a collection of attractive visualizations and advanced maps Datawrapper gives you the configuration flexibility to craft the precise visual you need.In their words: Serving charts and maps for millions of readers every day. Datawrapper helps some of the world’s best teams to tell their stories with data.Strengths: Maps chart configuration options labeling features scaling for millions of views.Cost: Free plan. Pro plan $599/month.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


Data Storytelling as a FeatureThis set of solutions are comprehensive business intelligence and visual analytics platforms. Data storytelling is presented as a feature or technique that can be accomplished within the larger platform.Tableau Story PointsTableau a leader in visual analytics saw the potential for data storytelling early on. They released a feature called ‘Story Points’ in 2014. The feature has not achieved wide-adoption among their customer base and Tableau appears to be focusing on PowerPoint export options instead.In their words: Story Points is a way to build a narrative from data. People tend to understand and remember concepts through stories. Story Points gives anyone the tools to create a narrative with data.Strengths: Wide-adoption of Tableau; powerful data manipulation and visualization tools.Cost: $70/editor/month








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


Qlik Sense Stories ﻿Qlik Sense is a well-established analytics platform with strong visualization capabilities. While it gets less press than its competitors Tableau and PowerBI Qlik understands the need to reach broader audiences in the enterprise through data storytelling.In their words: The purpose of data storytelling is to turn data discoveries into a story. Emphasizing important elements helps create convincing stories and supports stakeholders in decision-making.Strengths: Powerful querying technology enables rapid analysis.Cost: $30/user/month








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


PowerBIPowerBI is Microsoft’s answer to the success of visual analytics powerhouse Tableau. Like the other solutions in this category PowerBI provides guidance features and instruction around data storytelling without providing a focused solution for users.In their words: The job of a data analyst is not just technical. It entails more than just transforming data into information. It is also about clearly communicating the key messages derived from this data.Strengths: Comprehensive BI platform; integrations with deeply-adopted technologies.Cost: Pro starts at $10/user/month.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


Design over DataThese solutions for designers are focused on creating infographics and presentations that may include charts and graphs as part of the document. The data is one of many media elements that tell the story.InfogramInfogram is a flexible design platform that includes capabilities for adding lightweight charts. It offers an array of formats for presenting information including everything from dashboards and reports to social media posts and posters.In their words: Create engaging infographics and reports in minutesStrengths: Consistent branding pre-defined templates animations output formatsCost: Free plan. Pro starts at $25/month/user








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


VismeWhether you want to create infographics posters social media graphics or even videos Visme is a designer’s toolbox. Like the other design-first tools charts are intended to show a few data points rather than to enable analysis.In their words: Create visual brand experiences for your business whether you are a seasoned designer or a total novice.Strengths: A vast collection of icons and widgets; 1000s of templates.Cost: Free plan. Pro starts at $25/month/user








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


PiktochartPiktochart is a design tool for building infographics posters flyers social media graphics and presentations. Data seems to be mostly an afterthought for a solution that focuses on brand styling and templates.In their words: Improve your internal and external communication with Piktochart. Quickly turn any text- or data-heavy content into a visual story that your audience will love.Strengths: Colors and branding video stories.Cost: Free plan. Pro starts at $29/month/user








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


Stories with WordsThese solutions focus on using words as the primary way to convey the story in the data. Their algorithms identify insights in the data and present those insights in sentences and bullet points.SiSense NarrativesSiSense is a traditional business intelligence and dashboard solution that has added narrative capabilities.In their words: With Sisense Narratives we use natural language generation (NLG) to automatically present you with calculations and insights in plain easy to understand language based on what the engine recognizes as interesting.Strengths: Integrated as part of a complete BI solution.Cost: Not available.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


Lexio by Narrative ScienceNarrative Science has been a leader for years in extracting analytical insights and presenting results as text. They believe that pre-defined dashboards should be replaced by automatically-generated text stories.In their words: No more dashboards. Data should be easy to use and understand. Start data storytelling with Lexio.Strengths: Machine learning to analyze your data without the need for analysts.Cost: Not available.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


"
2021,5,3,11 Best Data Storytelling Courses 2021,https://www.juiceanalytics.com/writing/best-data-storytelling-courses-2021,"Are you looking to upgrade your Data Storytelling skills? There are many options for learning including this list of best data storytelling workshops grabbing our free data storytelling lessons and absorbing the lessons of masters from 20 amazing data storytelling examples.If you are looking for a packaged course that will teach you about data visualization narrative and engaging your audience we’ve tracked down some of the best options. In our search we wanted to find solutions that were accessible to everyone delivered by an experienced instructor and did not focus on a particular piece of software.Story IQCourse: Data Storytelling for Business provides learners with a solid grounding in fundamental data storytelling learning concepts. By the end of the course learners will have the skills needed to produce impactful data visualizations layered with compelling narratives.Access: Self-paced onlineInstructor: StoryIQ is a small group of training professionals focused on hands-on practical teaching for a business audience.Cost: Starts at $99








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


Bill ShandlerCourse Data Storytelling &amp; Visualization Workshop teaches the art and science the practical hands-on tactics of creating compelling communication experiences.Access: By RequestInstructor: Bill Shandler has been “working with clients on information design and data storytelling/visualization projects for over 25 years. This includes governments NGOs and commercial enterprises across just about every industry.”Cost: N/A








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


Juice AnalyticsCourse: Data Storytelling Lessons. With more than 20 short lessons this course provides a complete overview of the skills tips and tricks required to become a data storyteller. The hands-on interactive lessons are self-paced and take 5-10 minutes to complete.Access: On-demand self-pacedInstructor: Zach Gemignani has spent 15 years helping organizations design and develop interactive analytical applications presentations and data stories. He is author of the book Data Fluency Empowering Your Organization with Effective Data Communication and has guided the development of a leading data storytelling platform Juicebox.Cost: Free








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


MIT Executive EducationCourse: Persuading with Data — highly practical and collaborative this course combines visualization and strategic communication best practices to help you communicate data more effectively and influence others to take action based on data through data storytelling.Access: Live Online on specific dates.Instructor: Miro Kazakoff is a Senior Lecturer in Managerial Communication at the MIT Sloan School of Management where he focuses on how individuals use data to persuade others.Cost: $4300








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


Data Story AcademyCourse: Data Story Academy is a three-part framework built for business professionals providing the tools they need to grow their career and access to frameworks that virtually guarantee success in doing more with data.Access: On-demandInstructor: Zack Mazzoncini has helped hundreds of organizations and individuals develop data-driven cultures centered around data storytelling. Zack's personal mission statement is: ""Change people's lives for the better by being the best version of myself"".Cost: $697








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


Plural SightCourse: Data Storytelling: Moving Beyond Static Data Visualizations. Learn how to package a data story for different mediums and audiences and how to craft a data story by defining your audience and end goals. Explore how to create animations and motion graphics to present an impactful moment.Access: On-demandInstructor: Troy Kranendonk is a Curriculum Manager for Data Access and Analytics as well as an author with Pluralsight. He considers himself to be a Pixel Ninja.Cost: $199-299 per year (Plural Sight subscription)








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


Knight CenterCourse: Data Visualization for Storytelling and Discovery. The four-week course which was powered by Google took place from June 11 to July 8 2018. We are now making the content free and available to students who took the course and anyone else who is interested in learning how to create data visualizations to improve their reporting and storytelling.Access: On-demandInstructor: Alberto Cairo is an information designer and professor. Cairo is the Knight Chair in Visual Journalism at the School of Communication of the University of Miami.Cost: Free








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


LinkedIn LearningCourse: Telling Stories with Data. The same techniques that are used to tell stories with words—structure conflict resolution emotion and surprise—can be used with data. You can craft compelling narratives that help audiences visualize information without complex charts or graphs.Access: On-demandInstructor: Paul A. Smith is author of the best-selling book Sell with a Story: How to Capture Attention Build Trust and Close the Sale.Cost: $30/month (for LinkedIn Learning)








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


PurdueCourse: Data Storytelling 101 offers an introduction to the concept of Data Storytelling why it matters and how it can transform the results of your research into impactful narratives from which your audience learns new things remembers important findings and acts on them.Access: OnlineInstructor: Sorin Adam Matei Data Storytelling Program Director and Associate Dean of Research Purdue University. Cost: Free. Additional paid courses are $350-$1000








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


BI BrainzCourse: Master BI Data Storytelling. An online course that will teach you how to easily setup build and design your first compelling data story!Access: On-demandInstructor: Mico Yuk Founder of BI Brainz and creator of the Analytics on Fire Podcast.Cost: $497-$697








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


&nbsp;Udemy (multiple courses)Tell a Story with Data. Learn how to pique and keep your audience's attention so they will understand and remember your data presentation. The instructor Mike X Cohen is a neuroscientist and associate professor at the Radboud University in the Netherlands.Data Storytelling and Data Visualization. You'll learn the skills that make up the entire art of speaking the language of data: from communicating with data to creating impactful data visualizations to storytelling with data to driving action with data-driven decisions and finally to creating stunning communications that will leave a lasting impression on an audience and get results. The instructor Joshua Brindley says: “I love telling great stories with data and driving results with visualization that resonate with an audience.”Access: On-demandCost: $14-18








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  





	Try Juicebox Free"
2021,5,1,The Art of Data Storytelling,https://www.juiceanalytics.com/writing/the-art-of-data-storytelling-pixar-style,"Data Storytelling is a powerful way to present data in ways that influence your audience. It is a skill that combines elements of artistic expression and structured methods. In this article we will start by learning from the mindset of a leading storytelling organization Pixar. Then we will discuss how to structure data stories to guide your audience through data.Part 1: Lessons in Data Storytelling from PixarPixar is the gold standard in storytelling. With their 17 feature films they’ve redefined how to create animated worlds and compelling characters.What if you could know the secrets of Pixar’s storytelling success? Now you can. Pixar announced recently that they would team up with Khan Academy to deliver free lessons on how they deliver storytelling magic.We’re long-time fans of Pixar at Juice because their methods don’t just apply to good storytelling but to good data storytelling as well.Pixar’s lessons offer some great principles that can be used to improve the narratives you create with your data. We’ve pulled out some of the best tips that can be applied to data storytelling:1. On Asking the “What If” QuestionAlthough our movies involve hundreds of people and take years to make they all begin with a simple idea about some world and character. What if there’s life out there in the universe? What if a rat wanted to cook haute cuisine? What if our toys that are all around us actually were sentient and can come alive? These what-if questions invite the imagination into a story we want to explore.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


“The best ‘what ifs’ are questions that sort of feel like a key that unlocks the door.”Asking “what if” is a great question to ask yourself when you’re first deciding what direction you want to take your narrative. Not only does asking this question guide how you structure your story but it also allows you to determine what information is most important to your audience and what can be left on the cutting room floor. It can be easy to overwhelm with data; narrowing your focus is the greatest favor you can offer your audience.Here are some “What If” questions you could apply to your data storytelling:What If my sales team knew exactly which prospects needed the most attention today?What If nurses could tell which patients were at risk for sepsis?What If human resources leaders could explore the complexity of their organization in the same way they explore Google maps zooming out to see how all the parts connect and zooming in to see what’s going on on the ground?What If teachers could visually see how each of their students was doing on their learning journey and quickly identify the knowledge gaps and resources to fill those gaps?2. On WorldA ‘what if’ statement is ultimately connected to a world and a character… When we say ‘rule’ what we really mean is the environment or set of rules in which our story will take place.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


Choosing your world can be most closely associated with the effort to set the context in our data stories. It’s important to ground the audience in the “world” before you start introducing the “characters” - your data. In our data apps we are careful to set the stage for the audience by explaining the purpose and context before thrusting users into a series of charts.3. On Flawed CharactersEntertaining characters are often deeply flawed...these flaws can also be the key to why audiences care about them.This lesson reminds us of “flawed” data points. Often the outliers and the unexpected data points are the most interesting. Sure they don’t tell the whole story but they definitely give more insight into what’s actually happening with your data and can provide some colorful detail in your data story.4. On Fully-Developed CharactersWe call these characters fully developed. This means we’ve gotten to know them so well that we can imagine them in almost any situation.Providing full context around the characters in your data allows you to be able to look at your data points from multiple perspectives and draw out three-dimensional insights an important step in data storytelling.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


5. On Behavioral CharacteristicsWe can talk about characters in two ways. They have external features which are their design their clothes what they look like. Then much more interesting is the internal features. Are they insecure are they brave are they jealous?With this lesson the distinction between descriptive and behavioral data comes to mind. For example we can look at descriptive data about customers but really the behavioral data is much more interesting. How do customers react to stimuli? That’s where the real story is.6. On Authentic ExperiencesCharacters have to come from authentic human emotions and experiences.When working constantly with numbers it’s easy to sometimes forget that behind each data point is a living breathing person. How do you connect the data to the actual real-life actions that are taking place to create that data? One of the best ways to do this is by bringing specific examples into your data story. As John Hodgman likes to say: “Specificity is the soul of narrative.”We created a complete lesson in our Data Storytelling curriculum about making data relatable and specific to help connect to your audience.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


7. On Story FlowWhat happens when I tell the story to another person is that these other things show up without me asking for them even while I’m telling them. The story starts to come alive. The characters start to come alive. And then also the person you told the story to will tell you what they thought of it notes they’re free. They actually are helping you make your story and characters better.&nbsp;








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


This is very true of data storytelling. The more you run through the story flow with users the more insight you receive into how they think about the data they are seeing and what they need to know. Based on this feedback you can adapt and change the way you present your data to make a better overall experience for your users.These lessons from Pixar give us some of the mindset it takes to tell a data story. Now let’s look at the structure that provides a strong architecture for your data story.Part 2: Data Story StructureWhile traditional storytelling and data storytelling are not identical mediums there is quite a bit of overlap between the two and many of the best practices for one can be applied to the other. Take for example the idea of structure when it comes to storytelling. Structure or in simpler terms “what do you want the audience to know and when?” is hugely important when it comes to the practice of data storytelling.It may seem counterintuitive to consider modeling your data presentations after traditional storytelling structure. After all storytelling is an inherently subjective act. The storyteller is crafting something that helps the audience learn about a theme that the storyteller finds important and consequently a moral that should be learned. Applying this to data can seem like enemy territory for analysts who feel that their job in presenting data is to “let the data tell the story.” It’s important to note however that the data doesn’t have an opinion on what is important. For example I was speaking to an HR Analytics team recently and it was clear to me that they wanted to use data to share important lessons with the business. It was less clear that they felt empowered to do so because they felt the data should speak for itself. Data often needs a voice to give it meaning.When creating the structure of your data stories keep in mind that it often takes a while to get to the structure that works best for what you are trying to accomplish. That is why it is important to create something ‒ even in a rough form ‒ and get it in front of people who will give you feedback. Does it resonate and connect with the audience ‒ or is it more like the unpopular original structure of Finding Nemo? Without this knowledge you’re more lost than Dory and Marlin ever were.&nbsp;Dive deeper into a lesson on story structure from our collection of Data Storytelling Lessons.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


Story Beats &amp; Story SpineAn effective way of organizing story structure is by utilizing story beats the most important moments in your story and story spine a pattern into which most stories can fit. While your data story most likely won’t open with “once upon a time…” and end with “and ever since then…” the lesson can still be applied. Using a structure that is broadly familiar to audiences and hitting familiar story beats will help ensure that a data story leverages the hooks that storytelling already has in people.Your audience is looking for certain things in a data story just like they would in a Pixar film. Who or what are the key players? What’s the conflict? How can it be resolved? Utilizing these when appropriate will make your data stories much more effective. The traditional 3-part play provides a template for designing data stories.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


Act 1The first act of a film serves to introduce the audience to a protagonist establish the setting provide information into how the characters’ world works and introduce an obstacle that sets the rest of the story in motion.In traditional dashboards and reports this information is often missing and leads to users not knowing where to start. If your audience is going to go on a data adventure with you they should start off by caring about the situation that exists. Data stories should start with a high-level summary that then lets users progressively and logically drill into more complex details and context.Act 2Pixar states that the second act of a story as “a series of progressive complications.” My favorite way of describing act two is “the part of the story in which you throw rocks at your characters.” Either way what happens in the next part of your data story is clear: addressing conflict.When it comes to data stories act two is the back-and-forth exploration of the problem. In the traditional story spine they refer to it as “because of that…”; for analytics we call it “slicing-and-dicing.” Throughout act two of your data story you are showing your audience the drivers of problems and identify any outliers.Act 3In traditional storytelling the third act is the part of the story where the main character learns what she truly needs — as opposed to what she thought she wanted. The character has gone on a transformation along the course of the story and that is evidenced in the final act.This is much harder to pull off in data storytelling. In data storytelling I believe the protagonist is the audience. Much like the main character the audience needs to be transformed and understand something new and important. A satisfying story is when a problem is fixed and the world is set right in some way. Great data stories deliver that change -- but to do so they need to do more than change the audience’s perspective. They need to make the audience act on not just discuss this transformation.Work BackwardThe best bit of advice from the Pixar storytellers is simple: work backward. This is how we do it at Juice: we consider what is the endpoint the change or impact that we want to make on the audience and then craft the story that can help get us there.


	Try Juicebox Free"
2021,4,28,A Review of Data Storytelling Solutions,https://www.juiceanalytics.com/writing/a-review-of-data-storytelling-solutions,"If you are in the data or analytics industry it is worth getting to know Ted Cuzzillo. He’s been following our industry since 2007 was covering Tableau as an industry analyst before anyone else and wrote for esteemed publications TDWI and Information Management.He knows the analytics landscape. And he’s always looking for what’s next.His new venture DataDoodle is explicit in where he has set his sights: he is exploring nascent trends and vendors in the area of smarter cities and data narrative.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


I had the opportunity to share what we’ve learned about data narrative (i.e. data storytelling) in the form of our own emerging solution Juicebox.As part of the launch of his new DataDoodle site Ted posted an article entitled Two recent “storytelling” tools for public audiences.The article begins by highlighting the value of data storytelling — particularly as it relates to municipalities but for all organizations. He profiles both Juicebox and our esprits apparentés Toucan Toco using a delightful cooking analogy. 








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


A few delicious morsels:Comparing presentation of data to serving a meal:The Juice Analytics’s product Juicebox is more like dinner at home with Blue Apron or other meal kit service.We’ve always taken a people-first approach to technology. It was encouraging to hear Ted see that focus in the product:Juicebox makes data consumption easy while it prods the data-shy into gradual self-confidence…Juicebox’s active approach grows analysts. A certain portion of its users will no doubt sprout legs such as when they see that cooking by number is just a short leap to cooking by touch smell and sizzle. They may have not quite as satisfying a dinner the first few times they try that but in the end they’re better cooks.Finally in comparison to Toucan Toco which has committed to a more centralized controlled approach Ted’s verdict is thatThe enterprise that wants to breed deep enterprise-wide intelligence will prefer Juicebox."
2021,4,25,Keys to Data Fluency: New Decision-Making Behaviors,https://www.juiceanalytics.com/writing/keys-to-data-fluency-data-into-decision-making,"How do you know that your organization is becoming more data fluent? You’ll see new behaviors such as the language people use the things they focus on and the way meetings are run. Data analysis and key performance measures are elevated from after-thought to starting-point. Here are five behaviors that you should start to expect from your team as your data fluency matures:Everyone Knows the Key Metrics — and Debates ThemData fluent organizations have a common understanding of how progress is measured. People at all levels have become familiar with these metrics and think about how their work relates to these numbers.At Juice we have a North Star Metric (the unchanging measure of progress) and three “key drivers” that are the current bottlenecks or leverage-points for improving the North Star Metric. We encourage discussion about how to best measure and what the results imply.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


Product Features or Investments Are Weighed Against Key MetricsData fluent organizations evaluate new investments by how they are likely to impact key performance metrics. At Juice our product roadmap is increasingly guided by our quantified understanding of user behaviors. We look for the most frequent blockers to success and consider new features as ways to knock down those walls. 








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            Investment ROI calculator http://labs.juiceanalytics.com/valuation/index.html
          
        
      
        
      

    
  


  


Anecdotes Get TestedWe talk about data storytelling all the time. However individual stories are often just a clue to a pattern — or simply a one-off outlier.Does it happen often? What is the implication? Why did it occur?At Juice we’ve discovered all sorts of user behaviors in Juicebox that we had not anticipated. For example our European users often load CSV (comma delimited files) that are delimited by semi-colons. We discovered this through one user’s story but then validated the frequency through data.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            via Saturday Morning Breakfast Cereal https://www.smbc-comics.com/index.php?db=comics&amp;id=2159
          
        
      
        
      

    
  


  


Important Processes Get Focus Through Data ProductsFor each of the priorities in the organization someone should create a data product that sheds light on the progress toward this goal. The data product may be a quarterly summary of results or a real-time dashboard of operations.Is there an important element of what your organization does that is not transparent and could benefit from an effective data product?








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


&nbsp;One-off Analyses Bloom EverywhereData fluent organizations aren’t satisfied with tracking data. People want to get to The Why. They are hungry for data sources that will help them explain customer behaviors operational issues and marketing performance. They want to put key metric results into context: What is the goal? How does that compare to industry benchmarks?At Juice our focus on key metrics has spawned dozens of analyses to understand what it takes to get a new user to succeed with Juicebox. 








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            Photo by keith davey on Unsplash
          
        
      
        
      

    
  


  





	Try Juicebox Free"
2021,4,21,Keys to Data Fluency: Matching Tools to User Needs,https://www.juiceanalytics.com/writing/keys-to-data-fluency-right-tools-for-the-jobs,"A data fluent organization should have a massive appetite for data. As you build your data fluency in front-line decision-makers and create a vibrant ecosystem the demand for data products will grow. And if there is one truism in analytics it is:Good analytics generates better questions.In what form do you answer the growing array of questions and needs? What data solutions or products do your data consumers needs? There are many choices:DashboardsReportsSelf-service BI toolsPredictive modelsOne-off analyses using slidesSpreadsheet modelsIt is a confusing array of ways to deliver data to these data consumers.What’s the right tool for the job?Of course there isn’t a single answer; it depends on the specific needs. Start by considering these two dimensions:How much flexibility and control does the data consumer need? Do they need to be able to dig deeply into the data or can results be shared with a static presentation of insights?How much will the raw data be enhanced with analysis modeling and pre-digested insights? For some audiences simply knowing the trend of key metrics is sufficient.Across these two axes it becomes clear there are a wide variety of different forms of data products.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


Take the marketing function as an example:Analytics Tools (upper left): A Marketing Analyst wants to explore the performance of different advertising campaigns to understand what creatives are working best.Data Storytelling (upper right): A Director of Marketing needs to present a data-driven plan for spending to convinced the executive team to allocate budget.Data Access (lower left): A Data Analyst needs to extract data from a 3rd-party platform to explore the behaviors of new users.Performance Reporting (lower right): The CEO wants an overview of marketing performance to share with the sales product development and the Board.With so many different needs and use cases it seems evident that there isn’t just one tool that can fill all these situations. My friend at GoodData Roman Stanek has been talking about ‘Data as a Service’ the transformation from traditional tightly-coupled data platforms to a new model:The data industry now has a unique opportunity. Cloud-based data infrastructure can allow every decision to be data-driven. And as both people and machines make decisions today this new infrastructure needs to support automated decision-making as well. We need to break down the monolithic nature of existing BI tools and we need to deliver Data as a Service to every device and person so that access to data becomes truly pervasive.He recognizes the diverse needs of data consumers that we see as organizations become more mature in their data fluency. A restrictive dashboard tool isn’t the right answer for telling a data story nor does it serve the data scientist who wants to spend less time extracting data and more time exploring the data.If you’ve made the commitment to becoming a data fluency organization you’re already thinking about how to better serve all the people who might be working with that data. Mapping the right tool to each specific job-to-be-done is an essential step.


	Try Juicebox to tell your data story"
2021,4,20,Data Storytelling Lessons: Be Relatable and Specific,https://www.juiceanalytics.com/writing/lessons-in-data-storytelling-relatable-and-specific,"Data &amp; RealityData is an abstract representation of reality. We take real things processes and actions and turn them into numbers.This is useful for analysis. However it creates a conceptual disconnect from the reality you are interested in explaining. Take these numbers for example:56000&nbsp;people.840000&nbsp;square miles.Those are the population and size of Greenland. Alone those numbers are a little hard to conceptualize. Are they big or small? How do they relate?The graphic below (by&nbsp;UCI) delivers far greater impact:








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


By making the data relatable (as above) or re-connecting it to reality with examples you have the opportunity to help your audience and deliver a more compelling data story.In the following interactive Juicebox learning app we share examples of making data&nbsp;relatable&nbsp;and&nbsp;specific. This is the special sauce that separates data storytelling from traditional dashboards and reports. Looking for more lessons on data storytelling? We’ve got you covered with our collection of data storytelling lessons.&nbsp;








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


&nbsp;&nbsp;


	Try Juicebox Free"
2021,4,19,Keys to Data Fluency: Believe in Your Front-Line Decision-Makers,https://www.juiceanalytics.com/writing/keys-to-data-fluency-believe-in-your-front-line-decision-makers,"In 2007 Professor Thomas Davenport wrote an influential book called Competing on Analytics: The New Science of Winning. At the time he stoked a smoldering ember into a flame by examining the power of analytics to improve organizations. The book was a catalyst for a generation of business leaders looking to find value in their data.For all its influence we had a quibble with Davenport promotion of a centralized model for analytics where the data is managed at an enterprise-level by a cadre of data scientists building complex models to drive decisions throughout the organization. He believed that the best organizational structure is:central analytics and data science organization based in a Strategy function with analysts assigned to and rotated among business units and functions: This is I think the optimal structure and home for analytics and data science. The central function allows for a critical mass of quants and for central coordination of their skill and career development. It should be a shared service where anyone with the money and the high-priority requirements can get the help they need.To this day the question of where analytics should happen is still unclear for many organizations. Research by Deloitte shows that many organizations are confused or conflicted:








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


We are advocates for bringing analytics to the front-line decision-makers of your organization. The marketers operators managers salespeople and customer service teams all need to understand data to be better at their jobs. For us management guru Peter Drucker sums it up best:Most discussions of decision making assume that only senior executives make decisions or that only senior executives' decisions matter. This is a dangerous mistake. Decisions are made at every level of the organization beginning with individual professional contributors and frontline supervisors. These apparently low-level decisions are extremely important in a knowledge-based organization.Senior leaders in your organization may make the so-called “big strategic” decisions in effect choosing the path to travel down. But the speed with which you travel toward your goal and stay on course when distractions arise—these decisions are controlled by your front-line staff. This belief that data can inform better decisions throughout an organization is part of our motivation for Juicebox.Data needs to be formed into targeted purposeful solutions to be of use to most people. The common practice of delivering a general-purpose analytical tool to end-users and expecting something useful to happen with it typically results in little added value. People are busy with their jobs. The last thing most information workers have time for is to learn how to use a new analysis tool figure out what data might be relevant to them and dive deep into a data analysis exercise. It is the difference between throwing someone an anchor and throwing them a lifeline.We have also made clear our belief in people over technology. There are many suitable technologies for capturing managing manipulating and presenting data. Better technology or tools is seldom the problem. Actually many of the data challenges that required large information technology investments a decade ago can be done quickly and economically today. 








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


The challenges are in the skills and collaboration of the people that use those technologies. Poor communication misalignment of values limited data communication skills unfocused messages…these are the challenges that most organizations we work with face today. The good news is that these are all solvable by focusing on the skills of your people.


	Try Juicebox Free"
2021,4,12,Keys to Data Fluency: Creating the Data Product Ecosystem,https://www.juiceanalytics.com/writing/keys-to-data-fluency-creating-the-data-product-ecosystem,"For data-driven thinking to flourish in your organization you need to give people easy access to ‘data products’ that will answer their pressing questions.Easier said than done.In fact for most organizations the collection of dashboards reports and analysis tools feels like a chaotic mess. When we worked for a global manufacturer a survey of information workers revealed that the top problem was an inability to find data products that served their needs. Also a big concern: the quality and usefulness of those data products.This is the challenge of creating a data product ecosystem. Creating a vibrant ecosystem for data products requires processes and tools. Processes set standards and ensure that the right priorities and qualities are built into every data product. Tools gather data visualize the results and distribute data products to users. Here are the six conditions (“the Six Ds” shown below) that are essential to building this ecosystem:








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


DemandWhat are the most important areas that would benefit from the insights and guidance of better data?There is nothing so useless as doing efficiently that which should not be done at all.—Peter DruckerWe begin with the end in mind. The consumers of data have needs. A healthy ecosystem will support those needs through the right data products. Discovering the information that will best serve the organization is the first step.Understanding data consumer demand is not a one-time endeavor. It requires a process of continually mapping the important decisions made by the organization and evaluating whether and how data can improve those decisions.One framework to use: Map the expressed needs of your data product consumers into a matrix to evaluate a) whether the data product will bring real value to the organization; b) whether a solution can truly drive better decision with data. This model will reveal data product concepts with the potential to deliver the greatest impact.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


DesignWhat processes and tools can help ensure the effective design of data products?Less than 30 percent of the potential users of organizations’ standard business intelligence (BI) tools use the technology today. This low take-up is due to the fact that longstanding tools and approaches to BI are often too difficult to use slow to respond or deliver content of limited relevance.—GartnerThe three reasons cited by Gartner for this problem are:1. Ease of use (“is hard to work with”)2. Performance (“users are frustrated by delays”)3. Relevance (“does not express content in line with their frame of reference”)The first and last reasons link directly to issues of poor data product design.In our role as dashboard and analytical application designers this is an area that is close to home. We see it all the time: reports and dashboards that lack focus and a message that targets their audience which is often undefined. We see poor choices in data visualization that distract from the important elements in the data and put the burden of deciphering meaning on the readers. We see data products that lack an obvious starting point and logical flow to conclusions.Poor design is wasteful. It results in solutions that users don’t want to use as noted by Gartner. It wastes the audience’s valuable time as it struggles to comprehend the data. And it wastes the development and distribution efforts necessary to deliver the data product.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            Juicebox delivers beautiful data presentations with good design decisions built-in.
          
        
      
        
      

    
  


  


DevelopWhat processes and tools support the efficient production of data products including gathering multiple data sources presenting this data providing user customization and delivering the information to data consumers?Ideally you want to have a small set of data tools that support the variety of types of data products your organization needs. A single solution is unlikely to offer the breadth of capabilities necessary. In our experience four to five tools for data presentation are usually sufficient for most organizations.There are many forms your data products may take. And for every form there are many technology options. However here are some common features that are worth evaluating in almost every case:End-user customization—Some presentations may target a single audience. This is the exception to the rule. Most often a data product goes out into the world alone and is used by many people each of whom comes from a unique perspective. Whether it is their department region or products all audience members will want to see data that is customized and scoped to their situation. Many interactive applications can support this ability to filter the relevant data.Sharing support—Data should spur conversation. However some solutions for data products create an isolating environment. The data product should make it easy to share discuss and capture insights— whether the discussion happens online offline on a desktop or on mobile devices.Quality visualization—It matters how data is visualized. Clean clear charts can make it easy for readers to quickly understand the data. The default settings for data visualizations should adhere to the fundamentals shared by well-known data visualization authors like Stephen Few and Edward Tufte.Fit workflows—Finally it is important that data products integrate into how people do their jobs. If the consumer of data is constantly on the run bombarded by information of all types an effective data product will deliver simple narrow content to this person. If the consumer wants to drill deeply into the data to understand underlying assumptions the functionality should exist to allow for this need.DiscoverHow can you help people discover the many data products in your organization and find the right information for their individual needs?Data product discovery should mirror the capabilities of online content subscription services. Podcasts blogs or Twitter all have established features for ensuring an audience can find and access the latest content. These include:1. Searching of metadata about the content including title author and description2. Browsing of content sorted into categories and ranked by popularity or ratings3. Surfacing of related content based on the consumer’s expressed areas of interest4. Subscribing to allow consumers to sign up to receive updates to content5. Automated pushing that allows consumers to receive updated content automatically rather than having to remember to return to the source6. User permissions to control who has access to applications and content








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            Browsing Spotify
          
        
      
        
      

    
  


  


DiscussWhat capabilities encourage data consumers to take the insights they find in the data and share these insights with others?The best data ecosystems don’t simply assume discussions will occur. They encourage discussions through mechanisms for sharing capturing and saving information and insights. The discoveries found in the data are treated as precious assets—after all they are the purpose of all the effort put into creating data products. Finally the ecosystem should encourage people to take action when the discussion is complete.Some organizations consider data products a one-way information broadcast. They implicitly assume that a dashboard is intended to deliver an information result not drive action.Discussions on data—like most of data fluency—are more a social and human problem than a technology problem. The technology approaches may be simple. For each data product create a document or folder for capturing insights. The document may simply be screenshots of the relevant part of the content along with an annotation explaining why it is interesting. As a historical artifact this document will reveal patterns of common issues and best practice approaches for responding to those issues. More important than a complex technology solution is an organizational culture that encourages dialogue and action after the insights are first found.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            Source: https://www.ontotext.com/knowledgehub/fundamentals/dikw-pyramid/
          
        
      
        
      

    
  


  


DistillHow do you filter out the irrelevant content and provide feedback to enhance those data products that remain?The scourge of data in most organizations is the ever-growing collection of reports that get generated month after month. New reports are created but seldom killed. The mass of data products quickly becomes difficult to navigate and the right information is hard to find. Even for the data that has found a suitable audience there is seldom a feedback loop. The direction and needs of the organization may change yet the content does not change to fit evolving demands.Data products should be living documents. They should improve over time or be removed if they are no longer relevant. It is a matter of survival of the fittest.Data fluent organizations recognize that too much content—particularly data content—will clog up the channels of communication. The data products must be distilled to the essential information. You want to clean out distractions and emphasize the most useful remaining parts.To filter and clean your data product ecosystem you need processes in place to gather feedback from your users. The feedback needs to impact how data products are designed and produced. There are at least three ways to continuously distill the best data products:Create a lightweight feedback mechanism like a simple “star” system.Track usage both on the volume of usage and the levels of engagement.Conduct content reviews by gathering the audience for a product and having a focus group-style discussion.Because knowledges are so specialized we need also a methodology a discipline a process to turn this potential into performance. Otherwise most of the available knowledge will not become productive; it will remain mere information. To make knowledge productive we will have to learn to connect.—Peter Drucker Pro-Capitalist SocietyIt is hard work to create an environment that enables the creation of high-quality data products ensures those products get into the right hands and has mechanisms for self-improvement. But without doing this work all the data investments your organization makes will struggle to reach the important decision-makers and consumers of that data.


	Try Juicebox"
2021,4,7,Data Discussion Lessons from Brad Pitt,https://www.juiceanalytics.com/writing/data-discussion-etiquette-from-brad-pitt,"Before Matt Damon impersonates an investigator in Ocean’s Eleven Brad Pitt’s character delivers a little pep talk.&nbsp;Watch this 40 second clip:Rusty Ryan (Brad Pitt) explains the rules of undercover conversation to Linus (Matt Damon). From: Ocean's Eleven (2001)Now imagine yourself giving a pep talk to the next email PowerPoint slide or dashboard that you are about to send out.&nbsp;Presumably your data is not meant to distort yet we can gather from this short scene some practical communication tips to improve data-informed discussions.Let’s break down the key moments.Be natural.







 

  
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  



[Damon takes an unnatural stiff stance] “No good. Don’t touch your tie. Look at me.”&nbsp;&nbsp;His first posture is fidgety and self-conscious with an overly professional stance.&nbsp;First impressions endure when it comes to perceived levels of interest and credibility. Most of us have an uncanny ability to sniff out a fake and how data enters the discussion is no exception. We’re not computers&nbsp;so we don’t enjoy an overwhelming data dump of facts findings and insights. Two paragraphs and 15 slides in everyone wonders “Where is this going? What’s the point?” Messages must be clear and focused and eliminate the unnatural mechanical chart headings and the unnecessarily complex statistical jargon.&nbsp;Be honest.







 

  
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  



“I ask you a question. You have to think of the answer. Where do you look? No good. You look down; they know you’re lying. And up; they know you don’t know the truth.”&nbsp;Be honest with what you do and do not know and what data you do and do not have. Your audience expects to have certain questions answered in order to take your information seriously. Your audience wants to both hear and understand answers to questions like these:How do I know I can trust this data? How was it collected and who was involved?How exactly is this metric calculated?I see the number is X but how do I know whether that is good or bad?What’s the history of this number and the frequency of its collection?How quickly does this number usually change?Why is this useful for me to know? How will it change what I care about?These questions aren’t novel. They follow the 5W’s basics. Yet they are often either left out or overcomplicated in most data discussions. The goal here is to acknowledge these needs in the simplest most useful way.Start with a (very) short story.







 

  
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  



“Don’t use 7 words when 4 will do.”&nbsp;&nbsp;&nbsp;With data as with words precision is as much an art as a science. Still helpful tools exist. Ann Gibson wrote a relevant post and I highly recommend reading the article for all the details but here’s the magical excerpt:


  
    &#147;Once upon a time there was a [main character] living in [this situation] who [had this problem]. [Some person] knows of this need and sends the [main character] out to [complete these steps]. They [do things] but it’s really hard because [insert challenges]. They overcome [list of challenges] and everyone lives happily ever after.&#148;
  
  

The beauty of this frame narrative is that it provides a structure for those who are too long-winded to focus on the essence of their own message and it helps others whose ideas tend to dart all over the place to preserve a sequential flow.Each of these [placeholders] are candidates for data context that help satisfy the previous ""Be Honest"" section. I mocked up a quick scenario that demonstrates a short story with useful data context:







 

  
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  



Set your mark.







 

  
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  



“Don’t shift your weight. Look always at your mark but don’t stare.”&nbsp;&nbsp;You’ve likely heard of S.M.A.R.T. goals before but are your charts smart? Something as simple as a target value by a specific date on a chart can work wonders at moving towards something tangible. People crave purpose&nbsp;so set and communicate your goals. But don’t be that presenter who stares incessantly at your metrics and goals.&nbsp;Be enjoyably useful.







 

  
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  



“Be specific but not memorable. Be funny but don’t make him laugh. He’s got to like you; then forget you the moment he’s left your sight.”&nbsp;&nbsp;“Jazz it up”&nbsp;“Make it shine”&nbsp;and “Make it pretty” are all phrases you’ve either heard or used yourself. Few situations are more disappointing than when a company tries to overcompensate with their insufficient irrelevant data by lathering on the “wow factor.” Don’t succumb to making your data memorable for the wrong reasons. For businesses the goal isn’t memorable chart-junk but that does not mean your data should be lifeless and shallow.Don’t leave people hanging.







 

  
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  



“And for God’s sake whatever you do don’t under any circumstances…”&nbsp;&nbsp;The worst move you can make is to omit the call to action. End with clear next steps key questions posed or an action button that allows your audience to engage with immediacy while your solid ideas are fresh and ripe for action.


	Try Juicebox Free"
2021,4,7,9 Habits of Data Fluent Organizations — and How to Learn Them,https://www.juiceanalytics.com/writing/10-habits-of-data-fluent-organizations,"With our book resources and workshops we’ve shared guidance about what it takes to become a data fluent organization. Most of all it starts with cultural habits that get people focused on using data in their decision-making. At Juice we are working everyday to create these habits and we wanted to share how we are building a data-first mindset and where we look for inspiration.Habit 1: Define shared metricsData fluency requires getting everyone on the same page as to what matters most. Matt Lerner in conjunction with Business of Software delivers online workshops that help you determine your “North Star Metric” and the set of key drivers that are bottlenecks to achieving that overall success. He also emphasizes how the key drivers will change over time as you improve.Habit 2: Create a shared vocabulary for your dataWhat is an “active user”? How do we track “first success” for a user? These are terms that need to be carefully defined and documented so we can move on to how we are going to improve them.Val Logan of The Data Lodge is one of the premier thinkers on how organizations can build shared skills in using data. Check out her podcast: Speaking Data: Information as a Second LanguageThe Data LodgeHabit 3: Evaluate the strengths and weaknesses of data sourcesWhen we measure user activities for Juicebox we have multiple 3rd-party tools and internal data sources to choose from. We needed to determine what sources are more reliably accurate and what is the trade-off for convenience. If you are going to lean on data you want to understand its quality. Here’s an overview article from Neil Patel about assessing data quality.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


Habit 4: Ensure transparency into how data is manipulated modeled and presentedYou need to build alignment to avoid constantly reverting to discussions about the quality or meaning of your data. The discussion needs to move on to: What are we going to do to improve?Alberto Cairo is a preeminent advocate for truth in presentation of data his book How Charts Lie is a must-read on this topic.Another thought leader in this area is Alan Schwarz a journalist who has consistently used data to uncover hard truths. 
Habit 5: Use metrics as the starting point for everyday discussionsGetting focused on a few key metrics has started to transform how we work. We start meetings by reviewing how we are performing then focus on what activities are going to move those metrics. For difficult choices we have shared baseline: How will it impact our North Star Metric?Fortunately we have a tool in Juicebox that fluidly integrates data visualization with the ability to explain context priorities and next steps. It acts like a dashboard combined with a project management tool.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


Habit 6: Established guidelines to create purposeful data productsA data-eager organization will spawn plenty of reports dashboards and data presentation in an effort to communicate and explore. A data fluent organization will be purposeful about why each of those data products exists. You want to start with a clear understanding of the problem you want to solve and who is going to use the information.We created a short Data Product Checklist to help evaluate if your solution is ready to share.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


Habit 7: Develop a feedback mechanism for data products to evolve and improveYour report or dashboard is a product. Like any other product it needs to show value — even if you are only asking for your users’ attention. To fully discover that value you are going to need to talk to users improve and iterate on the data and how you show it.There are many ways to think about product development. We’ve found the product-led growth framework a useful set of guidelines.
Habit 8: Celebrate examples of quality data productsWhen you create data products that have made a difference make a big deal out of it. These winning examples will give other groups in your organization a benchmark to pursue.Here’s our 20 best data storytelling examples that show how to change minds with data.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


Habit 9: Leadership promotes a data-driven cultureData fluency needs to emanate from the top because people emulate the behaviors of their leaders.Few people have been studying and advocating for data-driven cultures longer than Tom Davenport the Surgeon General of Analytics in the Enterprise.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  





	Learn more about Juicebox

"
2020,8,14,Calculating variance in the log domain,https://justindomke.wordpress.com/2020/08/14/calculating-variance-in-the-log-domain/,Say you&#8217;ve got a positive dataset and you want to calculate the variance. However the numbers in your dataset are huge so huge you need to represent them in the log-domain. How do you compute the log-variance without things blowing up? I ran into the problem today. To my surprise I couldn&#8217;t find a standard &#8230; Continue reading Calculating variance in the log&#160;domain &#8594;
2020,5,18,Back to Basics: Approximate Bayesian Computation,https://justindomke.wordpress.com/2020/05/18/back-to-basics-approximate-bayesian-computation/,(All based on these excellent slides from Umberto Picchini) &#8220;Approximate Bayesian Computation&#8221; sounds like a broad class of methods that would potentially include things like message passing variational methods MCMC etc. However for historical reasons the term is used for a very specialized class of methods. The core idea is as follows: Sample from the &#8230; Continue reading Back to Basics: Approximate Bayesian&#160;Computation &#8594;
2019,10,16,Three opinions on theorems,https://justindomke.wordpress.com/2019/10/16/three-opinions-on-theorems/,1. Think of theorem statements like an API. Some people feel intimidated by the prospect of putting a &#8220;theorem&#8221; into their papers. They feel that their results aren&#8217;t &#8220;deep&#8221; enough to justify this. Instead they give the derivation and result inline as part of the normal text. Sometimes that&#8217;s best. However the purpose of a &#8230; Continue reading Three opinions on&#160;theorems &#8594;
2019,10,4,Exponential Families Cheatsheet,https://justindomke.wordpress.com/2019/10/04/exponential-families-cheatsheet/,As part of the graphical models course I taught last spring I developed a &#8220;cheatsheet&#8221; for exponential families. The basic purpose is to explain the standard moment-matching condition of maximum likelihood. The goal of the sheet was to clearly show how this property generalized to maximum likelihood in conditional exponential families with hidden variables or &#8230; Continue reading Exponential Families Cheatsheet &#8594;
2018,6,12,Statistics – the rules of the game,https://justindomke.wordpress.com/2018/06/12/statistics-the-rules-of-the-game/,What is statistics about really? It&#8217;s easy to go through a class and get the impression that it&#8217;s about manipulating intimidating formulas. But what&#8217;s the goal of them? Why did people invent them? If you zoom out the big picture is more conceptual than mathematical. Statistics has a crazy grasping ambition: it wants to tell &#8230; Continue reading Statistics – the rules of the&#160;game &#8594;
2017,11,16,A Divergence Bound For Hybrids of MCMC and Variational Inference and …,https://justindomke.wordpress.com/2017/11/16/a-divergence-bound-for-hybrids-of-mcmc-and-variational-inference-and/,At ICML I recently published a paper that I somehow decided to title &#8220;A Divergence Bound for Hybrids of MCMC and Variational Inference and an Application to Langevin Dynamics and SGVI&#8221;. This paper gives one framework for building &#8220;hybrid&#8221; algorithms between Markov chain Monte Carlo (MCMC) and Variational inference (VI) algorithms. Then it gives an &#8230; Continue reading A Divergence Bound For Hybrids of MCMC and Variational Inference and&#160;&#8230; &#8594;
2017,5,24,The second and third best features of LyX you aren’t using.,https://justindomke.wordpress.com/2017/05/24/the-second-and-third-best-features-of-lyx-you-arent-using/,LyX is a WYSIWYG editor for latex files. It&#8217;s a little bit clunky to use at first and isn&#8217;t perfect (thank you open source developers&#8211; I&#8217;m not ungrateful!) but after becoming familiar with it it&#8217;s probably the single piece of software that has most improved my productivity. I like it so much I use it &#8230; Continue reading The second and third best features of LyX you aren&#8217;t&#160;using. &#8594;
2017,4,22,You deserve better than two-sided finite differences,https://justindomke.wordpress.com/2017/04/22/you-deserve-better-than-two-sided-finite-differences/,In calc 101 the derivative is derived as . So if you want to estimate a derivative an easy way to do so would be to just pick some small and estimate: This can work OK. Let&#8217;s look at an example of trying to calculate the derivative of  using a range of different What&#8217;s &#8230; Continue reading You deserve better than two-sided finite&#160;differences &#8594;
2016,12,6,Sneaking up on Bayesian Inference (A fable in four acts),https://justindomke.wordpress.com/2016/12/06/sneaking-up-on-bayesian-inference-a-fable-in-four-acts/,Act 1: Magical Monkeys Two monkeys Alfred () and Betty () live in a parallel universe with two kinds of blocks green () and yellow (). Alfred likes green blocks and Betty prefers the yellow blocks. One day a Wizard decides to give one of the monkeys the magical power to send one block over &#8230; Continue reading Sneaking up on Bayesian Inference (A fable in four&#160;acts) &#8594;
2015,9,14,Algorithmic Dimensions,https://justindomke.wordpress.com/2015/09/14/algorithmic-dimensions/,There are many dimensions on which we might compare a machine learning or data mining algorithm. A few of the first that come to mind are: 1) Sample complexity convergence How much predictive power is the algorithm able to extract from a given number of examples? All else being equal if algorithm A with N &#8230; Continue reading Algorithmic Dimensions &#8594;
2019,9,25,Registration is open - Deep Learning Autumn School at Bar Ilan University,https://bickson.blogspot.com/2019/09/registration-is-open-deep-learning.html,My friend Ely Porat sent me the following registration notice for the deep learning and AR/VR Autumn courses:https://sites.google.com/datalab.cs.biu.ac.il/biusummerschool/home?authuser=0Registration is pretty low and you can get academic credit from Bar Ilan University.
2019,7,14,Israeli AI Week - call for talks ending Aug 1st,https://bickson.blogspot.com/2019/07/israeli-ai-week-call-for-talks-ending.html,My friend Assaf Araki from Intel invited me to give a hand organizing the Israeli AI Week. We are looking for talks from innovative companies and machine learning researchers.The&nbsp;AI-Week&nbsp;in Tel Aviv is a nationwide community event organized by representatives from the Academia Government Industry and NGOs.&nbsp;&nbsp;AI&nbsp;week&nbsp;will bring together ~2000&nbsp;AI&nbsp;researchers and practitioners from Israel and around the globe. The event will take place in Tel Aviv university campus during November 17th&nbsp;to November 20th&nbsp;and will include one day of hands on workshops two conference days with +100 speakers a two days of data hackathon and multiple additional events.&nbsp;The goals of&nbsp;AI&nbsp;Week&nbsp;are to increase the synergy among the local&nbsp;AI&nbsp;eco system and with leading global innovators while exposing Israel&nbsp;AI&nbsp;innovation to the world.&nbsp; This event will include cutting edge sessions delivered by leading researchers and industry innovators from Israel and around the world.&nbsp;The Israel&nbsp;AI&nbsp;week&nbsp;founding members include Tel Aviv University Startup Nation Central Israel Innovation Authority &amp; Intel. We are adding more and more partners into this community effort.Among our steering committee members are &nbsp;Prof. Amnon Shashua&nbsp;(SVP Intel President and CEO of Mobileye &amp; Professor at the Hebrew University)&nbsp;Major Gen. (Res.) Professor Isaac Ben-Israel(Chairman of Israel Space Agency Chairman of Israel National Council for R&amp;D &amp; Professor at Tel-Aviv University)&nbsp;Prof. Eugene Kandel&nbsp;(CEO of Start-Up Nation Central &amp; Professor at the Hebrew University) and&nbsp;Aharon Aharon&nbsp;(CEO of Israel Innovation Authority).
2019,5,23,Allen Institue Opens an Israeli Branch,https://bickson.blogspot.com/2019/05/allen-institue-opens-israeli-branch.html,This Tuesday Prof. Oren Etsioni announced on his keynote talk at the Data Science Summit that the Seattle Allen Institute opens an Israeli Branch:Prof. Yoav Goldberg from Bar Ilan is heading the research effort with focus on NLP.
2019,3,7,Upcoming data science events in Israel,https://bickson.blogspot.com/2019/03/upcoming-data-science-events-in-israel.html,My friend Assaf Araki is organizing the AI Week&nbsp;Nov 17-21 at Tel Aviv University. It is going to be the first national AI event with around 2000 participants. Dr. Ben Lorica chief scientist of O'Reilly will be one of the speakers along with Prof. Amnon Shashua CEO of MobileEye.Another interesting event is the AI Data Science Summit - May 21 in Jerusalem.&nbsp; It is a follow up event organized by Avner Algom of our European Data Science Summit. Prof. Oren Etzioni from the Allen Institute for AI is one of the keynote speakers.
2019,2,19,Hacking Deep Learning (Bar Ilan) Workshop Videos,https://bickson.blogspot.com/2019/02/hacking-deep-learning-bar-ilan-workshop.html,Hacking Deep Learning (Bar Ilan) Workshop Videos now online. Thanks for my friend Prof. Yossi Keshet for organizing and inviting me!One notable talk which is unfortunately missing from the videos is of Prof. Adi Shamir described in this paper. The work analysis how many pixels one should change to confuse a deep learning based classifier. The result is surprising - only a few! A related describe work is this.
2019,2,17,UAI 2019 is coming to Tel Aviv,https://bickson.blogspot.com/2019/02/uai-2019-is-coming-to-tel-aviv.html,I got this from my friend Nick:This year Tel Aviv will host&nbsp;UAI&nbsp;http://auai.org/uai2019/&nbsp;(July 22-25). Several students have been able to attend the conference and present their work thanks to the generosity&nbsp;of your institutions. We hope that you will continue to support us this year as well. You can find more information about sponsorship packages here&nbsp;http://auai.org/uai2019/sponsorships.php&nbsp;.RegardsNikolaos Vasiloglou&nbsp;UAI&nbsp;2019 Sponsorship ChairFeel free to reach out to Nick in case you are interested in sponsoring the event.
2019,1,10,Alibaba acquires Data Artisans?,https://bickson.blogspot.com/2019/01/alibaba-acquires-data-artisans.html,Data Artisans is the company behind Apache Flink - the European answer to Apache Spark.According to this news article&nbsp;Alibaba acquires Data Artisans.I wrote back in 2014 on Apache Flink project.
2017,12,9,Apple shares Turi Create open source framework,https://bickson.blogspot.com/2017/12/apple-shares-turi-create-open-source.html,It is very exciting that after many years of hard work we have finally released our machine learning framework as open source! The announcement made yesterday at NIPS by Prof. Carlos Guestrin:And here is our github link: https://github.com/apple/turicreate
2017,9,8,Prof. Joseph Keshet from BIU fools deep learning,https://bickson.blogspot.com/2017/09/prof-joseph-keshet-from-biu-fools-deep.html,My friend Joseph (Yossi) Keshet have recently released work for fooling deep learning systems. His work got a lot of attendion including MIT Technology Review&nbsp;and the New Scientist. Nice work!!
2017,9,8,Dataiku raised 28M$,https://bickson.blogspot.com/2017/09/dataiku-raised-28m.html,According to VentureBeat Dataiku just raised 28M$. Dataiku has a web based platform for data science.Here is my personal connection. Strangely last time I wed a couple I was wearing their t-shirt.Unrelated I just learned from my colleague Brian that Cloudera just acquired Fast Forward Labs which is the company behind Hilary Mason. I visited Hilary in her offices a couple of years ago and learned they had an interesting consulting models of sharing periodical tech reports for educating data scientists to become more proficient. Congrats Hilary!
2017,9,5,Deepgram - Audio Search with Deep Learning,https://bickson.blogspot.com/2017/09/deepgram-audio-search-with-deep-learning.html,A very interesting podcast by Sam Charrington who is interviewing Scott Stephenson from DeepGram. DeepGram is using deep learning activations for creating indexes that allows to search text in voice recordings.DeepGram have released Kur which is a high level abstraction of deep learning framework to allow quickly defining network layouts. But still writing the target persona is researchers with deep learning knowledge.A related Israeli startup is AudioBurst. &nbsp;They claim to use AI for indexing but it is not clear what they actually do. Another Israeli startup is Verbit. They seem to transcribe audio with humans going over the preliminary result.In addition my friend Yishay Carmiel is working on importing parts of Kaldi to TensorFLow. A recent Google developer blog post describes this effort. Yishay is leading a spinoff of Spoken called IntelligentWire who is also searching audio files using deep learning.Overall it seems that search in audio files using deep learning is getting hotter!
2017,7,27,Some misc news,https://bickson.blogspot.com/2017/07/some-misc-news.html,I just learned my postdoc roommate Yisong Yue from Caltech released a new interesting paper: Factorized Variational Autoencoders for Modeling Audience Reactions to Movies: a joint work with Disney Research published @ CVPR 2017.Another interesting paper: Accelerating Innovation Through Analogy Mining just received the best paper award at KDD 2017. The paper is by Dafna Shahaf who studied with me at CMU and her student Tom Hope.An earlier related work of Dafna is a paper for identifying humor in cartoon captions.Misha Bilenko formerly from M$ released an source for gradient boosting. It seems to compete with XGBoost with the claim that it supports categorical variables as well. (In GraphLab Create we had an extended XGBoost with categorical variable support).
2017,2,13,Data Science Summit Europe 2017,https://bickson.blogspot.com/2017/02/data-science-summit-europe-2017.html,The 3rd Data Science Summit Europe is coming! This year I am not involved in the organization she it should probably be a better event :-) Save the date - May 29 2017 in Jerusalem. The date was picked just after O'Reilly Strata London 2017 thus the conference will attract many speakers and guests from abroad.The keynote speaker is my friend Dr. Ben Lorica chief scientist of O'Reilly Media and the content organizer for O'Reilly Strata and O'Reilly AI conferences.Hope to see you there!
2017,1,27,Deep learning for cancer research,https://bickson.blogspot.com/2017/01/deep-learning-for-cancer-research.html,A recent interesting news from Stanford regarding identification of skin cancer using deep leaning for images.A different project featured by NVIDIA is using deep learning for breast cancer research where they claim that the error went down 85%.Unrelated I heard today about Grail who raised 100M$ for cancer detection&nbsp;in blood tests. Grail raised money from Amazon Google and Microsoft (Bill Gates). &nbsp;Looking at their career page they are also looking for deep learning researchers.Another interesting company is Zebra Medical Research which shares medical data with researchers in return for a fraction of future revenues.Following this blog post publication my friend Assaf Araki from Intel sent me a reminder for Intel's cloud cancer research initiative. Broad MIT institute joined last year.
2017,1,25,Amazing deep learning visualization,https://bickson.blogspot.com/2017/01/amazing-deep-learning-visualization.html,I found this amazing deep learning visualization:&nbsp;http://playground.tensorflow.org/The tool is written by Daniel Smilkov and Shan Carter from Google Brain's team.It is a great tool to understand using small examples the network operation.The tool promised:It took me 5 minutes to find a configuration which breaks it ! :-)Here it is:Note that both Test loss and training loss are NaN. I am waiting for the fix. (I see this error was already reported)
2017,1,19,TensorFlow to support Keras API,https://bickson.blogspot.com/2017/01/tensorflow-to-support-keras-api.html,I found this interesting blog post&nbsp;by Rachel Thomas. My favorite quote:Using TensorFlow makes me feel like I’m not smart enough to use TensorFlow; whereas using Keras makes me feel like neural networks are easier than I realized.&nbsp;This is because TensorFlow’s API is verbose and confusing and because Keras has the most thoughtfully designed expressive API I’ve ever experienced. I was too embarrassed to publicly criticize TensorFlow after my first few frustrating interactions with it. It felt so clunky and unnatural but surely this was my failing. However Keras and Theano confirm my suspicions that tensors and neural networks don’t have to be so painful.&nbsp;
2017,1,19,Pipeline.io - production environment to serve TensorFlow models,https://bickson.blogspot.com/2017/01/pipelineio-production-environment-to.html,"I recently stumbled upon pipeline.io - an open source production environment to serve TensorFlow deep learning models. By looking into Giuhub activity plots&nbsp;I see the Chris Fregly&nbsp;is the main force behind it. Pipeline.io is trying to solve the major headache around scoring and maintaining ML models in production.Here us their general architecture diagram:Here is a talk by Chris:&nbsp;Alternative related systems are seldon.io prediction.io (sold to SalesForce) sense.io (sold to Cloudera) Domino Data Labs and probably some others I forgot :-)BTW Chris will be giving a talk at AI by the bay conference (March 6-8 in San Francisco). The conference looks pretty interesting.&nbsp;And here is a note I got from Chris following my initial blog post:Thanks for the mention Danny! Love your work.Here's an updated video:Here's the jupyter notebook that powers the entire demo:&nbsp;I asked Chris which streaming applications he has in mind and this is what I got:We've got a number of streaming-related Github issues (features) in the works:  here are the some relevant projects that are in the works:  - working with the Subscriber-Growth Team @ Netflix to replace their existing multi-armed bandit Spark-Streaming-based data pipeline to select the best model to increase signups. we're using Kafka + Kafka Streams + Spark + Cassandra (they love Cassandra!) + Jupyter/Zeppelin Notebooks in both Python/Scala.  - working with the Platform Team @ Twilio to quickly detect application logs that potentially violate Privacy Policies. this is already an issue outside the US but quickly becoming an issue here in the US. we're using Kafka + custom Kafka Input Readers for Tensorflow + Tensorflow to train the models (batch) and score every log line (real-time).  - working with a super-large Oil &amp; Gas company out of Houston/Oslo (stupid NDA's) to continuously train deploy and compare scikit-learn and Spark ML models on live data in parallel - all from a Jupyter notebook.   - working with PagerDuty to predict potential outages based on their new ""Event"" stream which includes code deploys configuration changes etc. we're using Kafka + the new Spark 2.0 Structure Streaming. &nbsp;What are the main benefits of piepline.io vs. other systems?  - the overall goal as you can probably figure out is to give data scientists the ""freedom and responsibility"" (hello Netflix Culture Deck!) to iterate quickly without depending on production engineers or an ops group.  - this is a life style that i really embraced while at Netflix. with proper tooling anyone (devs data scientists etc) should be able to deploy scale and rollback their own code or model artifacts.  - we're providing the platform for this ML/AI-focused freedom and responsibility!  - you pointed out a few of our key competitors/cooperators like seldon.io. i have a list of about 20 more that i keep an eye on each and every day. i'm in close talks with all of them.   - we're looking to partner with guys like Domino Data Labs who have a weak deployment story.   - and we're constantly sharing experience and code with seldon.io and hydrosphere.io and others.  - we're super performance-focused as well. we have a couple efforts going on including PMML optimization native code generation etc.  - also super-focused on metrics and monitoring - including production-deployment dashboards targeted to data scientists.  - i feel like our main competitors are actually the cloud providers. they're the ones that keep me awake. one of our underlying themes is to reverse engineer Google and AWS's Cloud ML APIs.&nbsp;&nbsp;"
2017,1,17,CryptoNets: scoring deep learning on encrypted data,https://bickson.blogspot.com/2017/01/cryptonets-scoring-deep-learning-on.html,Last week I attended &nbsp;an interesting lecture by Ran Gilad Bachrach from MSR. Ran presented CryptoNets who was reported in ICML 2016. CryptoNets allows to score trained deep learning models on encrypted data. They use homomorphic encryption&nbsp;a well known mechanism which allows computing encrypted products and sums. So the main trick is to limit the neural net operations to include only sums and products. To overcome this problem CryptoNet is using the square function as the only non-linear operation supported (vs. sigmoids ReLU etc.)On the up side CryptoNets reports 99% accuracy on MNIST data which is the toy example everyone is using for deep learning. On the downside you can not train a network but just score on new test data. Scoring is quite slow - around 5 minutes although you can batch up to a few thousands scoring operations together at the same batch. Due to increasing complexity of the represented numbers the technique is also limited to a certain number of network layers.I believe that in the coming few years additional research effort will be invested for trying to tackle the training of neural networks on private data without revealing the data contents.Anyone who is interested in reading about other primitives who may be used for performing similar computation is welcome to take a look at my paper: D. Bickson D. Dolev G. Bezman and B. Pinkas  Secure Multi-party Peer-to-Peer Numerical Computation. Proceedings of the 8th IEEE Peer-to-Peer Computing (P2P'08) Sept. 2008 Aachen Germany - where we use both homomorphic encryption but also Shamir Secret Sharing to compute a similar distributed computation (in terms of sums and products).
2016,12,15,Neural networks for graphs,https://bickson.blogspot.com/2016/12/neural-networks-for-graphs.html,I met at Thomas Kipf&nbsp;from University of Amsterdam at NIPS 2016 and he pointed out some interesting blog post he wrote regarding neural networks for graph analytics.
2016,6,25,A new machine learning podcast,https://bickson.blogspot.com/2016/06/a-new-machine-learning-podcast.html,A new machine earning podcast by Sam Charrington. It tracks down the weekly machine learning news and summarizes the major events. Highly recommended!
2016,6,23,4th Large Scale Recommender Systems workshop - deadline extended,https://bickson.blogspot.com/2016/06/large-scale-recommender-systems.html,We have extended the deadline of our Large Scale Recommender Systems workshop to June 30. This is the 4th year we are organizing this workshop as part of ACM Recsys 2016. Anyone with novel work in the area of applied recommender systems is welcome to submit a talk proposal.
2016,6,19,GraphLab Create healthcare use case,https://bickson.blogspot.com/2016/06/graphlab-create-healthcare-use-case.html,A nice blog post from Mark Pinches our Manchester evangelist who is working with John Snow &nbsp;Labs. It shows how to use GraphLab Create for slicing dicing and aggregations of healthcare data.
2016,6,19,"Novomatic - comparing spark, pandas and Dato",https://bickson.blogspot.com/2016/06/novomatic-comparing-spark-pandas-and.html,Interesting slides from Bogdan Pirvu head data science @&nbsp;Novomatic (Austrian Gaming Industries). I met Bogdan at RecSys Vienna last fall and he got interested in GraphLab Create. In his talk Bogdan compares pandas Spark and Graphlab Create. Guess who is the winner?
2016,6,16,Prof. Alex Smola moves to Amazon,https://bickson.blogspot.com/2016/06/prof-alex-smola-moving-to-amazon.html,Here is the note he wrote on his blog. Alex will be heading Amazon Cloud ML effort. Definitely a great win for Amazon!If you like to hear Alex speaking about his recent research you should attend our Data Science Summit July 12-13 in SF. You are welcome to use discount code DSSfriend.
2016,6,14,RSA fraud in social media report,https://bickson.blogspot.com/2016/06/rsa-fraud-in-social-media-report.html,An interesting report from RSA is found here. A lot of cool visualizations of social communities of fraudsters.
2016,12,27,"Ethics, logistic regression, and 0-1 loss",https://lingpipe-blog.com/2016/12/27/ethics-logistic-regression-and-0-1-loss/,Andrew Gelman and David Madigan wrote a paper on why 0-1 loss is so problematic: Gelman and Madigan. 2015. How is Ethics Like Logistic Regression? Chance 28(12). This is related to the issue of whether one should be training on an artificial gold standard. Suppose we have a bunch of annotators and we don&#8217;t have [&#8230;]
2014,10,29,Becky’s and My Annotation Paper in TACL,https://lingpipe-blog.com/2014/10/29/beckys-and-my-annotation-paper-in-tacl/,Finally something published after all these years working on the problem: Rebecca J. Passonneau and Bob Carpenter. 2014. The Benefits of a Model of Annotation. Transactions of the Association for Comptuational Linguistics (TACL) 2(Oct):311−326. [pdf] (Presented at EMNLP 2014.) Becky just presented it at EMNLP this week. I love the TACL concept and lobbied Michael Collins [&#8230;]
2014,4,10,Document Classification with Lucene,https://lingpipe-blog.com/2014/04/10/lucene4-document-classification/,As promised in my last post this post shows you how to use Lucene&#8217;s ranked search results and document store to build a simple classifier. Most of this post is excerpted from Text Processing in Java Chapter 7 Text Search with Lucene. The data and source code for this example are contained in the source [&#8230;]
2014,3,8,Lucene 4 Essentials for Text Search and Indexing,https://lingpipe-blog.com/2014/03/08/lucene-4-essentials-for-text-search-and-indexing/,Here&#8217;s a short-ish introduction to the Lucene search engine which shows you how to use the current API to develop search over a collection of texts. Most of this post is excerpted from Text Processing in Java Chapter 7 Text Search with Lucene. Lucene Overview Apache Lucene is a search library written in Java. It’s [&#8230;]
2014,1,31,New Book: Text Processing in Java,https://lingpipe-blog.com/2014/01/31/text-processing-in-java/,I&#8217;m pleased to announce the publication of Text Processing in Java ! This book teaches you how to master the subtle art of multilingual text processing and prevent text data corruption.  Data corruption is the all-too-common problem of words that are garbled into strings of question marks black diamonds or random glyphs.  In Japanese this [&#8230;]
2014,1,11,Canceled: Help Build a Watson Clone–Participants Sought for LingPipe Code Camp,https://lingpipe-blog.com/2014/01/10/help-build-a-watson-clone-participants-sought-for-lingpipe-code-camp/,Code Camp is canceled. We are late on delivering a LingPipe recipes book to our publisher and that will have to be our project for March. But we could have testers/reviewers come and hang out. Less fun I think. Apologies. Breck &#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212; &#160; Dates: To be absolutely clear: Dates are 3/2/2014 to 3/31/14 in Driggs [&#8230;]
2013,12,19,Limits of Floating Point and the Asymmetry of 0 and 1,https://lingpipe-blog.com/2013/12/19/limits-of-floating-point-and-the-asymmetry-of-0-and-1/,If we are representing probabilities we are interested in numbers between 0 and 1. It turns out these have very different properties in floating-point arithmetic. And it&#8217;s not as easy to solve as just working on the log scale. Smallest Gaps between Floating Point Numbers of Different Magnitudes The difference between 0 and the next [&#8230;]
2013,9,25,Fokkens et al. on Replication Failure,https://lingpipe-blog.com/2013/09/25/fokkens-et-al-on-reproduction-replication-failure/,After ACL Becky Passonneau said I should read this: Antske Fokkens Marieke van Erp Marten Postma Ted Pedersen Piek Vossen Nuno Freire. 2013. Offspring from Reproduction Problems: What Replication Failure Teaches Us. ACL Proceedings. You should too. Or if you&#8217;d rather watch there&#8217;s a video of their ACL presentation. The Gist The gist of the [&#8230;]
2013,9,9,LingPipe Incubator Welcomes Seer,https://lingpipe-blog.com/2013/09/09/lingpipe-incubator-welcomes-getseer/,We have started an incubator program for start-ups with natural language processing (NLP) needs. Seer is our first egg. The company creates productivity apps based on unstructured data sources&#8211;that is where LingPIpe fits in. The guys (Conall and Joe) fit a pattern that we see quite often&#8211;smart people with a good idea that presupposes NLP [&#8230;]
2013,8,21,Token Lumpers and Splitters: Spider-Man vs. Superman,https://lingpipe-blog.com/2013/08/21/tokenization-lumpers-splitters-superman-spider-man/,One of the perennial problems in tokenization for search classification or clustering of natural language data is which words and phrases are spelled as compounds and which are separate words. For instance consider &#8220;dodgeball&#8221; (right) vs. &#8220;dodge ball&#8221; (wrong) and &#8220;golfball&#8221; (wrong) vs. &#8220;golf ball&#8221; (right)? It&#8217;s a classic lumpers vs. splitters problem. Alas the [&#8230;]
2021,6,22,What you need to know before you get started: A brief tour of Calculus Pre-Requisites,https://machinelearningmastery.com/what-you-need-to-know-before-you-get-started-a-brief-tour-of-calculus-pre-requisites/,"We have previously seen that calculus is one of the core mathematical concepts in machine learning that permits us to [&#8230;]
The post What you need to know before you get started: A brief tour of Calculus Pre-Requisites appeared first on Machine Learning Mastery."
2021,6,21,Calculus in Machine Learning: Why it Works,https://machinelearningmastery.com/calculus-in-machine-learning-why-it-works/,"Last Updated on June 21 2021 Calculus is one of the core mathematical concepts in machine learning that permits us [&#8230;]
The post Calculus in Machine Learning: Why it Works appeared first on Machine Learning Mastery."
2021,6,18,Key Concepts in Calculus: Rate of Change,https://machinelearningmastery.com/key-concepts-in-calculus-rate-of-change/,"Last Updated on June 19 2021 The measurement of the rate of change is an integral concept in differential calculus [&#8230;]
The post Key Concepts in Calculus: Rate of Change appeared first on Machine Learning Mastery."
2021,6,17,What is Calculus?,https://machinelearningmastery.com/what-is-calculus/,"Last Updated on June 19 2021 Calculus is the mathematical study of change.&#160; The effectiveness of calculus to solve a [&#8230;]
The post What is Calculus? appeared first on Machine Learning Mastery."
2021,6,15,Differential Evolution from Scratch in Python,https://machinelearningmastery.com/differential-evolution-from-scratch-in-python/,"Last Updated on June 19 2021 Differential evolution is a heuristic approach for the global optimisation of nonlinear and non- [&#8230;]
The post Differential Evolution from Scratch in Python appeared first on Machine Learning Mastery."
2021,6,13,Modeling Pipeline Optimization With scikit-learn,https://machinelearningmastery.com/modeling-pipeline-optimization-with-scikit-learn/,"Last Updated on June 19 2021 This tutorial presents two essential concepts in data science and automated learning. One is [&#8230;]
The post Modeling Pipeline Optimization With scikit-learn appeared first on Machine Learning Mastery."
2021,6,10,Gradient Descent With AdaGrad From Scratch,https://machinelearningmastery.com/gradient-descent-with-adagrad-from-scratch/,"Gradient descent is an optimization algorithm that follows the negative gradient of an objective function in order to locate the [&#8230;]
The post Gradient Descent With AdaGrad From Scratch appeared first on Machine Learning Mastery."
2021,6,8,Gradient Descent Optimization With AMSGrad From Scratch,https://machinelearningmastery.com/gradient-descent-optimization-with-amsgrad-from-scratch/,"Gradient descent is an optimization algorithm that follows the negative gradient of an objective function in order to locate the [&#8230;]
The post Gradient Descent Optimization With AMSGrad From Scratch appeared first on Machine Learning Mastery."
2021,6,6,Gradient Descent Optimization With AdaMax From Scratch,https://machinelearningmastery.com/gradient-descent-optimization-with-adamax-from-scratch/,"Gradient descent is an optimization algorithm that follows the negative gradient of an objective function in order to locate the [&#8230;]
The post Gradient Descent Optimization With AdaMax From Scratch appeared first on Machine Learning Mastery."
2021,6,3,A Gentle Introduction to Premature Convergence,https://machinelearningmastery.com/premature-convergence/,"Convergence refers to the limit of a process and can be a useful analytical tool when evaluating the expected performance [&#8230;]
The post A Gentle Introduction to Premature Convergence appeared first on Machine Learning Mastery."
2020,12,5,Distributionally Robust Contextual Bandit Learning,http://www.machinedlearnings.com/2020/12/distributionally-robust-contextual.html,This blog post is about improved off-policy contextual bandit learning via distributional robustness.  I'll provide some theoretical background and also outline the implementation in vowpal wabbit. Some of this material is in a NeurIPS expo talk video and additional material is in the accepted paper.  Motivation In off-policy learning in contextual bandits our goal is to produce the best policy possible from historical data and we have no control over the historical logging policy which generated the data.  (Note production systems that run in closed-loop configurations nonetheless are in practice doing off-policy learning because of delays between inference training and model update.)  Off-policy learning reduces to optimization of a policy value estimator analogously to supervised learning; however the accuracy of policy value estimation depends upon the mismatch between the policy being evaluated and the policy that generated the data and therefore can be quite different for different policies (unlike supervised learning where the differences in estimator resolution across the hypothesis class are less pronounced in practice).  To appreciate this effect consider the IPS policy value estimator $$ \hat{V}(\pi; \mu) = \frac{1}{N} \sum_{n \in N} \frac{\pi(a_n|x_n)}{\mu(a_n|x_n)} r_n $$ where $\mu$ is the historical policy $\pi$ is the policy being estimated and our historical data consists of tuples $\{ (x_n a_n r_n) \}$.  The importance weight $\frac{\pi(a_n|x_n)}{\mu(a_n|x_n)}$ can be quite large if $\pi$ frequently takes an action that $\mu$ rarely takes causing the estimator to be highly sensitive to a few examples with large importance weights.  Even if we initialize learning with $\pi = \mu$ as optimization progresses $\pi$ will induce increasingly different distributions over actions than $\mu$ as the learning algorithm encounters rare events with high reward.  To combat this overfitting technique we will introduce regularization.  Distributionally Robust Optimization Distributionally robust optimization is a generic method for regularizing machine learning objectives.  The basic idea is to consider the observed data as one possible distribution of data (the “empirical distribution”) and then to optimize a worst-case outcome over all distributions that are “sufficiently close” to the empirical distribution.  In the case of IPS we can find the smallest policy value estimate over a set of distributions that are close in KL divergence to the empirical distribution $$ \begin{alignat}{2} &amp;\!\min_{P \in \Delta} &amp; \qquad &amp; \mathbb{E}_P\left[w r\right] \\ &amp;\text{subject to} &amp; &amp; \mathbb{E}_P\left[w\right] = 1 \\ &amp; &amp; &amp; \mathbb{E}_N \left[\log\left( P\right) \right] \geq \phi. \end{alignat} $$ where $w \doteq \frac{\pi(a|x)}{\mu(a|x)}$.  It turns out you can do this cheaply (in the dual) and the value of $\phi$ can be computed from a desired asymptotic confidence level.  These results follow from classic work in the field of Empirical Likelihood.  The above problem finds a lower bound; finding an upper bound is analogous resulting in the confidence intervals from the paper:  Empirical Likelihood Confidence Intervals are tighter Gaussian intervals.&nbsp; Not shown: coverage of Empirical Likelihood CIs is better calibrated than Binomial (Clopper-Pearson). When we do distributionally robust optimization we are actually optimizing the lower bound on the policy value.  The green curve in the above plot is a Clopper-Pearson interval which does have guaranteed coverage but is so wide that optimizing the lower bound wouldn't do much until the amount of data is large.  The tighter intervals generated by the blue Empirical Likelihood curve imply that lower bound optimization will induce an interesting policy ordering with only modest data.  In practice even when the empirical mean (empirical IPS value) is fixed the lower bound is:   smaller when the policy generates value via few events with importance weights much larger than 1 and many events with importance weights near zero; and  larger when the policy generates value via many events with importance weights near 1.This is precisely the kind of regularization we desired.  Intuitively any estimator which is sensitive to a few of the observed examples (aka high leverage) will have a larger penalty because it is “cheap” as measured by KL divergence to reduce the probability of those points.  Implementation in Vowpal Wabbit To activate the functionality add the --cb_dro flag to your contextual bandit command line in VW.  Note it only effects training so if you are only predicting you will not see a difference.  Hopefully with the default hyperparameters you will see an improvement in the quality of your learned policy such as in this gist.  Internally VW is solving the lower bound optimization problem from above on every example.  There are some modifications: As stated above this would be too expensive computationally but switching from the KL divergence to the Cressie-Read power divergence allows us to derive a closed form solution which is fast to compute.As stated above the lower bound requires remembering all policy decisions over all time.  Instead we accumulate the sufficient statistics for the Cressie-Read power divergence in $O(1)$ time and space.To track nonstationarity we use exponentially weighted moving averages of the sufficient statistics.  The hyperparameter --cb_dro_tau specifies the decay time constant.As always YMMV.
2020,7,18,Convex-concave games off the shelf,http://www.machinedlearnings.com/2020/07/convex-concave-games-off-shelf.html,If you need to solve a convex optimization problem nowadays you are in great shape.  Any problem of the form $$\begin{alignat}{2}&\!\inf_z & \qquad & f(z) \\& \text{subject to} & & h(z) = 0 \\& & & g(z) \preceq 0\end{alignat}$$ where $f$ and $g$ are convex and $h$ is affine can be attacked by several excellent freely available software packages: my current favorite is cvxpy which is a joy to use.  If you have a lot of variables and not a lot of constraints you can instead solve a dual problem.  It ends up looking like $$\begin{alignat}{2}&\!\sup_{x}  & \qquad & L(x) \\& \text{subject to} & & \tilde{h}_x(x) = 0 \\& & & \tilde{g}_x(x) \preceq 0\end{alignat}$$ where $L$ is concave assuming that you get lucky and can analytically eliminate all the primal variables $z$ such that only the dual variables $x$ remain.  But what if you can't eliminate all the primal variables but only most of them?  You might end up with a problem like $$\begin{alignat}{2}&\!\sup_{x} \inf_y & \qquad & L(x y) \\& \text{subject to} & & \tilde{h}_x(x) = 0 \\& & & \tilde{g}_x(x) \preceq 0 \\& & & \tilde{h}_y(y) = 0 \\& & & \tilde{g}_y(y) \preceq 0\end{alignat}$$ where $\tilde{g}_x$ and $\tilde{g}_y$ are convex and $\tilde{h}_x$ and $\tilde{h}_y$ are affine $L(x \cdot)$ is convex in $y$ given fixed $x$ and $L(\cdot y)$ is concave in $x$ given fixed $y$.  It feels like this problem should be easier to solve than the original problem if many primal variables have been analytically eliminated.  Unfortunately none of my favorite convex optimization toolkits will accept a problem of this form.  This is despite the viability of interior-point methods for such problems.  Bummer.One thing I tried was to solve the inner infimum using a standard toolkit compute the gradient of the solution wrt the outer parameters via the sensitivity map and then use a first-order method for the outer supremum.  This did not work for me: it works for toy problems but on real problems the outer supremum has very slow convergence suggesting ill-conditioning.  What I need is the power of interior-point methods to handle ill-conditioning via second-order information.  I'm able to achieve this via sequential quadratic minimax programming: first locally approximate the objective $L(\lambda \mu y)$ with a quadratic around the current point and linearize the constraints. $$\begin{alignat}{2}&\!\sup_{\delta x} \inf_{\delta y} & \qquad & \frac{1}{2} \left(\begin{array}{c} \delta x \\ \delta y \end{array}\right)^\top \left(\begin{array}{cc} P_{xx} & P_{yx}^\top \\ P_{yx} & P_{yy} \end{array}\right) \left(\begin{array}{c} \delta x \\ \delta y \end{array} \right) + \left(\begin{array}{c} q_x \\ q_y \end{array} \right)^\top \left(\begin{array}{c} \delta x \\ \delta y \end{array} \right) \\& \text{subject to} & & \left(\begin{array}{cc} A_x & 0 \\ 0 & A_y \end{array} \right) \left(\begin{array}{c} \delta x \\ \delta y \end{array}\right) = \left(\begin{array}{c} b_x \\ b_y \end{array}\right) \\& & & \left(\begin{array}{cc} G_x & 0 \\ 0 & G_y \end{array} \right) \left(\begin{array}{c} \delta x \\ \delta y \end{array}\right) \preceq \left(\begin{array}{c} h_x \\ h_y \end{array}\right) \\\end{alignat}$$ The Wolfe dual converts this problem into a standard QP: $$\begin{alignat}{2}&\!\sup_{\delta x \delta y \lambda \mu} & \qquad &  \frac{1}{2} \left(\begin{array}{c} \delta x \\ \delta y \\ \lambda \\ \mu \end{array}\right)^\top \left(\begin{array}{cccc} P_{xx} & 0 & 0 & 0 \\ 0 & -P_{yy} & 0 & 0 \\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 \end{array}\right) \left(\begin{array}{c} \delta x \\ \delta y \\ \lambda \\ \mu \end{array}\right) + \left(\begin{array}{c} q_x \\ 0 \\ -b_y \\ -h_y \end{array} \right)^\top \left(\begin{array}{c} \delta x \\ \delta y \\ \lambda \\ \mu \end{array} \right) \\& \text{subject to} & & \left(\begin{array}{cc} A_x & 0 & 0 & 0 \\ P_{yx} & P_{yy} & A_y^\top & G_y^\top \end{array} \right) \left(\begin{array}{c} \delta x \\ \delta y \\ \lambda \\ \mu \end{array}\right) = \left(\begin{array}{c} b_x \\ -q_y \end{array}\right) \\& & & \left(\begin{array}{cc} G_x & 0 & 0 & 0  \\ 0 & 0 & 0 & -I \end{array} \right) \left(\begin{array}{c} \delta x \\ \delta y \\ \lambda \\ \mu \end{array}\right) \preceq \left(\begin{array}{c} h_x \\ 0 \end{array}\right) \\\end{alignat}$$ If you solve this for $(\delta x \delta y)$ you get a Newton step for your original problem.  The step acceptance criterion here is tricky: if the iterate is feasible you want to leverage the saddle point condition (see equation (11) of Essid et. al.).  If the iterate is infeasible more sophistication is required but fortunately my constraints were actually linear so doing an initial exact inner solve allowed me to iterate while staying feasible.  (Note: if you solve a more general convex problem on each step you don't need to linearize the $x$ constraints.)YMMV!
2019,5,10,ICLR 2019 Thoughts,http://www.machinedlearnings.com/2019/05/iclr-2019-thoughts.html,ICLR 2019 was reminiscent of the early NeurIPS days (sans skiing): a single track of talks vibrant poster sessions and a large mid-day break.  The Tuesday morning talks were on climate change modeling proteins generating music and modeling the visual cortex.  Except for climate change these were all hot topics at NeurIPS in the late 1990s.  History doesn't repeat but it does rhyme.My favorite talk was by Pierre-Yves Oudeyer whose research in curiosity based learning spans both human subjects and robotics.  Pierre's presentation was an entertaining tour de force of cognitive science and I highly recommend watching the video (starts about 9 minutes 30 seconds).  These ideas have extensively influenced the reinforcement learning community: the well-known Achilles' Heel of reinforcement learning is sample complexity and recently practitioners have attacked it inspired by ideas from curiosity based learning (e.g. the Burda et. al. poster at the conference).   Furthermore the view &ldquo;exploration is for building world models&rdquo; is reflected in recent theoretical results in contextual decision processes.The strangest moment for me at the conference was seeing the GLUE poster.  Apparently with the latency of conference review and publication GLUE is just now being presented.  Of course it is already obsolete so the presenters had another poster about their new dataset called SuperGLUE.  Things are moving so quickly that the former &ldquo;fast path&rdquo; of conference proceedings is now noticeably behind.Here's some stuff that caught my attention:Non-Vacuous Generalization Bounds at the ImageNet Scale: A PAC-Bayesian Compression Approach: A couple years ago Zhang et. al. stunned the community by demonstrating convnets can fit Imagenet labels to randomly generated images destroying the common belief that convnets generalized well due to capacity control.  Here Zhou et. al. show that an MDL-style generalization bound applies i.e. networks whose representation can be compressed after training have tighter deviation bounds.  This is a (training) data-dependent bound and they seal the argument by noting networks trained on randomized training data do not compress as well.Episodic Curiousity through Reachability: One of many curiosity-based exploration posters Savinov et. al. propose a combination of a memory and something akin to a policy cover with promising results.  Also cool: the poster includes QR codes which trigger videos of agents learning to move via different algorithms.Supervised Policy Update for Deep Reinforcement Learning: Vuong et. al. presented a plausible improvement over TRPO and PPO by convexifying the constrained policy optimization.
2019,3,8,RL will disrupt OR,http://www.machinedlearnings.com/2019/03/rl-will-disrupt-or.html,"Operations research (OR) is in the initial stages of a revolution driven by reinforcement learning (RL).When I was at eHarmony years ago we used classical OR techniques to drive the matchmaking process.  Machine learning played a critical role but was limited to specfiying parameters to the OR solver.  In essence machine learning was to used to estimate the value function and the OR solver then produced a policy.  OR has historically focused on highly tractable specializations of convex optimization.  In an age of scarce compute this made perfect sense: indeed at that time eHarmony was pushing the limits of what was possible using high-end commercial OR solvers.  However compute is less scarce now: in predictive modeling convex optimization has been spectacularly superseded by non-convex techniques (aka “deep learning”).  A similar revolution is unfolding in OR.  The recipe is: develop a generative model of the problem (aka a “simulator”) and then directly optimize a policy on simulated data using RL techniques.All models are wrong but some models are useful.  At first blush it seems implausible that using advanced RL techniques on a crude approximation of the real world would yield substantial benefits.  However traditional OR techniques also preform extremely aggressive optimization (to machine precision (!)) on an approximation of the real world.  OR models are useful despite typically making tremendous simplifications such as replacing all random variables with their expected values (or in more sophisticated setups high probability bounds).  The simplifications for the RL techniques involve the assumptions in the generative model such as a particular parametric model for probability of an airplane service event.  Early research results suggest that for some economically important problems relatively crude simulators coupled with RL techniques can induce superior policies to those developed using traditional OR techniques.  Furthermore simulators admit expressing increasingly refined approximations of reality without the constraints imposed by classical OR formulations. Reaction time is a factor in this so please pay attention.Reaction time.&nbsp;You should almost never take seriously anyone's explanation for why something works.  Nonetheless I'll give you my intution as to why RL will eventually dominate OR.&ldquo;Classical OR techniques are optimized to try to avoid bad events ballistically whereas RL trained policies are optimized to react to events as they occur.&rdquo;  If this is true then it doesn't matter if the simulation exactly gets the probability of tail events right as long as they are all present in the simulation and somewhat rare because the ""use of remediation actions"" portion of the learned policy will be conditioned on events as they actually occur in practice.  (If the events are not rare then getting the coocurrence statistics right could matter.)If this explanation has merit then the upside to RL will be large for scenarios where classic OR optimization is frequently re-run in order to react to new events because RL will have the “reaction time advantage”."
2018,3,6,pytorch PU learning trick,http://www.machinedlearnings.com/2018/03/pytorch-pu-learning-trick.html,"I'm often using positive-unlabeled learning nowadays.  In particular for observational dialog modeling next utterance classification is a standard technique for training and evaluating models.  In this setup the observed continuation of the conversation is considered a positive (since a human said it it is presumed a reasonable thing to say at that point in the conversation) and other randomly chosen utterances are treated as unlabeled (they might be reasonable things to say at that point in the conversation).Suppose you have a model whose final layer is a dot product between a vector produced only from context and a vector produced only from response.  I use models of this form as &ldquo;level 1&rdquo; models because they facilitate precomputation of a fast serving index but note the following trick will not apply to architectures like bidirectional attention.  Anyway for these models  you can be more efficient during training by drawing the negatives from the same mini-batch.  This is a well-known trick but I couldn't find anybody talking about how to do this explicitly in pytorch.  Structure your model to have a leftforward and a rightforward like this:class MyModel(nn.Module):...    def forward(leftinput rightinput):        leftvec = self.leftforward(leftinput)        rightvec = self.rightforward(rightinput)        return torch.mul(leftvec rightvec).sum(dim=1 keepdim=True)At training time compute the leftforward and rightforward for your mini-batch distinctly:...criterion = BatchPULoss()model = MyModel()...leftvec = model.leftforward(batch.leftinput)rightvec = model.rightforward(batch.rightinput)(loss preds) = criterion.fortraining(leftvectors rightvectors)loss.backward()# ""preds"" contains the highest score right for each left # so for instance calculate ""mini-batch precision at 1""gold_labels = torch.arange(0 batch.batch_size).long().cuda()n_correct += (preds.data == gold_labels).sum()...Finally use this loss:import torchclass BatchPULoss():    def __init__(self):      self.loss = torch.nn.CrossEntropyLoss()    def fortraining(self left right):      outer = torch.mm(left right.t())      labels = torch.autograd.Variable(torch.arange(0outer.shape[0]).long().cuda()                                        requires_grad=False)      loss = self.loss(outer labels)      _ preds = torch.max(outer dim=1)      return (loss preds)    def __call__(self *args **kwargs):      return self.loss(*args **kwargs)At training time you call the fortraining method but if you have fixed distractors for evaluation you can also call it directly just like CrossEntropyLoss."
2017,12,15,NIPS Conversation AI Workshop,http://www.machinedlearnings.com/2017/12/nips-conversation-ai-workshop.html,"I only attended NIPS for the Conversation AI workshop so my thoughts are limited to that.  I really liked the subtitle of the workshop: ""today's practice and tomorrow's potential.""  Since I'm on a product team trying to build chatbots that are actually effective it struck me as exactly the right tone.Several presentations were related to the Alexa prize.  When reading these papers keep in mind that contestants were subject to extreme sample complexity constraints.  Semifinalists had circa 500 on-policy dialogs and finalists less than 10 times more.  This is because 1) the Alexa chat function is not the primary purpose of the device so not all end users participated and 2) they had to distribute the chats to all contestants.The result of sample complexity constraints is a &ldquo;bias against variance&rdquo; as I've discussed before.  In the Alexa prize that meant the winners had the architecture of &ldquo;learned mixture over mostly hand-specified substrategies.&rdquo;  In other words the (scarce) on-policy data was limited to adjusting the mixture weights.  (The MILA team had substrategies that were trained unsupervised on forum data but it looks like the other substrategies were providing most of the benefit.)  Sample complexity constraints are pervasive in dialog but nonetheless the conditions of the contest were more extreme than what I encounter in practice so if you find yourself with more on-policy data consider more aggressive usage.Speaking of sample complexity constraints we have found pre-training representations on MT tasks a la CoVE is extremely effective in practice for multiple tasks.  We are now playing with ELMo-style pre-training using language modeling as the pre-training task (very promising: no parallel corpus needed!).Another sample complexity related theme I noticed at the workshop was the use of functional role dynamics.  Roughly speaking this is modeling the structure of the dialog independent of the topic.  Once topics are abstracted the sample complexity of learning what are reasonably structured conversations seems low.  Didericksen et. al. combined a purely structural L1 model with a simple topically-sensitive L2 (tf-idf) to build a retrieval based dialog simulator.  Analogously for their Alexa prize submission Serban et. al. learned a dialog simulator from observational data which utilized only functional role and sentiment information and then applied Q-learning: this was more effective than off-policy reinforce with respect to some metrics.Overall the workshop gave me enough optimism to continue plugging away despite the underwhelming performance of current dialog systems."
2017,8,11,ICML 2017 Thoughts,http://www.machinedlearnings.com/2017/08/icml-2017-thoughts.html,"ICML 2017 has just ended.  While Sydney is remote for those in Europe and North America the conference centeris a wonderful venue (with good coffee!) and the city is a lot of fun.  Everything went smoothly and the organizers did a great job.You can get a list of papers that I liked from my Twitter feed so instead I'd like to discuss some broad themes I sensed.Multitask regularization to mitigate sample complexity in RL.  Both in video games and in dialog it is useful to add extra (auxiliary) tasks in order to accelerate learning.Leveraging knowledge and memory.  Our current models are powerful function approximators but in NLP especially we need to go beyond ""the current example"" in order exhibit competence.Gradient descent as inference.  Whether it's inpainting with a GAN or BLUE score maximization with an RNN gradient descent is an unreasonably good inference algorithm.Careful initialization is important.  I suppose traditional optimization people would say ""of course"" but we're starting to appreciate the importance of good initialization for deep learning.  In particular start close to linear with eigenvalues close to 1. (Balduzzi et. al.  Poole et. al.)Convolutions are as good as and faster than recurrent models for NLP.  Nice work out of Facebook on causal convolutions for seq2seq.  This aligns with my personal experience: we use convolutional NLP models in production for computational performance reasons.Neural networks are overparameterized.  They can be made much sparser without losing accuracy (Molchanov et. al. Lobacheva et. al.).maluuba had the best party.  Woot!Finally I kept thinking the papers are all &ldquo;old&rdquo;.  While there were lots of papers I was seeing for the first time it nonetheless felt like the results were all dated because I've become addicted to &ldquo;fresh results&rdquo; on arxiv."
2017,7,26,Rational Inexuberance,http://www.machinedlearnings.com/2017/07/rational-inexuberance.html,"Recently Yoav Goldberg had a famous blog rant.  I appreciate his concern because the situation is game-theoretically dangerous: any individual researcher receives a benefit for aggressively positioning their work (as early as possible) but the field as a whole risks another AI winter as rhetoric and reality become increasingly divergent.  Yoav's solution is to incorporate public shaming in order to align local incentives with aggregate outcomes (c.f. reward shaping).I feel there is a better way as exemplified by a recent paper by Jia and Liang.  In this paper the authors corrupt the SQUAD dataset with distractor sentences which have no effect on human performance but which radically degrade the performance of the systems on the leaderboard.  This reminds me of work by Paperno et. al. on a paragraph completion task which humans perform with high skill and for which all state of the art NLP approaches fail miserably.  Both of these works clearly indicate that our current automatic systems only bear a superficial (albeit economically valuable) resemblance to humans.This approach to honest self-assessment of our capabilities is not only more scholarly but also more productive as it provides concrete tasks to consider.  At minimum this will result in improved technological artifacts.  Furthermore iterating this kind of goal-setting-and-goal-solving procedure many many times might eventually lead to something worthy of the moniker Artificial Intelligence.(You might argue that the Yoav Goldberg strategy is more entertaining but the high from the Yoav Goldberg way is a ""quick hit"" whereas having a hard task to think about has a lot of ""replay value"".)"
2017,7,17,"Tiered Architectures, Counterfactual Learning, and Sample Complexity",http://www.machinedlearnings.com/2017/07/tiered-architectures-counterfactual.html,I'm on a product team now and once again I find myself working on a tiered architecture: an &ldquo;L1&rdquo; model selects some candidates which are passed to an &ldquo;L2&rdquo; model which reranks and filters the candidates which are passed to an &ldquo;L3&rdquo; etc.  The motivation for this is typically computational e.g. you can index a DSSM model pretty easily but indexing a BIDAF model is more challenging.  However I think there are potential sample complexity benefits as well.I worry about sample complexity in counterfactual setups because I think it is the likely next source for AI winter.  Reinforcement learning takes a tremendous amount of data to converge which is why all the spectacular results from the media are in simulated environments self-play scenarios discrete optimization of a sub-component within a fully supervised setting or other situations where there is essentially infinite data.  In real life data is limited.So when I read Deep Reinforcement Learning in Large Discrete Action Spaces by Dulac-Arnold et. al. I noticed that the primary motivation was computational but figured another (more important?) benefit might be statistical.  Tiered architectures cannot overcome worst-case sample complexity bounds but I think in practice they are a good strategy for counterfactual setups.Tiered architectures admit semi-supervised approaches because an L1 model can often be initialized using unsupervised techniques (e.g. word embeddings sentence embeddings inverted indicies with tf-idf).  Learning the L2 model utilizing this L1 model only has a sample complexity based upon the number of candidates produced by the L1 model rather than the total number of candidates.  Of course learning the L1 still has a sample complexity based upon the total number of candidates but if the unsupervised initialization is good then it is ok that the L1 learns slowly.  Furthermore in practice the L1 hypothesis class is simpler (because of computational reasons) which mitigates the sample complexity.There was a workshop called ``coarse-to-fine inference'' at NIPS 2017 which presumably explored these ideas but I didn't attend it and their website is down.  Hopefully there will be another one I will attend!
2017,3,25,Why now is the time for dialog,http://www.machinedlearnings.com/2017/03/why-now-is-time-for-dialog.html,I'm working on a task-oriented dialog product and things are going surprisingly well from a business standpoint.  It turns out that existing techniques are sufficient to substitute some portion of commercial dialog interactions from human to machine mediated with tremendous associated cost savings which exceed the cost of developing the automatic systems.  Here's the thing that is puzzling: the surplus is so large that as far as I can tell it would have been viable to do this 10 years ago with then-current techniques.  All the new fancy AI stuff helps but only to improve the margins.  So how come these businesses didn't appear 10 years ago?I suspect the answer is that a format shift has occurred away from physical transactions and voice mediated interactions to digital transactions and chat mediated interactions.  The movement away from voice is very important: if we had to try and do this using ASR even today it probably wouldn't work.  Fortunately today you chat with your cable company rather than talking to them.  That shift was motivated by cost savings: a human agent can handle multiple concurrent chat sessions more easily than multiple concurrent voice conversations.  However it requires most of your customers to have a computer smartphone or other device rather than an old-school telephone.  The continuing dominance of e-commerce over physical stores is also a factor (RIP Sears).  In e-commerce human salespersons increasingly assist customers in transactions via live chat interfaces.  Once again what starts as a more effective way of deploying human resources becomes the vector by which automation increasingly handles the workload.The end game here is that the number of people employed in retail goes down but that their compensation goes up.  That is because the machines will increasingly handle the routine aspects of these domains leaving only the long tail of extremely idiosyncratic issues for the humans to resolve.  Handling these non-routine issues will require more skill and experience and therefore demand higher compensation (also an increasing part of the job will be to structure the torso of non-routine issues into something that the machines can handle routinely i.e. teaching the machines to handle more; this is analogous to programming and will also demand higher compensation).
2017,2,16,Software Engineering vs Machine Learning Concepts,http://www.machinedlearnings.com/2017/02/software-engineering-vs-machine.html,Not all core concepts from software engineering translate into the machine learning universe.  Here are some differences I've noticed.Divide and Conquer A key technique in software engineering is to break a problem down into simpler subproblems solve those subproblems and then compose them into a solution to the original problem.  Arguably this is the entire job recursively applied until the solution can be expressed in a single line in whatever programming language is being used.  The canonical pedagogical example is the Tower of Hanoi.Unfortunately in machine learning we never exactly solve a problem.  At best we approximately solve a problem.  This is where the technique needs modification: in software engineering the subproblem solutions are exact but in machine learning errors compound and the aggregate result can be complete rubbish.  In addition apparently paradoxical situations can arise where a component is &ldquo;improved&rdquo; in isolation yet aggregate system performance degrades when this &ldquo;improvement&rdquo; is deployed (e.g. due to the pattern of errors now being unexpected by downstream components even if they are less frequent).Does this mean we are doomed to think holistically (which doesn't sound scalable to large problems)?  No but it means you have to be defensive about subproblem decomposition.  The best strategy when feasible is to train the system end-to-end i.e. optimize all components (and the composition strategy) together rather than in isolation.  Often this is not feasible so another alternative (inspired by Bayesian ideas) is to have each component report some kind of confidence or variance along with the output in order to facilitate downstream processing and integration.In practice when systems get to a particular scope there needs to be decomposition in order to divide the work up amongst many people.  The fact that this doesn't work right now in machine learning is a problem as elegantly described by Leon Bottou in his ICML 2015 invited talk.Speaking of another concept that Leon discussed $\ldots$Correctness In software engineering an algorithm can be proven correct in the sense that given particular assumptions about the input certain properties will be true when the algorithm terminates.  In (supervised) machine learning the only guarantee we really have is that if the training set is an iid sample from a particular distribution then performance on another iid sample from the same distribution will be close to that on the training set and not too far from optimal.Consequently anyone who practice machine learning for a living has an experimental mindset.  Often times I am asked whether option A or option B is better and most of the time my answer is &ldquo;I don't know let's try both and see what happens.&rdquo;  Maybe the most important thing that people in machine learning know is how to assess a model in such a way that is predictive of generalization.  Even that is a &ldquo;feel&rdquo; thing: identifying and preventing leakage between training and validation sets (e.g. by stratified and temporal sampling) is something you learn by screwing up a few times; ditto for counterfactual loops.  Kaggle is great for learning about the former but the latter seems to require making mistakes on a closed-loop system to really appreciate.Experimental &ldquo;correctness&rdquo; is much weaker than the guarantees from other software and there are many ways for things to go badly.  For example in my experience it is always temporary: models go stale it just always seems to happen.  Ergo you need to plan to be continually (hence automatically) retraining models.Reuse This one is interesting.  Reuse is the key to leverage in traditional software engineering: it's not just more productive to reuse other code but every line of code you write yourself is an opportunity to inject defects.  Thus reuse not only allows you to move faster but also make less mistakes: in return you must pay the price of learning how to operate a piece of software written by others (when done well this price has been lowered through good organization documentation and community support).  Some aspects of machine learning exhibit exactly the same tradeoff.  For instance if you are writing your own deep learning toolkit recognize that you are having fun.  There's nothing wrong with having fun and pedagogical activities are arguably better than playing video games all day.  However if you are trying to get something done you should absolutely attempt to reuse as much technology as you can which means you should be using a standard toolkit.  You will move faster and make less mistakes once you learn how to operate the standard toolkit.Machine learning toolkits are &ldquo;traditional software&rdquo; however and are designed to be reused.  What about model reuse?  That can be good as well but the caveats about decomposition above still apply.  So maybe you use a model which produces features from a user profile as inputs to your model.  Fine but you should version the model you depend upon and not blindly upgrade without assessment or retraining.  Reusing the internals of another model is especially dangerous as most machine learning models are not identifiable i.e. have various internal symmetries which are not determined by the training procedure.  Couple an embedding to a tree for instance and when the next version of the embedding is a rotation of the previous one you can watch your performance go to crap immediately.Basically model reuse creates strong coupling between components which can be problematic if one component is changed.  Testing I find the role of software testing in machine learning to be the trickiest issue of all.  Without a doubt testing is necessary but the challenge in using something like property-based testing is that the concept that is being captured by the machine learning component is not easily characterized by properties (otherwise you would write it using non-ml software techniques).  To the extent there are some properties that the ml component should exhibit you can test for these but unless you incorporate these into the learning procedure itself (e.g. via parameter tying or data augmentation) you are likely to have some violations of the property that are not necessarily indicative of defects.Having a &ldquo;extra-test&rdquo; data set of with minimal acceptable quality is a good idea: this could be easy examples that &ldquo;any reasonable model&rdquo; should get correct.  There's also self-consistency: at Yahoo they used to ship models with a set of input-output pairs that were computed with the model when it was put together and if the loaded model didn't reproduce the pairs the model load was cancelled.  (That should never happen right?  Surprise!  Maybe you are featurizing the inputs using a library with a different version or something.)  Monitoring the metrics (proxy and true) of deployed models is also good for detecting problems.  If the proxy metric (i.e. the thing on which you actually trained your model and estimated generalization performance) is going south the inputs to your model are changing somehow (e.g. nonstationary environment change in feature extraction pipeline); but if the proxy metric is stable while the true metric is going south the problem might be in how the outputs of your model are being leveraged.Unfortunately what I find is many software systems with machine learning components are tested in a way that would make traditional software engineers cringe: we look at the output to see if it is reasonable.  Crazy!  As machine learning becomes a more pervasive part of software engineering this state of affairs must change.
2017,1,28,Reinforcement Learning and Language Support,http://www.machinedlearnings.com/2017/01/reinforcement-learning-and-language.html,What is the right way to specify a program that learns from experience?  Existing general-purpose programming languages are designed to facilitate the specification of any piece of software.  So we can just use these programming languages for reinforcement learning right?  Sort of.Abstractions matterAn analogy with high performance serving might be helpful.  An early influential page on high performance serving (the C10K problem by Dan Kegel) outlines several I/O strategies.  I've tried many of them.  One strategy is event-driven programming where a core event loop monitors file descriptors for events and then dispatches handlers.  This style yields high performance servers but is difficult to program and sensitive to programmer error.  In addition to fault isolation issues (if all event are running in the same address space) this style is sensitive to whenever any event handler takes too long to execute (e.g. hidden blocking I/O calls computationally intensive operations etc.).  In contrast thread-based programming allowed you to pretend that you were the only handler running.  It was less computationally efficient and still had fault isolation issues but it was easier to reason about.  (Subsequently I started getting into Erlang because it essentially tried to bake user-space threading with fault isolation into the language which was even better.)I don't know what the state-of-the-art is in high performance serving now I'm a bit out of that game.  The main point is that all programming languages are not created equal in that they create different cognitive burdens on the programmer and different computational burdens at runtime.  I could use an existing language (at that time C++) in one of two ways (cooperative scheduling vs. pre-emptive scheduling) or I could use a different language (Erlang) that was designed to mitigate the tradeoff.Imperative specification with automatic credit assignmentAs previously stated the difference between the programs we'd like to specify now versus the ones specified in the past is that we want our programs to be able to learn from experience.  As with high-performance serving we'd like to balance the cognitive burden on the programmer with the computational burden imposed at runtime (also possibly the statistical burden imposed at runtime; computational burdens correspond to resources such as time or space whereas the statistical burden corresponds to data resources).Within the current &ldquo;AI summer&rdquo; one idea that become popular is automatic differentiation.  Full AD means that essentially any language construct can be used to define a function and the computation to compute the gradient of the function with respect to the input is provided &ldquo;for free.&rdquo;  A language equipped with AD which is computing a (sub-)differentiable function can learn from experience in the sense of moving closer to a local optimum of a loss function.  Deep learning toolkits implement AD to various degrees with some frameworks (e.g. Chainer) aggressively pursuing the idea of allowing arbitrary language constructs when specifying the forward computation.The ability to use arbitrary language constructs becomes increasingly important as inference becomes more complicated.  Simple inference (e.g. classification or ranking) is easy to reason about but beyond that it quickly becomes a major source of defects to 1) specify how the output of a machine learning model is used to synthesize a complete system and 2) specify how the data obtained from running that complete system is used to update the model.The problem is clearly visible in the field of structured prediction.  &ldquo;Structured prediction&rdquo; of course is a somewhat ridiculous term analogous to the term &ldquo;nonlinear systems analysis&rdquo;; in both cases a simpler version of the problem was solved initially (classification and linear systems analysis respectively) and then an umbrella term was created for everything else.  Nonetheless Hal Daume has a good definition of structured prediction which is making multiple predictions on a single example and experiencing a joint (in the decisions) loss.  (He also has a Haiku version of this definition.)Because inference in structured prediction is complicated the ideas of imperative specification and automated credit assignment were essentially reinvented for structured prediction.  The technique is outlined in an Arxiv paper by Chang et. al. but fans of Chainer will recognize this as the analog of &ldquo;define-by-run&rdquo; for structured prediction.  (Note the optimization strategy here is not gradient descent at least not on the forward computation but rather something like a policy gradient method which translates to a discrete credit assignment problem over the predictions made by the forward computation.)One way to view episodic RL is structured prediction with bandit feedback: structured prediction is fully observed analogous to supervised learning in that it is possible to compute the loss of any sequence of decisions given a particular input.  In reinforcement learning you have bandit feedback i.e. you only learn about the loss associated with the sequence of decisions actually taken.  While this isn't the only way to view episodic RL it does facilitate connecting with some of the ideas of the paper mentioned in the previous paragraph.A Motivating ExampleHere's an example which will hopefully clarify things.  Suppose we want to build an interactive question-answering system in which users pose questions and then the system can optionally ask a (clarifying) question to the user or else deliver an answer.  We can view this as an episodic RL problem where the user statements are observations system questions are actions system answers are more actions and the episode ends as soon as we deliver an answer.What I'd like to do is specify the computation something like this pseudo-python:def interactive_qa_episode():  uq = get_user_question()  qapairs = []  sysaction = get_next_system_action(uq qapairs)  while (sysaction.is_question):    ua = get_user_answer(sysaction.utterance)    qapairs.append((sysactionua))    sysaction = get_next_system_action(uq qapairs)  deliverAnswer(sysaction.utterance)It is pretty clear what is going on here: we get a user question conditionally ask questions and then deliver an answer.  Before the advent of machine learning an implementer of such a system would attempt to fill out the unspecified functions above: in particular get_next_system_action is tricky to hand specify.  What we would like to do is learn this function instead.It would be nice to use decorators to achieve this.  First to learn we need some idea of doing better or worse so assume after delivering an answer there is some way to decide how satisfied the user is with the session (which ceterus perebus should be monotonically decreasing with the number of questions asked to encourage expediency):@episodicRLdef interactive_qa_episode():  uq = get_user_question()  qapairs = []  sysaction = get_next_system_action(uq qapairs)  while (sysaction.is_question):    ua = get_user_answer(sysaction.utterance)    qapairs.append((sysactionua))    sysaction = get_next_system_action(uq qapairs)# this next line is the only change to the original function  reward = deliverAnswer(sysaction.utterance) All too easy!  Pseudo-code is so productive.  We can even imagine updating reward multiple times with the decorator keeping track of the reward deltas for improved credit assignment.Now some magic metaprogramming kicks in and converts this into a model being trained with an RL algorithm (e.g. a value iteration method such as q-learning or a policy iteration method such as bandit LOLS).  Or does it?  We still haven't said which functions are to be learned and which are hand-specified.  The default will be hand-specified so we will decorate one function.@learnedFunctiondef get_next_system_action(uq qapairs):  ...Now we get into some thorny issues.  We need to specify this functions ultimately in terms of a parameterized model like a neural network; we'll have to say what the initial representation is that is computed from variables like uq and qapairs; and we'll have to say how the output of the model is mapped onto an actual decision.  Just to keep moving let's assume there is a fixed small set of system questions and system answers.action_table = [ ... ] # list containing action mapping@learnedFunctiondef get_next_system_action(uq qapairs):  not_allowed_action_ids = [ sysa.action_id for (sysa _) in qapairs ]  action_id = categorical_choice(uq: uq                                 qapairs: qapairs                                 not_allowed_action_ids: not_allowed_action_ids                                 tag: 'nextsystemaction')  return action_table[action_id]categorical_choice is the representation of a forced choice from one of a set of possibilities.  For small action spaces this could be directly implemented as an output per action but for large action spaces this might be implemented via action embeddings with an information-retrieval style cascading pipeline.Great right?  Well some problems remain. The best model structure (i.e. policy class) for the choice requires some specification by the programmer e.g. a convolutional text network vs. an iterated attention architecture.  Ideally this specification is distinct from the specification of inference so that many modeling ideas can be tried.  That's the purpose of the tag argument to join with a separate specification of the learning parameters.  (If not provided sane default tags could be generated during compilation.) As indicated in the previous post bootstrapping is everything.  So an initial implementation of get_next_system_action needs to be provided.  Maybe this reduces to providing an initial setting of the underlying model but maybe it doesn't depending upon the initialization scenario.  Note if initialization is done via simulation or off-policy learning from historical data these could be supported by facilitating the mockup of the I/O functions get_user_question and get_user_answer.  Another common scenario is that a not-learned function is provided as a reference policy with which the learned function should compete.Can't I do this with Chainer already? Sort of.  If you use a particular RL algorithm definitely.  For instance q-learning reduces reinforcement learning to regression so if you code that inline you get something Chainer could handle.  However the goal is to specify inference without leaking details about the learning algorithm so I'd rather not code that inline.  An alternative is to compile to Chainer akin to cfront in the early days of c++.  Ultimately however I would hope to have a different compilation strategy.  There's more at stake than just implementing the learning algorithm: there are all the issues mentioned in my previous post that have convinced me that the implementation should be able to leverage a reinforcement learning service.
2017,1,22,Reinforcement Learning as a Service,http://www.machinedlearnings.com/2017/01/reinforcement-learning-as-service.html,I've been integrating reinforcement learning into an actual product for the last 6 months and therefore I'm developing an appreciation for what are likely to be common problems.  In particular I'm now sold on the idea of reinforcement learning as a service of which the decision service from MSR-NY is an early example (limited to contextual bandits at the moment but incorporating key system insights).Service not algorithm Supervised learning is essentially observational: some data has been collected and subsequently algorithms are run on it.  (Online supervised learning doesn't necessarily work this way but mostly online techniques have been used for computational reasons after data collection.)  In contrast counterfactual learning is very difficult do to observationally.  Diverse fields such as economics political science and epidemiology all attempt to make counterfactual conclusions using observational data essentially because this is the only data available (at an affordable cost).  When testing a new medicine however the standard is to run a controlled experiment because with control over the data collection more complicated conclusions can be made with higher confidence.Analogously reinforcement learning is best done &ldquo;in the loop&rdquo; with the algorithm controlling the collection of data which is used for learning.  Because of this a pure library implementation of a reinforcement learning algorithm is unnatural because of the requisite state management.  For example rewards occur after actions are taken and these need to be ultimately associated with each other for learning.  (One of my first jobs was at a sponsored search company called Overture and maintaining the search-click join was the full time job of a dozen engineers: note this was merely an immediate join for a singleton session!)Ergo packaging reinforcement learning as a service makes more sense.  This facilitates distributed coordination of the model updates the serving (exploration) distribution and the data collection.  This scenario is a natural win for cloud computing providers.  However in practice there needs to be an offline client mode (e.g. for mobile and IOT applications); furthermore this capability would be utilized even in a pure datacenter environment because of low latency decision requirements.  (More generally there would probably be a &ldquo;tiered learning&rdquo; architecture analogous to the tiered storage architectures utilized in cloud computing platforms.  Brendan McMahan has been thinking along these lines under the rubric of federated learning.)Bootstrapping is everything It is amazing how clarifying it is to try and solve and actual problem.  I now appreciate that reinforcement learning has been oversold a bit.  In particular the sample complexity requirements for reinforcement learning are quite high.  (That's fancy talk for saying it takes a lot of data to converge.)  When you are working in a simulated environment that's not such a concern because you have the equivalent of infinite training data so we see dramatic results in simulated environments.  When reinforcement learning is done on live traffic with real users you have less data than you think because you always start with a test fraction of data and you don't get more until you are better (catch 22).  So I actually spend a lot of my time developing initial serving policies unfortunately somewhat idiosyncratically: imitation learning can be great with the right data assets but heuristic strategies are also important.  I suspect initialization via not-smartly-initialized-RL in a simulated environment is another possibility (in dialog simulators aren't so good so I haven't leveraged this strategy yet).This creates some design questions for RL as a service.   Assuming there is an initial serving policy how do I specify it?  In the decision service you pass in the action that the initial serving policy would take which is fine for contextual bandits but for a multi-step epoch this could be cumbersome because the initial serving policy needs to maintain state.  It would make sense for the service to make it easier to manage this. How does the service help me put together the initial serving policy?  Considering my experience so far here are some possible ways to develop an initial serving policy: An arbritrary program (``heuristic'').  Sometimes this is the easiest way to cold start or this might be the current ``champion'' system. Imitation learning.  Assumes suitable data assets are available. Off-policy learning from historical data.  This can be better than imitation learning if the historical policy was suitably randomized (e.g. the exhaust of previous invocations of RL as a service). Boostrapping via simulation.  In dialog this doesn't seem viable but if a good simulator is available (e.g. robotics and game engines?) this could be great.  Furthermore this would involve direct reuse of the platform albeit on generated data.Language is the UI of programming I think ideas from credit-assignment compilation would not only address the question of how to specify the initial policy but also provide the most natural interface for utilizing RL a service.  I'll do another post exploring that.
2017,1,13,Generating Text via Adversarial Training,http://www.machinedlearnings.com/2017/01/generating-text-via-adversarial-training.html,There was a really cute paper at the GAN workshop this year Generating Text via Adversarial Training by Zhang Gan and Carin.  In particular they make a couple of unusual choices that appear important.  (Warning: if you are not familiar with GANs this post will not make a lot of sense.)They use a convolutional neural network (CNN) as a discriminator rather than an RNN.  In retrospect this seems like a good choice e.g. Tong Zhang has been crushing it in text classification with CNNs.  CNNs are a bit easier to train than RNNs so the net result is a powerful discriminator with a relatively easy optimization problem associated with it.They use a smooth approximation to the LSTM output in their generator but actually this kind of trick appears everywhere so isn't so remarkable in isolation.They use a pure moment matching criterion for the saddle point optimization (estimated over a mini-batch).  GANs started with a pointwise discrimination loss and more recent work has augmented this loss with moment matching style penalties but here the saddle point optimization is pure moment matching.  (So technically the discriminator isn't a discriminator.  They actually refer to it as discriminator or encoder interchangeably in the text this explains why.)They are very smart about initialization.  In particular the discriminator is pre-trained to distinguish between a true sentence and the same sentence with two words swapped in position.  (During initialization the discriminator is trained using a pointwise classification loss).  This is interesting because swapping two words preserves many of the $n$-gram statistics of the input i.e. many of the convolutional filters will compute the exact same value.  (I've had good luck recently using permuted sentences as negatives for other models now I'm going to try swapping two words.)They update the generator more frequently than the discriminator which is counter to the standard folklore which says you want the discriminator to move faster than the generator.  Perhaps this is because the CNN optimization problem is much easier than the LSTM one; the use of a purely moment matching loss might also be relevant.The old complaint about neural network papers was that you couldn't replicate them.  Nowadays it is often easier to replicate neural network papers than other papers because you can just fork their code on github and run the experiment.  However I still find it difficult to ascertain the relative importance of the various choices that were made.  For the choices enumerated above: what is the sensitivity of the final result to these choices?  Hard to say but I've started to assume the sensitivity is high because when I have tried to tweak a result after replicating it it usually goes to shit.  (I haven't tried to replicate this particular result yet.)Anyway this paper has some cool ideas and hopefully it can be extended to generating realistic-looking dialog.
2016,12,17,On the Sustainability of Open Industrial Research,http://www.machinedlearnings.com/2016/12/on-sustainability-of-open-industrial.html,I'm glad OpenAI exists: the more science the better!  Having said that there was a strange happenstance at NIPS this year.  OpenAI released OpenAI universe which is their second big release of a platform for measuring and training counterfactual learning algorithms.  This is the kind of behaviour you would expect from an organization which is promoting the general advancement of AI without consideration of financial gain.  At the same time Google Facebook and Microsoft all announced analogous platforms.  Nobody blinked an eyelash at the fact that three for-profit organizations were tripping over themselves to give away basic research technologies.A naive train of thought says that basic research is a public good subject to the free-rider problem and therefore will be underfunded by for-profit organizations.  If you think this is a strawman position you haven't heard of the Cisco model for innovation.  When this article was written:&hellip;Cisco has no &ldquo;pure&rdquo; blue-sky research organization.  Rather when Cisco invests research dollars it has a specific product in mind.  The company relies on acquisitions to take the place of pure research &hellip;Articles like that used to worry me alot.  So why (apparently) is this time different?Factor 1: Labor Market ScarcityInformal discussions with my colleagues generally end up at this explanation template.  Specific surface forms include:&ldquo;You can't recruit the best people without good public research.&rdquo;  Facially I think this statement is true but the logic is somewhat circular.  You certainly can't recruit the best researchers without good public research but why do you want them in the first place?  So is the statement more like &ldquo;With good public research you can recruit the best people and then convince them to do some non-public research.&rdquo; (?) Alot of grad students do seem to graduate and then &ldquo;disappear&rdquo; so there is probably some truth to this.&ldquo;The best people want to publish: it's a perk that you are paying them.&rdquo; Definitely getting public recognition for your work is rewarding and it makes total sense for knowledge workers to want to balance financial capital and social capital.  Public displays of competence are transferable to a new gig for instance.  But this line of thought assumes that public research is a cost for employers that they chose to pay in lieu of e.g. higher salaries.I not only suspect this factor is only part of the picture: I strongly hope that it is only part of the picture.  Because if it is the whole picture as soon as the labor market softens privately funded public research will experience a big pullback which would suck.Factor 2: Positive ExternalitiesThis argument is: &ldquo;researchers improve the productivity of those nearby such that it is worth paying them just to hang out.&rdquo; In this line of thinking even a few weeks lead time on the latest ideas plus the chance to talk in person with thought leaders in order to explain the nuances of the latest approaches is worth their entire salary.   There is some truth to this e.g. Geoffrey Hinton performed some magic for the speech team here back in the day.  The problem I have with this picture is that in practice it can be easier to communicate and collaborate with somebody across the planet than with somebody downstairs.  It's also really hard to measure so if I had to convince the board of directors to fund a research division based upon this I think I would fail.This is another favorite argument that comes up in conversation by the way.  It's funny to hear people characterize the current situation as &ldquo; we're scarce and totally awesome.&rdquo;  As Douglas Adams points out there is little benefit to having a sense of perspective.Factor 3: Quality AssuranceThe idea here is basically &ldquo;contributing to the public research discussion ensures the high quality of ideas within the organization.&rdquo;  The key word here is contributing as the alternative strategy is something more akin to free-riding e.g. sending employees to conferences to attend but not contribute.There is definite value in preparing ideas for public consumption.  Writing the related work section of a paper is often an enlightening experience although honestly it tends to happen after the work has been done rather than before.  Before is more like a vague sense that there is no good solution to whatever the problem is hopefully informed by a general sense of where the state-of-the-art is.  Writing the experiment section in my experience is more of a mixed bag: you often need to dock with a standard metric or benchmark task that seems at best idiosyncratic and at worst unrelated to the thrust of your work and therefore forcing particular hacks to get over the finish line.  (Maybe this is why everybody is investing so heavily in defining the next generation of benchmark tasks.)The funny thing is most of the preceeding benefits occur during the preparation for publication.  Plausibly at that point you could throw the paper away and still experience the benefits (should we call these &ldquo;the arxiv benefits&rdquo;?).  Running the reviewer gauntlet is a way of measuring whether you are doing quality work but it is a noisy signal.  Quality peer feedback can suggest improvements and new directions but is a scarce resource.  Philanthropic organizations that want to advance science should attack this scarcity e.g. by funding high quality dedicated reviewers or inventing a new model for peer feedback.I don't find this factor very compelling as a rationale for funding basic research i.e. if I were the head of a research department arguing for funding from the board of directors I wouldn't heavily leverage this line of attack.  Truth is less important than perception here and I think the accounting department would rather test the quality of their ideas in the marketplace of products.Factor 4: MarketingA company can use their basic research accolades as a public display of the fitness and excellence of their products.  The big players definitely make sure their research achievements are discussed in high profile publications such as the New York Times.  However this mostly feels like an afterthought to me.  What seems to happen is that researchers are making their choices on what to investigate some of it ends up being newsworthy and another part of the organization has dedicated individuals whose job it is to identify and promote newsworthy research.  IBM is the big exception e.g. Watson going after Jeopardy.  This is arguably sustainable (IBM has been at it for a while) but it creates activity that looks like big pushes around specific sensational goals rather than distribution of basic research tools and techniques.  In other words it doesn't look like what was happening at this year's NIPS.Factor 5: MonopoliesI find this explanation agreeable: that technology has created more natural monopolies and natural monopolies fund research c.f. Bell Labs and Xerox PARC.  All market positions are subject to disruption and erosion but Microsoft Google and Facebook all have large competitive moats in their respective areas (OS search and social) so they are currently funding public basic research.  This factor predicts that as Amazon's competitive moats in retail (and cloud computing) widen they will engage in more public basic research something we have seen recently.For AI (née machine learning) in particular the key monopoly is data (which derives from customer relationships).  Arguably the big tech giants would love for AI technologies to be commodities because they would then be in the best position to exploit such technologies due to their existing customer relationships.  Conversely if a privately discovered disruptive AI technology were to emerge it would be one of the &ldquo;majors&rdquo; being disrupted by a start-up.  So the major companies get both benefits and insurance from a vibrant public research ecosystem around AI.Nonetheless a largish company with a decent defensive moat might look at the current level of public research activity and say &ldquo;hey good enough let's free ride.&rdquo; (Not explicitly perhaps but implicitly).  Imagine you are in charge of Apple or Salesforce what do you do?  I don't see a clear &ldquo;right answer&rdquo; although both companies appear to be moving in the direction of more open basic research.Factor 6: Firms are IrrationalTech firms are ruled by founder-emperors whose personal predilections can decide policies such as whether you can bring a dog to work.  The existence of a research department with a large budget in practice can be similarly motivated.  All the above factors are partially true but difficult to measure so it comes down to a judgement call and as long as a company is kicking ass deference for the founder(s) will be extreme.  If this factor is important however then when the company hits a rough patch or experiences a transition at the top things can go south quickly.  There have been examples of that in the last 10 years for sure.
2016,12,16,Dialogue Workshop Recap,http://www.machinedlearnings.com/2016/12/dialogue-workshop-recap.html,Most of the speakers have sent me their slides which can be found on the schedule page.  Overall the workshop was fun and enlightening.  Here are some major themes that I picked up upon.Evaluation There is no magic bullet but check out Helen's slides for a nicely organized discussion of metrics.  Many different strategies were on display in the workshop:Milica Gasic utilized crowdsourcing for some of her experiments.  She also indicated the incentives of crowdsourcing can lead to unnatural participant behaviours.Nina Dethlefs used a combination of objective (BLEU) and subjective (“naturalness”) evaluation.Vlad Serban has been a proponent of next utterance classification as a useful intrinsic metric.Antoine Bordes (and the other FAIR folks) are heavily leveraging simulation and engineered tasks.Jason Williams used imitation metrics (from hand labeled dialogs) as well as simulation.As Helen points out computing metrics from customer behaviour is probably the gold standard for industrial task-oriented systems but this is a scarce resource.  (Even within the company that has the customer relationship by the way: at my current gig they will not let me flight something without demonstrating limited negative customer experience impact.)Those who have been around longer than I have experienced several waves of enthusiasm and pessimism regarding simulation for dialogue.  Overall I think the takeaway is that simulation can be useful tool as long as one is cognizant of the limitations.Antoine quickly adapted his talk to Nina's with a fun slide that said “Yes Nina we are bringing simulation back.”  The FAIR strategy is something like this: “Here are some engineered dialog tasks that appear to require certain capabilities to perform well such as multi-hop reasoning interaction with a knowledge base long-term memory etc.  At the moment we have no system that can achieve 100% accuracy on these engineered tasks so we will use these tasks to drive research into architectures and optimization strategies.  We also monitor performance other external tasks (e.g. DSTC) to see if our learning generalizes beyond the engineered task set.”  Sounds reasonable.Personally as a result of the workshop I'm going to invest more heavily in simulators in the near-term.Leveraging Linguistics Fernando Pereira had the killer comment about how linguistics is a descriptive theory which need not have explicit correspondence to implementation: &ldquo;when Mercury goes around the Sun it is not running General Relativity.&rdquo;  Nonetheless linguistics seems important not only for describing what behaviours a competent system must capture but also for motivating and inspiring what kinds of automata we need to achieve it.  Augmenting or generating data sets seems like a natural way to leverage lingustics.  As an example in the workshop I learned that 4 year old native English speakers are sensitive to proper vs. improper word order given simple sentences containing some nonsense words (but with morphological clues such as capitalization and -ed suffix).  Consequently I'm trying a next utterance classification run on a large dialog dataset where some of the negative examples are token-permuted versions of the true continuation to see if this changes anything.Raquel Fernandez's talk focused on adult-child language interactions and I couldn't help but think about potential relevance to training artificial systems.  In fact current dialog systems are acting like the parent (i.e. the expert) e.g. by suggesting reformulations to the user.  But this laughable because our systems are stupid: shouldn't we be acting like the child?The most extreme use of linguistics was the talk by Eshghi and Kalatzis where they develop a custom incremental semantic parser for dialog and then use the resulting logical forms to drive the entire dialog process.  Once the parser is built the amount of training data required is extremely minimal but the parser is presumably built from looking at a large number of dialogs.Nina Dethlefs discussed some promising experiments with AMR.  I've been scared of AMR personally.  First it is very expensive to get the annotations.  However if that were the only problem we could imagine a human-genome-style push to generate a large number of them.  The bigger problem is the relatively poor inter-annotator agreement (it was just Nina and her students so they could come to agreement via side communication).  Nonetheless I could imagine a dialog system which is designed and built using a small number of prototypical semantic structures.  It might seem a bit artificial and constrained but so does the graphical user interface with the current canonical set of UX elements which users learn to productivity interact with.Angeliki Lazaridou's talk reminded me that communication is fundamentally a cooperative game which explains why arguing on the internet is a waste of time. Neural Networks: Game Changer?  I asked variants of the following question to every panel: &ldquo;what problems have neural networks mitigated and what problems remain stubbornly unaddressed.&rdquo;  This was essentially the content of Marco Baroni's talk.  Overall I would say: there's enthusiasm now that we are no longer afraid of non-convex loss functions (along these lines check out Julien Perez's slides).However we currently have only vague ideas on how to realize the competencies that are apparently required for high quality dialog.  I say apparently because the history of AI is full of practitioners assuming sufficient capabilities are necessary for some task and recent advances in machine translation suggest that savant-parrots might be able to do surprisingly well.  In fact during the discussion period there was some frustration that heuristic hand-coded strategies are still superior to machine learning based approaches with the anticipation that this may continue to be true for the Alexa prize.  I'm positive about the existence of superior heuristics however: not only do they provide a source of inspiration and ideas for data-driven approaches but learning methods that combine imitation learning and reinforcement learning should be able to beneficially exploit them.Entity Annotation Consider the apparently simple and ubiquitous feature engineering strategy: add additional sparse indicator features which indicate semantic equivalence of tokens or token sequences.  So maybe “windows 10” and “windows anniversary edition” both get the same feature.  Jason Williams indicated his system is greatly improved by this but he's trying to learn from $O(10)$ labeled dialogues so I nodded.  Antoine Bordes indicated this helps on some bAbI dialog tasks but those tasks only have $O(1000)$ dialogues so again I nodded.  Then Vlad Serban indicated this helps for next utterance classification on the Ubuntu Dialog Corpus.  At this point I thought &ldquo;wait that's $O(10^5)$ dialogs.&rdquo;Apparently knowing a turtle and a tortoise are the same thing is tricky.In practice I'm ok with manual feature engineering: it's how I paid the rent during the linear era.  But now I wonder: does it take much more data to infer such equivalences?  Will we never infer this no matter how much data given our current architectures?Spelling The speakers were roughly evenly split between “dialog” and “dialogue”.  I prefer the latter as it has more panache.
2016,12,12,NIPS 2016 Reflections,http://www.machinedlearnings.com/2016/12/nips-2016-reflections.html,It was a great conference.  The organizers had to break with tradition to accommodate the rapid growth in submissions and attendance but despite my nostalgia I feel the changes were beneficial.  In particular leveraging parallel tracks and eliminating poster spotlights allowed for more presentations while ending the day before midnight and the generous space allocation per poster really improved the poster session.  The workshop organizers apparently thought of everything in advance: I didn't experience any hiccups (although we only had one microphone so I got a fair bit of exercise during discussion periods).Here are some high-level themes I picked up on.Openness.  Two years ago Amazon started opening up their research and they are now a major presence at the conference.  This year at NIPS Apple announced they would be opening up their research practices.  Clearly companies are finding it in their best interests to fund open basic research which runs counter to folk-economic reasoning that basic research appears to be a pure public good and therefore will not be funded privately due to the free-rider problem.  A real economist would presumably say that is simplistic undergraduate thinking.  Still I wonder to what extent are companies being irrational?  Conversely what real-world aspects of basic research are not well modeled as a public good?  I would love for an economist to come to NIPS to give an invited talk on this issue.  Simulation.  A major theme I noticed at the conference was the use of simulated environments.  One reason was articulated by Yann LeCun during his opening keynote: (paraphrasing) ``simulation is a plausible strategy for mitigating the high sample complexity of reinforcement learning.''  But another reason is scientific methodology: for counterfactual scenarios simulated environments are the analog of datasets in that they allow for a common metric reproducible experimentation and democratization of innovation.  Simulators are of course not new and have had waves of enthusiasm and pessimism in the past and there are a lot of pitfalls which basically boil down to overfitting the simulator (both in a micro sense of getting a bad model but also in a macro sense of focusing scientific attention on irrelevant aspects of a problem).  Hopefully we can learn from the past and be cognizant of the dangers.  There's more than a blog post worth of content to say about this but here are two things I heard at the dialog workshop along these lines: first Jason Williams suggested that relative performance conclusions based upon simulation can be safe but that absolute performance conclusions are suspect; and second Antoine Bordes advocated for using an ensemble of realizable simulated problems with dashboard scoring (i.e. multiple problems for which perfect performance is possible which exercise apparently different capabilities and for which there is currently no single approach that is known to handle all the problems).Without question simulators are proliferating.  I noticed the following discussed at the conference this year: GVGAI  CommAI-env  Project Malmo  OpenAI universe  DeepMind Lab and I probably missed some others.By the way the alternatives to simulation aren't perfect either: some of the discussion in the dialogue workshop was about how the incentives of crowdsourcing induces unnatural behaviour in participants of crowdsourced dialogue experiments.GANs The frenzy of GAN research activity from other conferences (such as ICLR) colonized NIPS in a big way this year. This is related to simulation albeit more towards the mitigating-sample-complexity theme than the scientific-methodology theme.  The quirks of getting the optimization to work are being worked out which should enable some interesting improvements in RL in the near-term (in addition to many nifty pictures).  Unfortunately for NLU tasks generating text from GANs is currently not as mature as generating sounds or images but there were some posters addressing this.Interpretable Models The idea that model should be able to &ldquo;explain itself&rdquo; is very popular in industry but this is the first time I have seen interpretability receive significant attention at NIPS. Impending EU regulations have certainly increased interest in the subject.  But there are other reasons as well: as Irina Rish pointed out in her invited talk on (essentially) mindreading recent advances in representation learning could better facilitate scientific inquiry if the representations were more interpretable.Papers I noticedWould you trust a single reviewer on yelp?  I wouldn't.  Therefore I think we need some way to crowdsource what people thought were good papers from the conference.  I'm just one jet-lagged person with two eyeballs (btw use bigger font people!  it gets harder to see the screen every year &hellip;) plus everything comes out on arxiv first so if I read it already I don't even notice it at the conference.  That makes this list weird but here you go.Generating Text via Adversarial Training GANS for Sequences of Discrete Elements with the Gumbel-softmax Distribution and Adversarial Evaluation of Dialogue Models.   I'm interested in techniques that are relevant to simulating or evaluating dialogue systems.Building Machines That Learn and Think Like People.  The talk was great so I want to dig into the paper.   The talk explored how humans are leveraging lots of priors that we probably want to build into our systems with some specific observations resulting in actionable research directions.  (This appears relevant to dialog since this line of research might explain the pseudo-intelligibility of statements like &ldquo; the blorf flazzed the peezul.&rdquo;)Learning values across many orders of magnitude.  At first blush this might appear to be optimization minutae but this problem is pervasive in counterfactual setups; and I'm a big fan of scale invariance as a useful prior.Reward Augmented Maximum Likelihood for Neural Structured Prediction.  If you squint this reads as another way to use a world model to mitigate the sample complexity of reinforcement learning (e.g. what if edit distance was just the initial model of the reward?).Safe and Efficient Off-Policy Reinforcement Learning.  This is an important setting.  The particular adjustment is reminiscent of a previously proposed estimator in this area but nonetheless this is interesting.Also this paper was not at the conference as far as I know but I found out about it during the coffee break and it's totally awesome:Understanding deep learning requires rethinking generalization.  TL;DR: convnets can shatter the standard image training sets when the pixels are permuted or even randomized!  Of course generalization is poor in this case but it indicates they are way more flexible than their &ldquo;local pixel statistics composition&rdquo; architecture suggests.  So why do they work so well?
2016,12,3,Learning Methods for Dialog Workshop at NIPS This Saturday,http://www.machinedlearnings.com/2016/12/learning-methods-for-dialog-workshop-at.html,The schedule for the workshop has been finalized and I'm pretty excited.  We managed to convince some seasoned researchers in dialog who don't normally attend NIPS to give invited talks.  We're also devoting some time to &ldquo;Building Complete Systems&rdquo; because it's easy to focus on the trees instead of the forest especially when the tree is something really interesting like a neural network trained on a bunch of GPUs.  But don't worry there's plenty of &ldquo;NIPS red meat&rdquo; in the schedule as well.See you on Saturday!
2016,9,20,NIPS dialogue workshop,http://www.machinedlearnings.com/2016/09/nips-dialogue-workshop.html,I'm co-organizing a workshop on dialogue at NIPS 2016.  NIPS is not a traditional forum for dialogue research but there are increasing number of people (like myself!) in machine learning who are becoming interested in dialogue so the time seemed right.  From a personal perspective dialogue is interesting because 1) it smells like AI 2) recent advances in (deep learning) NLP techniques suggest the problem is more tractable and 3) corporate interest means both money and data will be plentiful.  Honestly the first point is very important: it was impossible to explain to my kids the minutiae on which I previously worked whereas now I can show them videos like this.  However there are a lot of issues in dialogue that aren't going to be demolished merely by using a flexible hypothesis class so I felt the need to educate myself about the activities of veteran dialogue researchers and the best way to ensure that was to organize a workshop and invite some of them.Hopefully you'll join the conversation.
2016,7,8,Update on dialogue progress,http://www.machinedlearnings.com/2016/07/update-on-dialogue-progress.html,In a recent blog post I discussed two ideas for moving dialogue forward; both ideas are related to the need to democratize access to the data required to evaluate a dialog system.  It turns out both ideas have already been advanced to some degree: Having computers &ldquo;talk&rdquo; to each other instead of with people: Marco Beroni is on it. Creating an open platform for online assessment: Maxine Eskenazi is on it.This is good to see.
2016,7,5,ICML 2016 Thoughts,http://www.machinedlearnings.com/2016/07/icml-2016-thoughts.html,ICML is too big for me to ``review'' it per se but I can provide a myopic perspective.The heavy hitting topics were Deep Learning Reinforcement Learning and Optimization; but there was a heavy tail of topics receiving attention.  It felt like deep learning was less dominant this year; but the success of deep learning has led to multiple application specific alternative venues (e.g. CVPR EMNLP) and ICLR is also a prestigious venue; so deep learning at ICML this year was heavyweight in either the more theoretical or multimodal works.  Arguably reinforcement learning and optimization both should partially count towards deep learning's footprint; reinforcement learning has been this way for at least a year but optimization has recently developed more interest in non-convex problems especially the kind that are empirically tractable in deep learning (sometimes although seemingly innocuous architecture changes can spoil the pudding; I suppose one dream of the optimization community would be the identification of a larger-than-convex class of problems which are still tractable to provide guidance).Here are some papers I liked:Strongly-Typed Recurrent Neural NetworksThe off-putting title makes sense if you are into type theory or if you've ever been a professional Haskell programmer and have had to figure out wtf a monad is.  tl;dr: if you put units of measurement on the various components of a recurrent neural network you'll discover that you are adding apples and oranges.  T-LSTM a modification of the standard LSTM to fix the problem behaves similarly empirically; but is amenable to analysis.  Theorem 1 was the nice part for me: the modified architectures are shown to compute temporal convolutions with dynamic pooling.  Could type consistency provide a useful prior on architectures?  That'd be a welcome development.Ask Me Anything:Dynamic Memory Networks for Natural Language Processing and Dynamic Memory Networks for Visual and Textual Question AnsweringMore titles I'm not over the moon about: everybody seems to be equating &ldquo;memory&rdquo; = &ldquo;attention over current example substructure&rdquo;.  If you ask for the layperson's definition they would say that memory is about stuff you can't see at the moment (note: Jason started this particular abuse of terminology with End-to-End Memory Networks).  Pedantry aside undeniably these iterated attention architectures have become the state of the art in question-answering style problems and the baseline to beat.  Note since the next step in iterated attention is to incorporate previously seen and stored examples the use of the term &ldquo;memory&rdquo; will soon become less objectionable.From Softmax to Sparsemax:A Sparse Model of Attention and Multi-Label Classification This is an alternative to the softmax layer (&ldquo;link function&rdquo;) used as the last layer of a neural network.  Softmax maps $\mathbb{R}^n$ onto the (interior of the) simplex whereas sparsemax projects onto the simplex.  One big difference is that sparsemax can &ldquo;hit the corners&rdquo; i.e. zero out some components.  Empirically the differences in aggregate task performance when swapping softmax with sparsemax are modest and attributable to the selection pressures on experimental sections. So why care?  Attention mechanisms are often implemented with softmax and it is plausible that a truly sparse attention mechanism might scale better (either computationally or statistically) to larger problems (such as those involving actual memory c.f. previous paragraph). Guided Cost Learning: Deep Inverse Optimal Control via Policy OptimizationI find Inverse RL unintuitive: didn't Vapnik say not to introduce difficult intermediate problems?  Nonetheless it seems to work well.  Perhaps requiring the learned policy to be &ldquo;rational&rdquo; under some cost function is a useful prior which mitigates sample complexity?  I'm not sure I have to noodle on it.  In the meantime cool videos of robots doing the dishes!Dueling Network Architectures for Deep Reinforcement Learning.Best paper so I'm not adding any value by pointing it out to you.  However after reading it meditate on why learning two things is better than learning one.  Then re-read the discussion section.  Then meditate on whether a similar variance isolation trick applies to your current problem.From the workshops some fun stuff I heard:Gerald Tesauro dusted off his old Neurogammon code ran it on a more powerful computer (his current laptop) and got much better results.  Unfortunately we cannot conclude that NVIDIA will solve AI for us if we wait long enough.  In 2 player games or in simulated environments more generally computational power equates to more data resources because you can simulate more.  In the real world we have sample complexity constraints: you have to perform actual actions to get actual rewards.  However in the same way that cars and planes are faster than people because they have unfair energetic advantages (we are 100W machines; airplanes are much higher) I think &ldquo;superhuman AI&rdquo; should it come about will be because of sample complexity advantages i.e. a distributed collection of robots that can perform more actions and experience more rewards (and remember and share all of them with each other).  So really Boston Dynamics not NVIDIA is the key to the singularity.  (In the meantime &hellip; buy my vitamins!)Ben Recht talked about the virtues of random hyperparameter optimization and an acceleration technique that looks like a cooler version of sub-linear debugging.  This style in my experience works.Leon Bottou pointed out that first order methods are now within constant factors of optimal convergence with the corollary that any putative improvement has to be extremely cheap to compute since it can only yield a constant factor.  He also presented a plausible improvement on batch normalization in the same talk.
2016,6,26,Accelerating progress in dialogue,http://www.machinedlearnings.com/2016/06/accelerating-progress-in-dialogue.html,In machine learning assessment isn't everything: it's the only thing.  That's the lesson from Imagenet (a labeled data set) and the Arcade Learning Environment (a simulation environment).  A simulator is the partial feedback analog of a labeled data set: something that lets any researcher assess the value of any policy.  Like data sets when simulators are publicly available and the associated task is well designed useful scientific innovation can proceed rapidly.In dialogue systems partial feedback problems abound: anyone who has ever unsuccessfully tried to get a job has considered the counterfactual: &ldquo;what if I had said something different?&rdquo;  Such questions are difficult to answer using offline data yet anybody trying to offline assess a dialogue system has to come up with some scheme for doing so and there are pitfalls.Online evaluation has different problems.  In isolation it is ideal; but for the scientific community at large it is problematic.  For example Honglak Lee has convinced the registrar of his school to allow him to deploy a live chat system for recommending course registrations.  This is a brilliant move on his part analogous to getting access to a particle accelerator in the 1940s: he'll be in a position to discover interesting stuff first.  But he can't share this resource broadly because 1) there are a finite number of chats and 2) the registrar presumably wants to ensure a quality experience.  Similar concerns underpin the recent explosion of interest in dialogue systems in the tech sector: companies with access to live dialogues are aware of the competitive moat this creates and they need to be careful in the treatment of their customers.That's fine and I like getting a paycheck but: how fast would reinforcement learning be advancing if the Arcade Learning Environment was only available at the University of Alberta?So here are some ideas.First we could have agents talk with each other to solve a task without any humans involved.  Perhaps this would lead to the same rapid progress that has been observed in 2 player games.  Arguably we might learn more about ants than people from such a line of research.  However with the humans out of the loop we could use simulated environments and democratize assessment.  Possibly we could discover something interesting about what it takes to learn to repeatedly communicate information to cooperate with another agent.Second we could make a platform that democratizes access to an online oracle.  Since online assessment is a scarce resource it would have to cost something but imagine: suppose we decide task foo is important.  We create a standard training program to create skilled crowdsource workers plus standard HITs which constitute the task quality control procedures etc.  Then we try as hard as possible to amortize these fixed costs across all researchers by letting anyone assess any model in the framework paying only the marginal costs of the oracle.  Finally instead of just doing this for task foo we try to make it easy for researchers to create new tasks as well.  To some degree the crowdsourcing industry does this already (for paying clients); and certainly researchers have been leveraging crowdsourcing extensively.  The question is how we can make it easier to 1) come up with reliable benchmark tasks that leverage online assessment and then 2) provide online access for every researcher at minimum cost.  Merely creating a data set from the crowdsourced task is not sufficient as it leads to the issues of offline evaluation.Of course it would be great for the previous paragraph if the task was not crowdsourced but some natural interactive task that is happening all the time at such large volume that the main issue is democratizing access.  One could imagine e.g. training on all transcripts of car talk and building a dialogue app that tries to diagnose car problems.  If it didn't totally suck people would not have to be paid to use it and it could support some level of online assessment for free.  Bootstrapping that however would itself be a major achievement.
2016,4,7,Thoughts on reviewing,http://www.machinedlearnings.com/2016/04/thoughts-on-reviewing.html,During ICML reviews I noticed that my personal take on reviewing is becoming increasingly distinct from my peers.  Personally I want to go to a conference and come away with renewed creativity and productivity.  Thus I like works that are thought provoking groundbreaking or particularly innovative; even if the execution is a bit off.  However I suspect most reviewers feel that accepting a paper is a validation of the quality and potential impact of the work.  There's no right answer here as far as I can tell.  Certainly great work should be accepted and presented but the problem is there really isn't that much of it per unit time.  Therefore like a producer on a Brittany Spears album we are faced with the problem of filling in the rest of the material.  The validation mindset leads to the bulk of accepted papers being extremely well executed marginal improvements.  It would be nice if the mix were tilted more towards the riskier novel papers.The validation mindset leads to reviews that are reminiscent of food critic reviews.  That might sound objectionable given that food quality is subjective and science is about objective truth: but the nips review experiment suggests that the ability of reviewers to objectively recognize the greatness of a paper is subjectively overrated.  Psychologists attempting to &ldquo;measure&rdquo; mental phenomena have struggled formally with the question of &ldquo;what is a measurement&rdquo; and lack of inter-rater reliability is a bad sign (also: test-retest reliability is important but it is unclear how to assess this as the reviewers will remember a paper).  So I wonder: how variable are the reviews among food critics for a good restaurant relative to submitted papers to a conference?  I honestly don't know the answer.What I do know is that while I want to be informed I also want to be inspired.  That's why I go to conferences.  I hope reviewers will keep this in mind when they read papers.
2016,1,31,The Future has more Co-authors,http://www.machinedlearnings.com/2016/01/the-future-has-more-co-authors.html,Here's something to noodle on while you finalize your ICML submissions.Have you ever heard of Max Martin?  You probably haven't which is something considering he (currently) has 21 #1 hits in the United States.  Lennon (26) and McCartney (32) have more but Max Martin has the advantage of still being alive to catch up.  A phenomenal genius right?  Well yes but if you look at his material he always has co-authors usually several.  His process is highly collaborative as he manages a constellation of young songwriting talent which he nurtures like a good advisor does grad students and post-docs.  In the increasingly winner-take-all dynamics of pop music it's better to write a #1 song with 5 people then to write a #20 song by yourself. I think Machine Learning is headed in this direction.  Already in Physics pushing the envelope experimentally involves an astonishing number of co-authors.  Presumably Physics theory papers have fewer co-authors but since the standard model is too damn good in order to make real progress some amazingly difficult experimental work is required. Now consider an historic recent achievement: conquering Go.  That paper has 20 authors.  Nature papers are a big deal so presumably everybody is trying to attribute fairly and this leads to a long author list: nonetheless there is no denying that this achievement required many people working together with disparate skills.   I think the days where Hastie and Tibshirani can just crush it by themselves like Lennon and McCartney in their day are over.  People with the right theoretical ideas to move something forward in e.g. reinforcement learning are still going to need a small army of developers and systems experts to build the tools necessary.So here's some advice to any young aspiring academics out there envisioning a future Eureka moment alone at a white-board: if you want to be relevant pair up with as many talented people as you can.
2016,1,12,Attention: More Musings,http://www.machinedlearnings.com/2016/01/attention-more-musings.html,The attention model I posed last post is still reasonable but the comparison model is not.  (These revelations are the fallout of a fun conversation with myself Nikos and Sham Kakade.  Sham recently took a faculty position at the University of Washington which is my neck of the woods.)As a reminder the attention model is a binary classifier which takes matrix valued inputs $X \in \mathbb{R}^{d \times k}$ with $d$ features and $k$ columns weights (&ldquo;attends&rdquo;) to some columns more than others via parameter $v \in \mathbb{R}^d$ and then predicts with parameter $u \in \mathbb{R}^d$ \[\begin{aligned}\hat y &= \mathrm{sgn \;} \left( u^\top X z \right) \\z &= \frac{\exp \left( v^\top X_i \right)}{\sum_k \exp \left (v^\top X_k \right) }.\end{aligned}\]  I changed the notation slightly from my last post ($w \rightarrow u$) the reasons for which will be clear shortly.  In the previous post the comparison model was an unconstrained linear predictor on all columns \[\begin{aligned}\hat y &= \mathrm{sgn \;} \left( w^\top \mathrm{vec\} (X) \right)\end{aligned}\] with $w \in \mathbb{R}^{d k}$.  But this is not a good comparison model because the attention model in nonlinear in ways this model cannot achieve: apples and oranges really.This is easier to see with linear attention and a regression task.  A linear attention model weights each column according to $(v^\top X_i)$ e.g. $(v^\top X_i)$ is close to zero for &ldquo;background&rdquo; or &ldquo;irrelevant&rdquo; stuff and is appreciably nonzero for &ldquo;foreground&rdquo; or &ldquo;relevant&rdquo; stuff.  In that case \[\begin{aligned}\hat y &= u^\top X (v^\top X)^\top = \mathrm{tr} \left( X X^\top v u^\top \right)\end{aligned}\] (using properties of the trace) which looks like a rank-1 assumption on a full model \[\begin{aligned} \hat y &= \mathrm{tr} \left( X X^\top W \right) = \sum_{ijk} X_{ik} W_{ij} X_{jk} \\%&= \sum_i \left( X X^\top W \right)_{ii} = \sum_{ij} \left( X X^\top \right) _{ij} W_{ji} \\%&= \sum_{ijk} X_{ik} X_{jk} W_{ji} = \sum_{ijk} X_{ik} X_{jk} W_{ij}\end{aligned}\] where $W \in \mathbb{R}^{d \times d}$ and w.l.o.g. symmetric.  (Now hopefully the notation change makes sense: the letters $U$ and $V$ are often used for the left and right singular spaces of the SVD.)  The symmetry of $W$ confuses me because it suggests $u$ and $v$ are the same (but then the prediction is nonnegative?) so clearly more thinking is required.  However this gives a bit of insight and perhaps this leads to some known results about sample complexity.
2021,6,15,What is a Composable Enterprise and Why You Need It?,https://www.mdmgeek.com/2021/06/15/what-is-a-composable-enterprise-and-why-you-need-it/,"Before we explore Composable Enterprise consider this scenario. Have you ever come across a situation where you realize your colleague is working on the same thing that you are? Perhaps it is a quarterly revenue report if you are looking at the business&#8217;s health. And my friends in technology you may be writing the exact [&#8230;]
The post What is a Composable Enterprise and Why You Need It? appeared first on MDMgeek."
2020,10,26,The Importance of Data Sharing in Organizations,https://www.mdmgeek.com/2020/10/26/the-importance-of-data-sharing-in-organizations/,"According to Gartner by 2023 organizations that promote data sharing will outperform their peers on most business value metrics. Gartner says data and analytics leaders who share data externally generate three times more measurable economic benefits than those who do not. The impact on business performance due to data sharing is real and organizations seem [&#8230;]
The post The Importance of Data Sharing in Organizations appeared first on MDMgeek."
2020,5,6,4 Ways to Keep Businesses Running During a Crisis,https://www.mdmgeek.com/2020/05/06/4-ways-to-keep-businesses-running-during-a-crisis/,"As business leaders we need to adjust to the circumstances of the rapidly changing world. COVID19 is impacting us in ways we never thought was possible. But there are things we can do to sustain the business and survive the impact of this pandemic. 
The post 4 Ways to Keep Businesses Running During a Crisis appeared first on MDMgeek."
2019,10,24,Make AI “Intelligent” Again,https://www.mdmgeek.com/2019/10/24/make-artificial-intelligence-intelligent-again/,"Today artificial intelligence and machine learning are hot once more. How can we get it right this time? I provide few tips.
The post Make AI “Intelligent” Again appeared first on MDMgeek."
2019,8,10,The Size of the Global Master Data Management Market,https://www.mdmgeek.com/2019/08/10/the-size-of-the-global-master-data-management-market/,"Master Data Management is a growing market. Every analyst has a way to estimate how large it is. Are they accurate in their estimation? What should we know?
The post The Size of the Global Master Data Management Market appeared first on MDMgeek."
2019,2,2,Chart of Accounts Domain in Master Data Management,https://www.mdmgeek.com/2019/02/02/chart-of-accounts-domain-in-master-data-management/,"A chart of accounts (COA) is a list of accounts used by an organization to define each class of items for which money is spent or received. It is used to organize the finances and segregate expenditures revenue assets and liabilities to give everyone a better understanding of the financial health of the organization. There [&#8230;]
The post Chart of Accounts Domain in Master Data Management appeared first on MDMgeek."
2018,6,26,"Hello GDPR, It’s Been a Month. How Are You Doing?",https://www.mdmgeek.com/2018/06/26/hello-gdpr-happy-birthday/,"It is about a month since General Data Protection Regulation(GDPR) came into effect across the European Union. It’s the most critical data privacy law thus far an 88-page monster translated into 26 different languages. When we summarize those pages GDPR on privacy requires companies to: Clearly state how they’re collecting and storing data about EU [&#8230;]
The post Hello GDPR It’s Been a Month. How Are You Doing? appeared first on MDMgeek."
2018,4,10,No Omnichannel Experiences? You Are Dead to Me,https://www.mdmgeek.com/2018/04/09/no-omnichannel-experiences-means-death/,"Harsh words? But I have a feeling I am not alone telling this when it comes to omnichannel experiences. Born in the early 80s I fall into a category called  Xennials &#8211; a micro-generation born on the cusp of the two generations between 1977 and 1985. This article has a lot of details and says as [&#8230;]
The post No Omnichannel Experiences? You Are Dead to Me appeared first on MDMgeek."
2017,11,21,Machine Learning – Next Big Step in Master Data Management,https://www.mdmgeek.com/2017/11/21/machine-learning-master-data-management/,"Over last 12 years I have seen Master Data Management help companies automate and improve data. It has helped companies take a strategic approach to managing data by removing processes that were mainly left manual and time-consuming for years. We have seen an exponential increase in volume and variety of data in last 5-6 years. [&#8230;]
The post Machine Learning &#8211; Next Big Step in Master Data Management appeared first on MDMgeek."
2017,5,9,Party Data Model in Master Data Management,https://www.mdmgeek.com/2017/05/08/party-data-model-in-master-data-management/,"Many MDM initiatives center around customer data. A standard definition used in the industry is “Party” and “Party Domain” is a shared phrase used amongst MDM practitioners. Data model design around party domain is a critical area to address during MDM. In this blog I share my observations and suggest best practices. Party usually has [&#8230;]
The post Party Data Model in Master Data Management appeared first on MDMgeek."
2021,5,21,The Akronomicon: an Extreme-Scale Leaderboard,https://nuit-blanche.blogspot.com/2021/05/the-akronomicon-extreme-scale.html,**&nbsp;Nuit Blanche is now on Twitter: @NuitBlog&nbsp;**As larger models seem to be providing more context and more ability for zero-shot learning&nbsp;Julien&nbsp;just created&nbsp;the Akronomicon: an Extreme-Scale Leaderboard featuring the world's largest Machine Learning Models. And yes LightOn is on that board for the moment!&nbsp;Want to contribute? https://github.com/lightonai/akronomicon&nbsp;&nbsp; Follow @NuitBlog&nbsp;or join the CompressiveSensing Reddit&nbsp;the Facebook page the Compressive Sensing group on&nbsp;LinkedIn&nbsp;&nbsp;or&nbsp;the Advanced Matrix Factorization group on&nbsp;LinkedInLiked this entry ? subscribe to Nuit Blanche's feed there's more where that came from.&nbsp;You can also subscribe to Nuit Blanche by Email.Other links:Paris Machine Learning:&nbsp;Meetup.com||@Archives||LinkedIn||Facebook|| @ParisMLGroup About&nbsp;LightOn:&nbsp;Newsletter ||@LightOnIO|| on LinkedIn || on CrunchBase || our BlogAbout myself:&nbsp;LightOn || Google Scholar || LinkedIn ||@IgorCarron ||Homepage||ArXiv
2021,4,28,Virtual Workshop: Conceptual Understanding of Deep Learning (May 17th 9am-4pm PST),https://nuit-blanche.blogspot.com/2021/04/virtual-workshop-conceptual.html,"**&nbsp;Nuit Blanche is now on Twitter: @NuitBlog&nbsp;**Just got an email from Rina PanigrahyHi IgorI am an algorithms researcher at Google (http://theory.stanford.edu/~rinap) and I am organizing this workshop on ""Conceptual Understanding of Deep Learning"" (details below). It's trying to understand the Brain/Mind as an algorithm from a mathematical/theoretical perspective. I believe that a mathematical/algorithmic approach for understanding the Mind is crucial and very much missing. I'd appreciate any help I can get with advertising this on your blog/mailing-lists/twitter.BestRinaHere is the invite:Please join us for a virtual Google workshop on “Conceptual Understanding of Deep Learning”When: May 17th 9am-4pm PST.Where: Live over YoutubeGoal: How does the Brain/Mind (perhaps even an artificial one) work at an algorithmic level? While deep learning has produced tremendous technological strides in recent decades there is an unsettling feeling of a lack of “conceptual” understanding of why it works and to what extent it will work in the current form. The goal of the workshop is to bring together theorists and practitioners to develop an understanding of the right algorithmic view of deep learning characterizing the class of functions that can be learned coming up with the right learning architecture that may (provably) learn multiple functions concepts and remember them over time as humans do theoretical understanding of language logic RL meta learning and lifelong learning.The speakers and panelists include Turing award winners Geoffrey Hinton Leslie Valiant and Godel Prize winner Christos Papadimitriou (full-details).Panel Discussion: There will also be a panel discussion on the fundamental question of “Is there a mathematical model for the Mind?”. We will explore basic questions such as “Is there a provable algorithm that captures the essential capabilities of the mind?” “How do we remember complex phenomena?” “How is a knowledge graph created automatically?” “How do we learn new concepts function and action hierarchies over time?” and “Why do human decisions seem so interpretable?”Twitter: #ConceptualDLWorkshop.Please help advertise on mailing-lists/blog-posts and Retweet.Hope to see you there!Rina Panigrahy&nbsp; Follow @NuitBlog&nbsp;or join the CompressiveSensing Reddit&nbsp;the Facebook page the Compressive Sensing group on&nbsp;LinkedIn&nbsp;&nbsp;or&nbsp;the Advanced Matrix Factorization group on&nbsp;LinkedInLiked this entry ? subscribe to Nuit Blanche's feed there's more where that came from.&nbsp;You can also subscribe to Nuit Blanche by Email.Other links:Paris Machine Learning:&nbsp;Meetup.com||@Archives||LinkedIn||Facebook|| @ParisMLGroup About&nbsp;LightOn:&nbsp;Newsletter ||@LightOnIO|| on LinkedIn || on CrunchBase || our BlogAbout myself:&nbsp;LightOn || Google Scholar || LinkedIn ||@IgorCarron ||Homepage||ArXiv"
2021,4,27,Randomized Algorithms for Scientific Computing (RASC),https://nuit-blanche.blogspot.com/2021/04/randomized-algorithms-for-scientific.html,"**&nbsp;Nuit Blanche is now on Twitter: @NuitBlog&nbsp;**At LightOn we build photonic hardware that performs random projections and it is nice to find a source of materials on the subject in one document.&nbsp;Here is a report comprehensively presenting how randomized algorithms are key to the future of computing:Randomized Algorithms for Scientific Computing (RASC)&nbsp;by&nbsp;Aydin Buluc Tamara G. Kolda Stefan M. Wild Mihai Anitescu Anthony DeGennaro John Jakeman Chandrika Kamath Ramakrishnan (Ramki)Kannan Miles E. Lopes Per-Gunnar Martinsson Kary Myers Jelani Nelson Juan M. Restrepo C. Seshadhri Draguna Vrabie Brendt Wohlberg Stephen J. Wright Chao Yang Peter ZwartRandomized algorithms have propelled advances in artificial intelligence and represent a foundational research area in advancing AI for Science. Future advancements in DOE Office of Science priority areas such as climate science astrophysics fusion advanced materials combustion and quantum computing all require randomized algorithms for surmounting challenges of complexity robustness and scalability. This report summarizes the outcomes of that workshop ""Randomized Algorithms for Scientific Computing (RASC)"" held virtually across four days in December 2020 and January 2021.&nbsp; Follow @NuitBlog&nbsp;or join the CompressiveSensing Reddit&nbsp;the Facebook page the Compressive Sensing group on&nbsp;LinkedIn&nbsp;&nbsp;or&nbsp;the Advanced Matrix Factorization group on&nbsp;LinkedInLiked this entry ? subscribe to Nuit Blanche's feed there's more where that came from.&nbsp;You can also subscribe to Nuit Blanche by Email.Other links:Paris Machine Learning:&nbsp;Meetup.com||@Archives||LinkedIn||Facebook|| @ParisMLGroup About&nbsp;LightOn:&nbsp;Newsletter ||@LightOnIO|| on LinkedIn || on CrunchBase || our BlogAbout myself:&nbsp;LightOn || Google Scholar || LinkedIn ||@IgorCarron ||Homepage||ArXiv"
2021,4,6,"The $1,000 GPT-3",https://nuit-blanche.blogspot.com/2021/04/the-1000gpt-3.html,**&nbsp;Nuit Blanche is now on Twitter: @NuitBlog&nbsp;**Progress usually comes from a steady technology bootstrap…until it doesn’t.Take for instance the race for the $1000 genome that started in the early 2000s. Initially sequencing the human genome meant a race between the well-funded public and private sectors but more importantly the resources for the first breakthrough ended up costing upwards of $450M. Yet despite all the economic promise of genome sequencing had Moore’s law been applied sequencing one full genome would still cost $100000 today. However once the goal became clearer to everyone a diversity of technologies and challengers emerged. This intense competition eventually yielded a growth faster than Moore’s Law. The main takeaway is that one cannot rely on the steady progress of one specific technology alone to commoditize tools.Figure from NIH “Facts sheets about genomics: The cost of Sequencing a Human Genome” Dec 7th&nbsp;2020.What does this have to do with the current state of silicon computing and the new demand for Large Language Models (LLMs)? Everything if you ask us and here is how.Less than a year into existence Large Language Models like GPT-3 have already spawned a new generation of startups built on the ability of the model to respond to requests for which it was not trained. More importantly for us hardware manufacturers are positing that one or several customers will be willing to put a billion dollars on the table to train an even larger model in the coming years.Interestingly much like the mass industrialization in the 1930s the good folks at OpenAI are sketching new scaling laws for the industrialization of these larger models.The sad truth is that extrapolating their findings to the training of a 10 Trillion parameters model involves a supercomputer running continuously for two decades. The minimum capital expenditure of this adventure is estimated in the realm of several hundreds of million dollars.Much like what happened in sequencing while silicon improvement and architecture may achieve speedups in the following years it is fair to say that even with Moore’s law no foreseeable technology can reasonably train a fully scaled-up GPT-4 and grab the economic value associated with it.Rebooting silicon with a different physics light and NvNsFor a real breakthrough to occur much like what happened in the sequencing story different technologies need to be jointly optimized. In our case this means performing co-design with new hardware and physics but also going rogue on full programmability.LightOn’s photonic hardware can produce massively parallel matrix-vector multiplications with an equivalent of 2 trillion parameters “for free”: this is about one-fifth of the number of parameters needed for GPT-4. Next comes revisiting the programmability. Current LightOn’s technology keeps these weights fixed by design. Co-design means finding the algorithms for which CPUs and GPUs can perform some of the most intelligent computations and how LightOn’s massive Non-von Neumann (NvN) hardware can do the heavy lifting. We already published how we are replacing backpropagation the workhorse of Deep Learning with an algorithm that unleashes the full potential of our hardware in distributed training. We are also working similarly on an inference step that will take full advantage of the massive number of parameters at our disposal. This involved effort relies in a heavy part thanks to our access to ½ million GPU hours on some of France’s and Europe’s largest supercomputers.And this is just the beginning. There is a vast untapped potential for repurposing large swaths of optical technologies directed primarily for entertainment and telecommunication into computing.The road towards a $1000 GPT-3Based on the GPT-3 training cost estimates achieving a $1000 GPT-3 requires four orders of magnitude improvements. Much like what occurred in 2007 with the genome sequencing revolution Moore’s law may take care of the first two orders of magnitude in the coming decade but the next two rely on an outburst of new efficient technologies — hardware and algorithms. It just so happens that GPT-3 has close to 100 layers so achieving two orders of magnitude savings may arise faster than you can imagine. Stay tuned!Igor Carron is the CEO and co-founder at LightOn&nbsp; Follow @NuitBlog&nbsp;or join the CompressiveSensing Reddit&nbsp;the Facebook page the Compressive Sensing group on&nbsp;LinkedIn&nbsp;&nbsp;or&nbsp;the Advanced Matrix Factorization group on&nbsp;LinkedInLiked this entry ? subscribe to Nuit Blanche's feed there's more where that came from.&nbsp;You can also subscribe to Nuit Blanche by Email.Other links:Paris Machine Learning:&nbsp;Meetup.com||@Archives||LinkedIn||Facebook|| @ParisMLGroup About&nbsp;LightOn:&nbsp;Newsletter ||@LightOnIO|| on LinkedIn || on CrunchBase || our BlogAbout myself:&nbsp;LightOn || Google Scholar || LinkedIn ||@IgorCarron ||Homepage||ArXiv
2021,3,24,Computing with Light: How LightOn intends to unlock Transformative AI,https://nuit-blanche.blogspot.com/2021/03/computing-with-light-how-lighton.html,I gave a talk at #mathia2021 conference on March 9th 2021 where I drew a parallel between the scaling laws that enabled industrialization in the 1920's and the new scaling laws in AI of the 2020's. AI is at its infancy and it needs to have guiding principles (as embedded in these empirical laws) and it also needs to develop new hardware. I showed how in this context LightOn can help unlock Transformative AI. Enjoy!All these other presentations by Yann LeCun Kathryn Hess Michael Jordan Emmanuel Candès and others can be found in this collection of videos on Vimeo. Let me note that Michael made a similar argument as mine where we think of current stage of AI at its infancy in terms of industrialization.&nbsp;&nbsp;Follow @NuitBlog&nbsp;or join the CompressiveSensing Reddit&nbsp;the Facebook page the Compressive Sensing group on&nbsp;LinkedIn&nbsp;&nbsp;or&nbsp;the Advanced Matrix Factorization group on&nbsp;LinkedInLiked this entry ? subscribe to Nuit Blanche's feed there's more where that came from.&nbsp;You can also subscribe to Nuit Blanche by Email.Other links:Paris Machine Learning:&nbsp;Meetup.com||@Archives||LinkedIn||Facebook|| @ParisMLGroup About&nbsp;LightOn:&nbsp;Newsletter ||@LightOnIO|| on LinkedIn || on CrunchBase || our BlogAbout myself:&nbsp;LightOn || Google Scholar || LinkedIn ||@IgorCarron ||Homepage||ArXiv
2021,3,8,Unveiling LightOn Appliance,https://nuit-blanche.blogspot.com/2021/03/unveiling-lighton-appliance.html,Today is a big day at LightOn as we unveil a hardware product the Appliance the world's first commercially available photonic co-processor for AI and HPCIf interested pre-ordering information is here:&nbsp;http://lighton.ai/lighton-appliance&nbsp;We have had a few of these optical processing units in our own LightOn Cloud for the past two years and just retired one after more than 800 days working full time.&nbsp;&nbsp;Here is the press release:&nbsp;The future is now!&nbsp;Leasing starts at 1900€/month or about US$2250/month&nbsp;&nbsp;Follow @NuitBlog&nbsp;or join the CompressiveSensing Reddit&nbsp;the Facebook page the Compressive Sensing group on&nbsp;LinkedIn&nbsp;&nbsp;or&nbsp;the Advanced Matrix Factorization group on&nbsp;LinkedInLiked this entry ? subscribe to Nuit Blanche's feed there's more where that came from.&nbsp;You can also subscribe to Nuit Blanche by Email.Other links:Paris Machine Learning:&nbsp;Meetup.com||@Archives||LinkedIn||Facebook|| @ParisMLGroup About&nbsp;LightOn:&nbsp;Newsletter ||@LightOnIO|| on LinkedIn || on CrunchBase || our BlogAbout myself:&nbsp;LightOn || Google Scholar || LinkedIn ||@IgorCarron ||Homepage||ArXiv
2021,3,4,Video: LightOn unlocks Transformative AI,https://nuit-blanche.blogspot.com/2021/03/video-lighton-unlocks-transformative-ai.html,In the coming days we'll be making another announcement but I wanted to first share a video we did recently. At LightOn we don't build photonic computing hardware because it's fancy or cool (even though it is cool) but because computing hardware is hitting the limits. I know what some say about Moore's law not being dead but the recent focus on Transformers and their attendant scaling laws makes it obvious that in order for more people to have access to these models we need a new computing paradigm. Indeed not everyone can afford to spend a billion dollars in training these models. As Azeem was recently pointing out in one of his newsletters this is how bad things will become:The amazing thing is that we can start to compare the cost of training single AI models with the cost of building the physical fabs that make chips. TSMC’s state-of-the-art 3nm&nbsp;fab&nbsp;will run to around $20bn&nbsp;when it is completed in two years. A&nbsp;fab&nbsp;like this may be competitive for 5-7 years which means it’ll need to churn out $7-8m worth of chips every day before it pays back.And so at LightOn we think that a combination of algorithms and (cool) hardware as the only pathway forward for computing large-scale AI. The video is right here enjoy!&nbsp; Follow @NuitBlog&nbsp;or join the CompressiveSensing Reddit&nbsp;the Facebook page the Compressive Sensing group on&nbsp;LinkedIn&nbsp;&nbsp;or&nbsp;the Advanced Matrix Factorization group on&nbsp;LinkedInLiked this entry ? subscribe to Nuit Blanche's feed there's more where that came from.&nbsp;You can also subscribe to Nuit Blanche by Email.Other links:Paris Machine Learning:&nbsp;Meetup.com||@Archives||LinkedIn||Facebook|| @ParisMLGroup About&nbsp;LightOn:&nbsp;Newsletter ||@LightOnIO|| on LinkedIn || on CrunchBase || our BlogAbout myself:&nbsp;LightOn || Google Scholar || LinkedIn ||@IgorCarron ||Homepage||ArXiv
2020,12,29,The Awesome Implicit Neural Representations Highly Technical Reference Page,https://nuit-blanche.blogspot.com/2020/12/the-awesome-implicit-neural.html,"**&nbsp;Nuit Blanche is now on Twitter: @NuitBlog&nbsp;**&nbsp;Here is a new curated page on the topic of Implicit Neural Representations aptly called Awesome Implicit Neural Representations. It is curated by Vincent Sitzmann&nbsp;(@vincesitzmann) and has been added to the Highly Technical Reference Page:From the page:A curated list of resources on implicit neural representations inspired by&nbsp;awesome-computer-vision. Work-in-progress.This list does not aim to be exhaustive as implicit neural representations are a rapidly evolving &amp; growing research field with hundreds of papers to date.Instead this list aims to list papers introducing key concepts &amp; foundations of implicit neural representations across applications. It's a great reading list if you want to get started in this area!For most papers there is a short summary of the most important contributions.Disclosure: I am an author on the following papers:Scene Representation Networks: Continuous 3D-Structure-Aware Neural Scene RepresentationsMetaSDF: MetaSDF: Meta-Learning Signed Distance FunctionsImplicit Neural Representations with Periodic Activation FunctionsInferring Semantic Information with 3D Neural Scene RepresentationsWhat are implicit neural representations?Implicit Neural Representations (sometimes also referred to coordinate-based representations) are a novel way to parameterize signals of all kinds. Conventional signal representations are usually discrete - for instance images are discrete grids of pixels audio signals are discrete samples of amplitudes and 3D shapes are usually parameterized as grids of voxels point clouds or meshes. In contrast Implicit Neural Representations parameterize a signal as a&nbsp;continuous function&nbsp;that maps the domain of the signal (i.e. a coordinate such as a pixel coordinate for an image) to whatever is at that coordinate (for an image an RGB color). Of course these functions are usually not analytically tractable - it is impossible to ""write down"" the function that parameterizes a natural image as a mathematical formula. Implicit Neural Representations thus approximate that function via a neural network.Why are they interesting?Implicit Neural Representations have several benefits: First they are not coupled to spatial resolution anymore the way for instance an image is coupled to the number of pixels. This is because they are continuous functions! Thus the memory required to parameterize the signal is&nbsp;independent&nbsp;of spatial resolution and only scales with the complexity of the underyling signal. Another corollary of this is that implicit representations have ""infinite resolution"" - they can be sampled at arbitrary spatial resolutions.This is immediately useful for a number of applications such as super-resolution or in parameterizing signals in 3D and higher dimensions where memory requirements grow intractably fast with spatial resolution.However in the future the key promise of implicit neural representations lie in algorithms that directly operate in the space of these representations. In other words: What's the ""convolutional neural network"" equivalent of a neural network operating on images represented by implicit representations? Questions like these offer a path towards a class of algorithms that are independent of spatial resolution!..........h/t Shubhendu Trivedi (@_onionesque)Follow @NuitBlog&nbsp;or join the CompressiveSensing Reddit&nbsp;the Facebook page the Compressive Sensing group on&nbsp;LinkedIn&nbsp;&nbsp;or&nbsp;the Advanced Matrix Factorization group on&nbsp;LinkedInLiked this entry ? subscribe to Nuit Blanche's feed there's more where that came from.&nbsp;You can also subscribe to Nuit Blanche by Email.Other links:Paris Machine Learning:&nbsp;Meetup.com||@Archives||LinkedIn||Facebook|| @ParisMLGroup About&nbsp;LightOn:&nbsp;Newsletter ||@LightOnIO|| on LinkedIn || on CrunchBase || our BlogAbout myself:&nbsp;LightOn || Google Scholar || LinkedIn ||@IgorCarron ||Homepage||ArXiv"
2020,12,21,Hardware Beyond Backpropagation: a Photonic Co-Processor for Direct Feedback Alignment,https://nuit-blanche.blogspot.com/2020/12/hardware-beyond-backpropagation.html,**&nbsp;Nuit Blanche is now on Twitter: @NuitBlog&nbsp;**&nbsp;We presented this work at the Beyond Backpropagation workshop at NeurIPS. A great conjunction between computational hardware and algorithm!&nbsp;Hardware Beyond Backpropagation: a Photonic Co-Processor for Direct Feedback Alignment&nbsp;by&nbsp;Julien Launay Iacopo Poli Kilian Müller Gustave Pariente Igor Carron Laurent Daudet Florent Krzakala Sylvain GiganThe scaling hypothesis motivates the expansion of models past trillions of parameters as a path towards better performance. Recent significant developments such as GPT-3 have been driven by this conjecture. However as models scale-up training them efficiently with backpropagation becomes difficult. Because model pipeline and data parallelism distribute parameters and gradients over compute nodes communication is challenging to orchestrate: this is a bottleneck to further scaling. In this work we argue that alternative training methods can mitigate these issues and can inform the design of extreme-scale training hardware. Indeed using a synaptically asymmetric method with a parallelizable backward pass such as Direct Feedback Alignement communication needs are drastically reduced. We present a photonic accelerator for Direct Feedback Alignment able to compute random projections with trillions of parameters. We demonstrate our system on benchmark tasks using both fully-connected and graph convolutional networks. Our hardware is the first architecture-agnostic photonic co-processor for training neural networks. This is a significant step towards building scalable hardware able to go beyond backpropagation and opening new avenues for deep learning.Follow @NuitBlog&nbsp;or join the CompressiveSensing Reddit&nbsp;the Facebook page the Compressive Sensing group on&nbsp;LinkedIn&nbsp;&nbsp;or&nbsp;the Advanced Matrix Factorization group on&nbsp;LinkedInLiked this entry ? subscribe to Nuit Blanche's feed there's more where that came from.&nbsp;You can also subscribe to Nuit Blanche by Email.Other links:Paris Machine Learning:&nbsp;Meetup.com||@Archives||LinkedIn||Facebook|| @ParisMLGroup About&nbsp;LightOn:&nbsp;Newsletter ||@LightOnIO|| on LinkedIn || on CrunchBase || our BlogAbout myself:&nbsp;LightOn || Google Scholar || LinkedIn ||@IgorCarron ||Homepage||ArXiv
2020,12,19,Diffraction-unlimited imaging based on conventional optical devices,https://nuit-blanche.blogspot.com/2020/12/diffraction-unlimited-imaging-based-on.html,**&nbsp;Nuit Blanche is now on Twitter: @NuitBlog&nbsp;**&nbsp;Aurélien&nbsp;sent me an email back in October and we are now in December! Time flies.Dear IgorI hope things are well.I have been following your NuitBlanche blog for quite a few years. It would thus be great for us if you consider a recent paper of ours to appear in your blog entitled “Diffraction-unlimited imaging based on conventional optical devices”. This paper has been published in Optics Express this year and its link is: https://www.osapublishing.org/oe/abstract.cfm?uri=oe-28-8-11243This manuscript proposes a new imaging paradigm for objects that are too far away to be illuminated or accessed which allows them to be resolved beyond the limit of diffraction---which is thus distinct from the microscopy setting. Our concept involves an easy-to-implement acquisition procedure where a spatial light modulator (SLM) is placed some distance from a conventional optical device. After acquisition of a sequence of images for different SLM patterns the object is reconstructed numerically. The key novelty of our acquisition approach is to ensure that the SLM modulates light before information is lost due to diffraction.Feel free to let us know what you think and happy to provide more information/pictures if needed. Thanks a lot for your time and consideration!Best regardsAurélien BourquardThank you&nbsp;Aurélien!&nbsp;&nbsp;Here is the paper's abstract:Diffraction-unlimited imaging based on conventional optical devices by&nbsp;Nicolas Ducros and Aurélien BourquardWe propose a computational paradigm where off-the-shelf optical devices can be used to image objects in a scene well beyond their native optical resolution. By design our approach is generic does not require active illumination and is applicable to several types of optical devices. It only requires the placement of a spatial light modulator some distance from the optical system. In this paper we first introduce the acquisition strategy together with the reconstruction framework. We then conduct practical experiments with a webcam that confirm that this approach can image objects with substantially enhanced spatial resolution compared to the performance of the native optical device. We finally discuss potential applications current limitations and future research directions.I note that Aurélien has also published some exciting research on Differential Imaging Forensics. His co-author Nicolas has also some interesting work on&nbsp;Single Pixel cameras.Follow @NuitBlog&nbsp;or join the CompressiveSensing Reddit&nbsp;the Facebook page the Compressive Sensing group on&nbsp;LinkedIn&nbsp;&nbsp;or&nbsp;the Advanced Matrix Factorization group on&nbsp;LinkedInLiked this entry ? subscribe to Nuit Blanche's feed there's more where that came from.&nbsp;You can also subscribe to Nuit Blanche by Email.Other links:Paris Machine Learning:&nbsp;Meetup.com||@Archives||LinkedIn||Facebook|| @ParisMLGroup About&nbsp;LightOn:&nbsp;Newsletter ||@LightOnIO|| on LinkedIn || on CrunchBase || our BlogAbout myself:&nbsp;LightOn || Google Scholar || LinkedIn ||@IgorCarron ||Homepage||ArXiv
2020,12,9,LightOn at #NeurIPS2020,https://nuit-blanche.blogspot.com/2020/12/lighton-at-neurips2020.html,**&nbsp;Nuit Blanche is now on Twitter: @NuitBlog&nbsp;**&nbsp;I posted the following on LightOn's Blog.We live in interesting times!A combination of post-Moore’s law era and the advent of very large ML models require all of us to think up new approaches to computing hardware and AI algorithms at the same time. LightOn is one of the few (20) companies in the world publishing in both AI and hardware venues to engage both communities into thinking how theories and workflows may eventually be transformed by the photonic technology we develop.This year thanks to the awesome Machine Learning team at LightOn we have two accepted papers at NeurIPS the AI flagship conference and have five papers in its“Beyond Backpropagation” satellite workshop that will take place on Saturday. This is significant on many levels not the least being that these papers have been nurtured and spearheaded by two Ph.D. students (Ruben Ohana and Julien Launay) who are doing their thesis as LightOn engineers.Here is the list of the different papers accepted at NeurIPS this year that involved LightOn members:Reservoir Computing meets Recurrent Kernels and Structured Transforms Jonathan Dong Ruben Ohana Mushegh Rafayelyan Florent Krzakala. Links: Oral poster paper (presenter: Ruben Ohana). Poster Session 4 on Wed Dec 9th 2020 @ 18:00–20:00 CET. GatherTown: Deep learning ( Town E1 — Spot C0 ) Join GatherTown. Only if and only if poster is crowded join ZoomDirect Feedback Alignment Scales to Modern Deep Learning Tasks and Architectures Jonathan Dong Ruben Ohana Mushegh Rafayelyan Florent Krzakala. Links: Poster paper (Presenter: Julien Launay). Poster Session 6 on Thu Dec 10th 2020 @ 18:00–20:00 CET. GatherTown: Neuroscience and Cognitive Science ( Town A3 — Spot B0 )And at the NeurIPS Beyond Backpropagation workshop taking place on Saturday December 12:Hardware Beyond Backpropagation: a Photonic Co-Processor for Direct Feedback Alignment Julien Launay Iacopo Poli Kilian Muller Igor Carron Laurent Daudet Florent Krzakala Sylvain GiganDirect Feedback Alignment Scales to Modern Deep Learning Tasks and Architectures Julien Launay François Boniface Iacopo Poli Florent Krzakala (Presenter: Julien Launay).Ignorance is Bliss: Adversarial Robustness by Design through Analog Computing and Synaptic Asymmetry Alessandro Cappelli Ruben Ohana Julien Launay Iacopo Poli Florent Krzakala (Presenter: Alessandro Cappelli). We had a blog post on this recently.Align then Select: Analysing the Learning Dynamics of Feedback Alignment Maria Refinetti Stéphane d’Ascoli Ruben Ohana Sebastian Goldt paper (Presenter: Ruben Ohana).How and When does Feedback Alignment Work Stéphane d’Ascoli Maria Refinetti Ruben Ohana Sebastian Goldt. paper (Presenter: Ruben Ohana)Some of these presentations are given in French at the “Déjeuners virtuels de NeurIPS” Follow @NuitBlog&nbsp;or join the CompressiveSensing Reddit&nbsp;the Facebook page the Compressive Sensing group on&nbsp;LinkedIn&nbsp;&nbsp;or&nbsp;the Advanced Matrix Factorization group on&nbsp;LinkedInLiked this entry ? subscribe to Nuit Blanche's feed there's more where that came from.&nbsp;You can also subscribe to Nuit Blanche by Email.Other links:Paris Machine Learning:&nbsp;Meetup.com||@Archives||LinkedIn||Facebook|| @ParisMLGroup About&nbsp;LightOn:&nbsp;Newsletter ||@LightOnIO|| on LinkedIn || on CrunchBase || our BlogAbout myself:&nbsp;LightOn || Google Scholar || LinkedIn ||@IgorCarron ||Homepage||ArXiv
2020,10,14,"Weight Agnostic Neural Networks, a virtual presentation by Adam Gaier, Thursday October 15th, LightOn AI meetup #7",https://nuit-blanche.blogspot.com/2020/10/weight-agnostic-neural-networks-virtual.html,**&nbsp;Nuit Blanche is now on Twitter: @NuitBlog&nbsp;**&nbsp;Ever since we started LightOn we have been putting some emphasis on having great minds think how new algorithms are possible and how they can be enabled with our photonic chips.&nbsp; We also have a regular meetup where we see how other great minds are devising new algorithms.&nbsp;Tomorrow Thursday (October 15th) we are continuing this&nbsp;journey by having Adam Gaier&nbsp;who will talk to us about Weight Agnostic Neural Networks. The virtual meetup will start at:16:00 (UTC+2) Paris time but also&nbsp;7AM PST&nbsp;10AM CST&nbsp;11PM JST.&nbsp;To have more information about connecting to the meetup please register here: https://meetup.com/LightOn-meetup/events/273660363/Follow @NuitBlog&nbsp;or join the CompressiveSensing Reddit&nbsp;the Facebook page the Compressive Sensing group on&nbsp;LinkedIn&nbsp;&nbsp;or&nbsp;the Advanced Matrix Factorization group on&nbsp;LinkedInLiked this entry ? subscribe to Nuit Blanche's feed there's more where that came from.&nbsp;You can also subscribe to Nuit Blanche by Email.Other links:Paris Machine Learning:&nbsp;Meetup.com||@Archives||LinkedIn||Facebook|| @ParisMLGroupAbout&nbsp;LightOn:&nbsp;Newsletter ||@LightOnIO|| on LinkedIn || on CrunchBase || our BlogAbout myself:&nbsp;LightOn || Google Scholar || LinkedIn ||@IgorCarron ||Homepage||ArXiv
2020,10,10,As The World Turns: Implementations now on ArXiv thanks to Paper with Code,https://nuit-blanche.blogspot.com/2020/10/as-world-turns-implementations-now-on.html,"**&nbsp;Nuit Blanche is now on Twitter: @NuitBlog&nbsp;**&nbsp;It's the little things.&nbsp;In the 2000s after featuring good work on Nuit Blanche I was usually following through by asking authors where their codes were. This is how the implementation tag was born. Some of the answers were along the lines of: ""I didn't make it available because I thought it was not worthy"". But what I usually responded was that in effect releasing one's code had a compounding effect on the community:&nbsp;""You may not think it's worthy of release but somehow someone somewhere needs your code for reasons you cannot fathom""&nbsp;As a result I made a conscious choice of featuring those papers that were actively featuring their implementations. The earliest post with featured implementations was February 28th 2007 with a blog post featuring three different implementations of reconstruction solver for compressed sensing. Yes implementations were already available before that but within the compressive sensing community it was a point in time with a collective realization that releasing one's code would bring others to reuse one's work and advance the field as a result. At some point I started making a long list of implementation available but got swamped after a while because it became most of the time the default behavior (a good thing).Five years ago Samim Winiger started GitXiv&nbsp;around Machine Learning papers. I was ecstatic but the site eventually stopped working. Two years ago the&nbsp;Paper with code site started around the same issue and flourished. Congratulations to Robert Ross Marcin Viktor and Ludovic&nbsp;on starting a vibrant community around this need for listing papers with their attendant code. Two days ago the next logical step occurred with the featuring of codes within ArXiv a fantastic advance for Science. Woohoo!Congratulations to&nbsp;Robert&nbsp;Ross&nbsp;Marcin&nbsp;Viktor and&nbsp;Ludovic&nbsp;on making this happen!&nbsp;My next question is:&nbsp;When are they going to get a prize for this? Follow @NuitBlog&nbsp;or join the CompressiveSensing Reddit&nbsp;the Facebook page the Compressive Sensing group on&nbsp;LinkedIn&nbsp;&nbsp;or&nbsp;the Advanced Matrix Factorization group on&nbsp;LinkedInLiked this entry ? subscribe to Nuit Blanche's feed there's more where that came from.&nbsp;You can also subscribe to Nuit Blanche by Email.Other links:Paris Machine Learning:&nbsp;Meetup.com||@Archives||LinkedIn||Facebook|| @ParisMLGroup&lt; br/&gt;  About&nbsp;LightOn:&nbsp;Newsletter ||@LightOnIO|| on LinkedIn || on CrunchBase || our BlogAbout myself:&nbsp;LightOn || Google Scholar || LinkedIn ||@IgorCarron ||Homepage||ArXiv"
2020,5,29,Photonic Computing for Massively Parallel AI is out and it is spectacular!,https://nuit-blanche.blogspot.com/2020/05/photonic-computing-for-massively.html,It’s been a long time brewing but we just released our first white paper on Photonic Computing for Massively Parallel AI. The document features the technology we develop at LightOn some of its use some testimonials and how we see the future of computing. It is downloadable here or from our website: LightOn.aiEnjoy! Follow @NuitBlog&nbsp;or join the CompressiveSensing Reddit&nbsp;the Facebook page the Compressive Sensing group on&nbsp;LinkedIn&nbsp;&nbsp;or&nbsp;the Advanced Matrix Factorization group on&nbsp;LinkedInLiked this entry ? subscribe to Nuit Blanche's feed there's more where that came from.&nbsp;You can also subscribe to Nuit Blanche by Email.Other links:Paris Machine Learning:&nbsp;Meetup.com||@Archives||LinkedIn||Facebook|| @ParisMLGroupAbout&nbsp;LightOn:&nbsp;Newsletter ||@LightOnIO|| on LinkedIn || on CrunchBase || our BlogAbout myself:&nbsp;LightOn || Google Scholar || LinkedIn ||@IgorCarron ||Homepage||ArXiv
2020,5,15,Tackling Reinforcement Learning with the Aurora OPU,https://nuit-blanche.blogspot.com/2020/05/tackling-reinforcement-learning-with.html,"**&nbsp;Nuit Blanche is now on Twitter: @NuitBlog&nbsp;** Martin Graive did an internship at LightOn and decided to investigate how to use Random Projections in the context of Reinforcement Learning. He just wrote a blog post on the matter entitled ""Tackling Reinforcement Learning with the Aurora OPU"". The attendant GitHub repo is located here. Enjoy!Follow @NuitBlog&nbsp;or join the CompressiveSensing Reddit&nbsp;the Facebook page the Compressive Sensing group on&nbsp;LinkedIn&nbsp;&nbsp;or&nbsp;the Advanced Matrix Factorization group on&nbsp;LinkedInLiked this entry ? subscribe to Nuit Blanche's feed there's more where that came from.&nbsp;You can also subscribe to Nuit Blanche by Email.Other links:Paris Machine Learning:&nbsp;Meetup.com||@Archives||LinkedIn||Facebook|| @ParisMLGroupAbout&nbsp;LightOn:&nbsp;Newsletter ||@LightOnIO|| on LinkedIn || on CrunchBase || our BlogAbout myself:&nbsp;LightOn || Google Scholar || LinkedIn ||@IgorCarron ||Homepage||ArXiv"
2020,4,29,"3-year PhD studentship in Inverse Problems and Optical Computing, LightOn, Paris, France",https://nuit-blanche.blogspot.com/2020/04/3-year-phd-studentship-in-inverse.html,**&nbsp;Nuit Blanche is now on Twitter: @NuitBlog&nbsp;** Come and join us at LightOn we have a 3-year PhD fellowship available for someone who can help us build our future photonic cores. Here is&nbsp;As part of the newly EU-funded ITN project “Post-Digital” LightOn has an opening for a fully-funded 3 year Ph.D. studentship to join its R&amp;D team at the crossroads between Computer Science and Physics.&nbsp;The goal of this 3 year Ph.D. position is to theoretically numerically and experimentally investigate how optimization techniques can be used in the design of hybrid computing pipelines including a number of photonic building blocks (“photonic cores”). In particular the optimized networks will be used to solve large-scale physics-based inverse problems in science and engineering - for instance in medical imaging (e.g. ultrasound) or simulation problems. The candidate will first investigate how LigthOn’s current range of photonics co-processors can be integrated within task-specific networks. The candidate will then develop a computational framework for the optimization of electro-optical systems. Finally optimized systems will be built and evaluated on experimental data. This project will be part of LightOn’s internal THEIA project aiming at automating the design of hybrid computing architectures including combinations of LightOn’s photonic cores and traditional silicon chips.In the framework of the EU funded ITN Post-Digital network this project involves collaborations and 3-month secondments with two research groups led by:Daniel Brunner (Université Bourgogne Franche-Comté / FEMTO-ST Besançon) who will be the academic supervisor - The candidate will be registered as a Ph.D. student at UBFC.Pieter Bienstman (IMEC Leuven Belgium).The supervisor at LightOn will be Laurent Daudet CTO - currently on leave from his position of professor of physics at Université de Paris.Due to the EU funding source please make sure you comply with the mobility and eligibility rule before applying. Application: Position to be filled no later than Sept 1st 2020.Send your application with a CV to jobs@lighton.io with [Post-Digital PhD] in the subject line. Shortlisted applicants will be asked to provide references. This project has received funding from the European Union’s Horizon 2020 research and innovation program under the Marie Skłodowska-Curie grant agreement No 860830.For more information: https://lighton.ai/careers/Follow @NuitBlog&nbsp;or join the CompressiveSensing Reddit&nbsp;the Facebook page the Compressive Sensing group on&nbsp;LinkedIn&nbsp;&nbsp;or&nbsp;the Advanced Matrix Factorization group on&nbsp;LinkedInLiked this entry ? subscribe to Nuit Blanche's feed there's more where that came from.&nbsp;You can also subscribe to Nuit Blanche by Email.Other links:Paris Machine Learning:&nbsp;Meetup.com||@Archives||LinkedIn||Facebook|| @ParisMLGroupAbout&nbsp;LightOn:&nbsp;Newsletter ||@LightOnIO|| on LinkedIn || on CrunchBase || our BlogAbout myself:&nbsp;LightOn || Google Scholar || LinkedIn ||@IgorCarron ||Homepage||ArXiv
2020,4,7,LightOn Cloud 2.0 featuring LightOn Aurora OPUs,https://nuit-blanche.blogspot.com/2020/04/lighton-cloud-20-featuring-lighton.html,**&nbsp;Nuit Blanche is now on Twitter: @NuitBlog&nbsp;** At LightOn we just launched LightOn Cloud 2.0 that feature several Aurora Optical Processing Unit for use by the Machine Learning Community. the blog post about this can be found here. You can request access to the Cloud at&nbsp;https://cloud.lighton.ai/We are also having a LightOn Cloud for Research program:&nbsp;https://cloud.lighton.ai/lighton-research/[En] Press Release: LightOn launches LightOn Cloud 2.0 featuring Aurora OPUs April 7th 2020&nbsp;[Fr] Communiqué de presse: LightOn lance le LightOn Cloud 2.0 avec des OPUs Aurora 7 Avril 2020&nbsp;Follow @NuitBlog&nbsp;or join the CompressiveSensing Reddit&nbsp;the Facebook page the Compressive Sensing group on&nbsp;LinkedIn&nbsp;&nbsp;or&nbsp;the Advanced Matrix Factorization group on&nbsp;LinkedInLiked this entry ? subscribe to Nuit Blanche's feed there's more where that came from.&nbsp;You can also subscribe to Nuit Blanche by Email.Other links:Paris Machine Learning:&nbsp;Meetup.com||@Archives||LinkedIn||Facebook|| @ParisMLGroupAbout&nbsp;LightOn:&nbsp;Newsletter ||@LightOnIO|| on LinkedIn || on CrunchBase || our BlogAbout myself:&nbsp;LightOn || Google Scholar || LinkedIn ||@IgorCarron ||Homepage||ArXiv
2020,3,26,Accelerating SARS-COv2 Molecular Dynamics Studies with Optical Random Features,https://nuit-blanche.blogspot.com/2020/03/accelerating-sars-cov2-molecular.html,**&nbsp;Nuit Blanche is now on Twitter: @NuitBlog&nbsp;** We just published a new blog post at LightOn. This time we used LightOn's Optical Processing Unit to show how our hardware can help in speeding up global sampling studies that are using Molecular Dynamics simulations such as in the case of metadynamics. Our engineer Amélie Chatelain&nbsp;wrote a blog post about it and it is here:&nbsp;Accelerating SARS-COv2 Molecular Dynamics Studies with Optical Random&nbsp;FeaturesWe showed that LightOn's OPU in tandem with the NEWMA algorithm becomes very interesting (compared to CPU implementations of Random Fourier Features and FastFood) for simulations featuring more than 4 000 atoms.&nbsp;&nbsp;Because building computational hardware makes no sense if we don't have a community that lifts us the code used to generate the plots in that blog post is publicly available at the following link:&nbsp;https://github.com/lightonai/newma-md.Follow @NuitBlog&nbsp;or join the CompressiveSensing Reddit&nbsp;the Facebook page the Compressive Sensing group on&nbsp;LinkedIn&nbsp;&nbsp;or&nbsp;the Advanced Matrix Factorization group on&nbsp;LinkedInLiked this entry ? subscribe to Nuit Blanche's feed there's more where that came from.&nbsp;You can also subscribe to Nuit Blanche by Email.Other links:Paris Machine Learning:&nbsp;Meetup.com||@Archives||LinkedIn||Facebook|| @ParisMLGroup&lt; br/&gt;  About&nbsp;LightOn:&nbsp;Newsletter ||@LightOnIO|| on LinkedIn || on CrunchBase || our BlogAbout myself:&nbsp;LightOn || Google Scholar || LinkedIn ||@IgorCarron ||Homepage||ArXiv
2020,3,14,Au Revoir Backprop ! Bonjour Optical Transfer Learning !,https://nuit-blanche.blogspot.com/2020/03/au-revoir-backprop-bonjour-optical.html,**&nbsp;Nuit Blanche is now on Twitter: @NuitBlog&nbsp;** We recently used LightOn's Optical Processing Unit to show how our hardware fared in the context of Transfer learning. Our engineer Luca Tommasone wrote a blog post about it and it is here:&nbsp;Au Revoir Backprop! Bonjour Optical Transfer Learning!Because building computational hardware makes no sense if we don't have a community that lifts us the code used to generate the plots in that blog post is publicly available at the following link:&nbsp;https://github.com/lightonai/transfer-learning-opu.Enjoy and most importantly stay safe !Follow @NuitBlog&nbsp;or join the CompressiveSensing Reddit&nbsp;the Facebook page the Compressive Sensing group on&nbsp;LinkedIn&nbsp;&nbsp;or&nbsp;the Advanced Matrix Factorization group on&nbsp;LinkedInLiked this entry ? subscribe to Nuit Blanche's feed there's more where that came from.&nbsp;You can also subscribe to Nuit Blanche by Email.Other links:Paris Machine Learning:&nbsp;Meetup.com||@Archives||LinkedIn||Facebook|| @ParisMLGroup&lt; br/&gt;  About&nbsp;LightOn:&nbsp;Newsletter ||@LightOnIO|| on LinkedIn || on CrunchBase || our BlogAbout myself:&nbsp;LightOn || Google Scholar || LinkedIn ||@IgorCarron ||Homepage||ArXiv
2020,1,15,Beyond Overfitting and Beyond Silicon: The double descent curve,https://nuit-blanche.blogspot.com/2020/01/beyond-overfitting-and-beyond-silicon.html,**&nbsp;Nuit Blanche is now on Twitter: @NuitBlog&nbsp;** We recently tried a small experiment with LightOn's Optical Processing Unit on the issue of generalization. Our engineer&nbsp;Alessandro Cappelli did the experiment and wrote a blog post on it and it is here:&nbsp;Beyond Overfitting and Beyond Silicon: The double descent curve&nbsp;Two days ago&nbsp;Becca Willett&nbsp;was talking on the same subject at the Turing Institute in London.A function space view of overparameterized neural networks Rebecca Willett.Attendant preprint is here:A Function Space View of Bounded Norm Infinite Width ReLU Nets: The Multivariate Case&nbsp;by&nbsp;Greg Ongie Rebecca Willett Daniel Soudry Nathan SrebroA key element of understanding the efficacy of overparameterized neural networks is characterizing how they represent functions as the number of weights in the network approaches infinity. In this paper we characterize the norm required to realize a function&nbsp;f:Rd→R&nbsp;as a single hidden-layer ReLU network with an unbounded number of units (infinite width) but where the Euclidean norm of the weights is bounded including precisely characterizing which functions can be realized with finite norm. This was settled for univariate univariate functions in Savarese et al. (2019) where it was shown that the required norm is determined by the L1-norm of the second derivative of the function. We extend the characterization to multivariate functions (i.e. networks with d input units) relating the required norm to the L1-norm of the Radon transform of a (d+1)/2-power Laplacian of the function. This characterization allows us to show that all functions in Sobolev spaces&nbsp;Ws1(R)&nbsp;s≥d+1 can be represented with bounded norm to calculate the required norm for several specific functions and to obtain a depth separation result. These results have important implications for understanding generalization performance and the distinction between neural networks and more traditional kernel learning.Follow @NuitBlog&nbsp;or join the CompressiveSensing Reddit&nbsp;the Facebook page the Compressive Sensing group on&nbsp;LinkedIn&nbsp;&nbsp;or&nbsp;the Advanced Matrix Factorization group on&nbsp;LinkedInLiked this entry ? subscribe to Nuit Blanche's feed there's more where that came from.&nbsp;You can also subscribe to Nuit Blanche by Email.Other links:Paris Machine Learning:&nbsp;Meetup.com||@Archives||LinkedIn||Facebook|| @ParisMLGroup&lt; br/&gt;  About&nbsp;LightOn:&nbsp;Newsletter ||@LightOnIO|| on LinkedIn || on CrunchBase || our BlogAbout myself:&nbsp;LightOn || Google Scholar || LinkedIn ||@IgorCarron ||Homepage||ArXiv
2019,12,18,"LightOn’s AI Research Workshop — FoRM #4: The Future of Random Matrices. Thursday, December 19th",https://nuit-blanche.blogspot.com/2019/12/lightons-ai-research-workshop-form-4.html,**&nbsp;Nuit Blanche is now on Twitter: @NuitBlog&nbsp;** Tomorrow we will feature LightOn’s 4th AI Research workshop on the Future of Random Matrices (FoRM). It starts at 2pm on Thursday December 19th&nbsp;(That’s 2pm CET/Paris 1pm GMT/UTC/London 8am EST/NY-Montreal 5am PST/California 9pm UTC+8/ Shenzhen). We have an exciting and diverse line-up with talks on compressive learning binarized neural networks particle physics and matrix factorization.Feel free to join us or to catch the event livestream — link to be available on this page on the day of the event.Without further ado here is the program:Program1:45pm — Welcome coffee and opening. A short introduction about LightOn Igor Carron2:00pm — Compressive Learning with Random Projections Ata Kaban2:45pm — Medical Applications of Low Precision Neuromorphic Systems Bodgan Penkovsky3:30pm — Comparing Low Complexity Linear Transforms Gavin Gray4:00pm — Coffee break and discussions4:20pm —LightOn’s OPU+Particle Physics David Rousseau Aishik Ghosh Laurent Basara Biswajit Biswas5:00pm — Accelerated Weighted (Nonnegative) Matrix Factorization with Random Projections Matthieu Puigt5:45pm — Wrapping-up and beers on our rooftopTalks and abstractsAta Kaban University of Birmingham.Compressive Learning with Random ProjectionsBy direct analogy to compressive sensing compressive learning has been originally coined to mean learning efficiently from random projections of high dimensional massive data sets that have a sparse representation. In this talk we discuss compressive learning without the sparse representation requirement where instead we exploit thenatural structure of learning problems.Bodgan Penkovsky Paris-Sud University.Medical Applications of Low Precision Neuromorphic SystemsThe advent of deep learning has considerably accelerated machine learning development but its development at the edge is limited by its high energy cost and memory requirement. With new memory technology available emerging Binarized Neural Networks (BNNs) are promising to reduce the energy impact of the forthcoming machine learning hardware generation enabling machine learning on the edge devices and avoiding data transfer over the network. In this talk we will discuss strategies to apply BNNs to biomedical signals such as electrocardiography and electroencephalography without sacrificing accuracy and improving energy use. The ultimate goal of this research is to enable smart autonomous healthcare devices.Gavin Gray Edinburgh University.Comparing Low Complexity Linear TransformsIn response to the development of recent efficient dense layers this talk discusses replacing linear components in pointwise convolutions with structured linear decompositions for substantial gains in the efficiency/accuracy tradeoff. Pointwise convolutions are fully connected layers and are thus prepared for replacement by structured transforms. Networks using such layers are able to learn the same tasks as those using standard convolutions and provide Pareto-optimal benefits in efficiency/accuracy both in terms of computation (mult-adds) and parameter count (and hence memory).David Rousseau&nbsp;Aishik Ghosh&nbsp;Laurent Basara Biswajit Biswas. LAL Orsay LRI Orsay BITS University.OPU+Particle PhysicsLightOn’s OPU is opening a new machine learning paradigm. Two use cases have been selected to investigate the potentiality of OPU for particle physics:End-to-End learning: high energy proton collision at the Large Hadron Collider have been simulated each collision being recorded as an image representing the energy flux in the detector. Two classes of events have been simulated: signal are created by a hypothetical supersymmetric particle and background by known processes. The task is to train a classifier to separate the signal from the background. Several techniques using the OPU will be presented compared with more classical particle physics approaches.Tracking: high energy proton collisions at the LHC yield billions of records with typically 100000 3D points corresponding to the trajectory of 10000 particles. Various investigations of the potential of the OPU to digest this high dimensional data will be reported.Matthieu Puigt Université du Littoral Côte d’Opale.Accelerated Weighted (Nonnegative) Matrix Factorization with Random ProjectionsRandom projections belong to the major techniques used to process big data. They have been successfully applied to e.g. (Nonnegative) Matrix Factorization ((N)MF). However missing entries in the matrix to factorize (or more generally weights which model the confidence in the entries of the data matrix) prevent their use. In this talk I will present the framework that we recently proposed to solve this issue i.e. to apply random projections to weighted (N)MF. We experimentally show the proposed framework to significantly speed-up state-of-the-art weighted NMF methods under some mild conditions.The workshop will take place at IPGG 6 Rue Jean Calvin 75005 Paris. The location is close to both the Place Monge and the Censier-Daubenton subway stations on line7. it is also close to the Luxembourg station on the RER B line. The location is close to bus stops on the 21 24 27 47 and 89 routes. Note that strikes are still ongoing and some of these options may not be available.We will be in the main amphitheater downstairs on your right when you enter the building. Please register in advance on our meetup group so as to help us in the organization of the workshop.Follow @NuitBlog&nbsp;or join the CompressiveSensing Reddit&nbsp;the Facebook page the Compressive Sensing group on&nbsp;LinkedIn&nbsp;&nbsp;or&nbsp;the Advanced Matrix Factorization group on&nbsp;LinkedInLiked this entry ? subscribe to Nuit Blanche's feed there's more where that came from.&nbsp;You can also subscribe to Nuit Blanche by Email.Other links:Paris Machine Learning:&nbsp;Meetup.com||@Archives||LinkedIn||Facebook|| @ParisMLGroup&nbsp;About&nbsp;LightOn:&nbsp;Newsletter ||@LightOnIO|| on LinkedIn || on CrunchBase || our BlogAbout myself:&nbsp;LightOn || Google Scholar || LinkedIn ||@IgorCarron ||Homepage||ArXiv
2019,12,11,"Ce Soir: Paris Machine Learning Meetup #2 Season 7: Symbolic maths, Data Generation thru GAN, ""Prevision Retards"" @SNCF, Retail and AI, Rapids.ai Leveraging GPUs",https://nuit-blanche.blogspot.com/2019/12/ce-soir-paris-machine-learning-meetup-2.html,"**&nbsp;Nuit Blanche is now on Twitter: @NuitBlog&nbsp;** A big thank you to Scaleway for hosting us in their inspiring office and sponsoring the networking event afterwards.So this is quite exciting. Our meetup group has 7 999 members and we are going to organize a meetup in a town that is paralyzed by strikes. During the course of existence of this meetup we have seen worse.&nbsp;&nbsp;For those of you who will not be able to make it all information slides and link to streaming are below:https://youtu.be/bvKzKfj-8uE0. Presentation Scaleway Mélodie Morice1. Aurélia Negre Michael Sok Quantmetry ""Data generation through GANs""Tabular data are the most common within companies. Generating synthetic data that respects the statistical properties of the original data can have several applications: a machine learning that respects data privacy improving the robustness of a model in relation to data drift etc. Since 2018 there has been an increasing number of academic publications presenting the use of GANs on this type of data particularly on patient medical data. We have performed a proof of concept on real data and present the results of several models from the research namely the Wasserstein GAN the Wasserstein GAN with Gradient Penalty and the Cramér-GAN with the objective of ""model compatibility"" i.e. the possibility of using synthetic data to replace real data to train a classifier.2. Eloïse Nonne Soumaya Ihihi ""Prévisions Retards""&nbsp;a Machine Learning project led by e.SNCF's Data IoT team.Its goal is to integrate predictions of train delays into the SNCF mobile application. Every day our model predicts delays for the next 7 days at each stop for every train in Paris area network. The challenge of this project is to improve the reliability of passenger information and to provide more relevant routes for the application users. We will present the project from the definition of needs and exploratory data analysis to its industrialization in the cloud and the reliability of its predictions.3. Léa Dalle Lucche  Elina Ashkinazi-Ildis Kasra Mansouri Retail and AI Carrefour Data Lab ArtefactThis talk is focussed on AI and ML applications in retail. Discover how Carrefour is transforming through the introduction of the Google - Carrefour Lab by Elina Ashkinazi-Ildis Director of the Lab. Then go further with the ""shelf out detection"" usecase presented by Kasra Mansouri Data Scientist within Artefact.4.&nbsp;Arnaud Wald&nbsp;Scaleway ""RAPIDS.AI Leveraging GPUs for accelerated data science and data analytics""RAPIDS makes it possible to have end-to-end data science pipelines run entirely on GPU architecture. It capitalizes on the parallelization capabilities of GPUs to accelerate data preprocessing pipelines with a pandas-like dataframe syntax. GPU-optimized versions of scikit-learn algorithms are available and RAPIDS also integrates with major deep learning frameworks.This talk will present RAPIDS and its capabilities and how to integrate it in your pipelines.5.&nbsp;François Charton ""Deep Learning for Symbolic Mathematics""Neural networks have a reputation for being better at solving statistical or approximate problems than at performing calculations or working with symbolic data. In this paper we show that they can be **surprisingly good** at more elaborated tasks in mathematics such as symbolic integration and solving differential equations. We propose a syntax for representing mathematical problems and methods for generating large datasets that can be used to train sequence-to-sequence models. We achieve results that outperform commercial Computer Algebra Systems such as Matlab or Mathematica.https://arxiv.org/abs/1912.01412Follow @NuitBlog&nbsp;or join the CompressiveSensing Reddit&nbsp;the Facebook page the Compressive Sensing group on&nbsp;LinkedIn&nbsp;&nbsp;or&nbsp;the Advanced Matrix Factorization group on&nbsp;LinkedInLiked this entry ? subscribe to Nuit Blanche's feed there's more where that came from.&nbsp;You can also subscribe to Nuit Blanche by Email.Other links:Paris Machine Learning:&nbsp;Meetup.com||@Archives||LinkedIn||Facebook|| @ParisMLGroupAbout&nbsp;LightOn:&nbsp;Newsletter ||@LightOnIO|| on LinkedIn || on CrunchBase || our BlogAbout myself:&nbsp;LightOn || Google Scholar || LinkedIn ||@IgorCarron ||Homepage||ArXiv"
2019,11,13,"Paris Machine Learning Meetup #1 Season 7: Neuroscience & AI, Time series, Deep Transfert learning in NLP, Media Campaign, Energy Forecasting",https://nuit-blanche.blogspot.com/2019/11/paris-machine-learning-meetup-1-season.html,**&nbsp;Nuit Blanche is now on Twitter: @NuitBlog&nbsp;** A big thank-you to Publicis Sapient for welcoming us to their inspiring office. Presentation slides will be available here. The streaming of the event can also be found here: Raphaëlle Abitbol&nbsp;How Publicis Sapient builds Machine Learning products to create value for its clients&nbsp;- Raphaëlle Abitbol Head of Data will introduce the Publicis Sapient Data Science TeamPublicis Sapient is a digital transformation partner helping companies and established organizations get to their future digitally-enabled state both in the way they work and the way they serve their customers. Within Publicis Sapient the Data Science Team builds machine learning products in order to support clients in their transformation.Margot Fournier Publicis Sapient. Classification of first time visitorsA significant share of visitors on a site do not return making it crucial to identify levers that can decrease bouncing rate. For a client in the retail sector we developed several models that are able to predict both the gender and the segment in which unlogged and unknown visitors fit in. This allows to personalize the experience from the first visit and prevent users from bouncing.Maxence Brochard Publicis Sapient. Media campaign optimizationInternet users leave multiple traces of micro-conversions (searches clicks whishlist...) during their visit on an ecommerce site: these micro-conversions can be weak signals of an act of purchase in the near future. To analyze those signals we built a solution to detect visitors that are likely to convert and target them in while optimizing media campaigns budgets.Rémi Louf Hugging Face. Transfert learning in NLPStéphane Sénécal Orange. Neuroscience-Inspired AIThe fields of neuroscience and artificial intelligence (AI) have a long and intertwined history. In more recent times however communication and collaboration between the two fields has become less commonplace.In&nbsp;https://bit.ly/2WLsMaQ&nbsp;the authors argue that better understanding biological brains could play a vital role in building intelligent machines. They survey historical interactions between the AI and neuroscience fields and emphasize current advances in AI that have been inspired by the study of neural computation in humans and other animals. Finally they conclude by highlighting shared themes that may be key for advancing future research in both fields.Guillaume Hochard Quantmetry&nbsp;Statistical and machine learning methods combination for improved energy consumption forecasting performance.The recent M4 forecasting competition (https://www.mcompetitions.unic.ac.cy) has demonstrated that the use of one forecasting method alone is not the most efficient approach in terms of forecasting accuracy. In this talk I will focus on an energy consumption forecasting use case integrating exogenous data such as weather conditions and open data. In particular I will present a forecasting time series challenge and the best practices observed on the best submissions and showcase an interesting approach based on a combination of classical statistical forecasting methods and machine learning algorithms such as gradient boosting for increased performance. Generalizing the use of these methods can be a major help to address the challenge of electricity demand and production adjustment.Follow @NuitBlog&nbsp;or join the CompressiveSensing Reddit&nbsp;the Facebook page the Compressive Sensing group on&nbsp;LinkedIn&nbsp;&nbsp;or&nbsp;the Advanced Matrix Factorization group on&nbsp;LinkedInLiked this entry ? subscribe to Nuit Blanche's feed there's more where that came from.&nbsp;You can also subscribe to Nuit Blanche by Email.Other links:Paris Machine Learning:&nbsp;Meetup.com||@Archives||LinkedIn||Facebook|| @ParisMLGroup&lt; br/&gt;  About&nbsp;LightOn:&nbsp;Newsletter ||@LightOnIO|| on LinkedIn || on CrunchBase || our BlogAbout myself:&nbsp;LightOn || Google Scholar || LinkedIn ||@IgorCarron ||Homepage||ArXiv
2019,11,9,"Paris Machine Learning Meetup Hors Série #1: A Talk with François Chollet Hors série with François Chollet, (Creator of the Keras Library)",https://nuit-blanche.blogspot.com/2019/11/paris-machine-learning-meetup-hors.html,**&nbsp;Nuit Blanche is now on Twitter: @NuitBlog&nbsp;** The first Paris Machine Learning Meetup Hors Série #1 of the season is a Talk with François Chollet Hors série with&nbsp;Francois Chollet (Creator of the Keras Library).This event is being recorded.We thank Morning coworking for hosting us and LightOn for their support in organizing this event.&nbsp;Today we welcome Francois Chollet. François is a researcher at Google and creator of the Keras Deep Learning library (https://keras.io). He will talk to us about the new features of the TensorFlow library as well as give us some insight of the latest in Deep Learning Research.Schedule :2pm : Keras &amp; TensorFlow for Deep Learning2.30pm : Q&amp;A2.40pm : Latest research in Deep Learning2.50pm : Q&amp;A3pm : networkingnb :1/ this is **not** a coding session2/ this event does not include a buffet (drink food)Follow @NuitBlog&nbsp;or join the CompressiveSensing Reddit&nbsp;the Facebook page the Compressive Sensing group on&nbsp;LinkedIn&nbsp;&nbsp;or&nbsp;the Advanced Matrix Factorization group on&nbsp;LinkedInLiked this entry ? subscribe to Nuit Blanche's feed there's more where that came from.&nbsp;You can also subscribe to Nuit Blanche by Email.Other links:Paris Machine Learning:&nbsp;Meetup.com||@Archives||LinkedIn||Facebook|| @ParisMLGroup&lt; br/&gt;  About&nbsp;LightOn:&nbsp;Newsletter ||@LightOnIO|| on LinkedIn || on CrunchBase || our BlogAbout myself:&nbsp;LightOn || Google Scholar || LinkedIn ||@IgorCarron ||Homepage||ArXiv
2019,11,1,"Videos: IMA Computational Imaging Workshop, October 14 - 18, 2019",https://nuit-blanche.blogspot.com/2019/11/videos-ima-computational-imaging.html,**&nbsp;Nuit Blanche&nbsp;is now on Twitter:&nbsp;@NuitBlog&nbsp;**&nbsp;Stanley Chan&nbsp;Jeff Fessler&nbsp;Justin Haldar&nbsp;Ulugbek Kamilov&nbsp;Saiprasad Ravishankar&nbsp;Rebecca Willett&nbsp;Brendt Wohlberg&nbsp;just organized a workshop at IMA on computational imaging. Short story as this blog just passed the 8 million page views. Understanding of Compressed sensing was in large part at least by looking at the stats:hits on this blog due to an IMA meeting on the subject and the fact that people could watch the videos afterward. Hoping for this workshop to follow the same path. Given the amount of ML in it I wonder if it shouldn't have been called&nbsp;TheGreatConvergence meeting:-)This workshop will serve as a venue for presenting and discussing recent advances and trends in the growing field of computational imaging where computation is a major component of the imaging system. Research on all aspects of the computational imaging pipeline from data acquisition (including non-traditional sensing methods) to system modeling and optimization to image reconstruction processing and analytics will be discussed with talks addressing theory algorithms and mathematical techniques and computational hardware approaches for imaging problems and applications including MRI tomography ultrasound microscopy optics computational photography radar lidar astronomical imaging hybrid imaging modalities and novel and extreme imaging systems. The expanding role of computational imaging in industrial imaging applications will also be explored.Given the rapidly growing interest in data-driven machine learning and large-scale optimization based methods in computational imaging the workshop will partly focus on some of the key recent and new theoretical algorithmic or hardware (for efficient/optimized computation) developments and challenges in these areas. Several talks will focus on analyzing incorporating or learning various models including sparse and low-rank models kernel and nonlinear models plug-and-play models graphical manifold tensor and deep convolutional or filterbank models in computational imaging problems. Research and discussion of methods and theory for new sensing techniques including data-driven sensing task-driven imaging optimization and online/real-time imaging optimization will be encouraged. Discussion sessions during the workshop will explore the theoretical and practical impact of various presented methods and brainstorm the main challenges and open problems.The workshop aims to encourage close interactions between mathematical and applied computational imaging researchers and practitioners and bring together experts in academia and industry working in computational imaging theory and applications with focus on data and system modeling signal processing machine learning inverse problems compressed sensing data acquisition image analysis optimization neuroscience computation-driven hardware design and related areas and facilitate substantive and cross-disciplinary interactions on cutting-edge computational imaging methods and systems.&nbsp;Morning Theme: Optics/PhotographyWelcome to the IMA&nbsp;&nbsp;Saiprasad Ravishankar (Michigan State University) Brendt Wohlberg (Los Alamos National Laboratory)&nbsp;Computational Microscopy&nbsp;Laura Waller (University of California Berkeley)&nbsp;&nbsp;Interpreting Photon and Electron Detections to Form Images&nbsp;Vivek Goyal (Boston University)&nbsp;Q&amp;A / Coffee &nbsp;Tutorial on Julia Programming for Computational Imaging&nbsp;Jeff Fessler (University of Michigan)&nbsp;&nbsp;Surfing the Technology Wave 2019 and Algorithm-Hardware Co-Design - Industry Perspective + Q&amp;A&nbsp;Sergio Goma (QUALCOMM)&nbsp;&nbsp;Afternoon Theme: Optimization for Computational Imaging (CI)Dictionary and model-based methods in quantitative MRI reconstruction&nbsp;Mariya Doneva (Philips Research Laboratory)&nbsp;A simple 2D graphing tool for the convergence of fixed-point iterations and plug-and-play methods&nbsp;Wotao Yin (University of California Los Angeles)&nbsp;&nbsp;Data Compression in Distributed Learning&nbsp;Ming Yan (Michigan State University)&nbsp;Morning Theme: Novel Imaging Domains&nbsp;Rebecca Willett (University of Chicago)&nbsp;Cryo-Electron Microscopy Image Analysis with Multi-Frequency Vector Diffusion Maps&nbsp;Zhizhen (Jane) Zhao (University of Illinois at Urbana-Champaign)&nbsp;Imaging the Unseen: Taking the First Picture of a Black Hole&nbsp;Katie Bouman (California Institute of Technology)&nbsp;&nbsp;Q&amp;A&nbsp;Computational Methods for Large-scale Inverse Problems: Data-driven VS Physics-driven or Combined?&nbsp;Youzuo Lin (Los Alamos National Laboratory)&nbsp;Affiliated Data Science Seminar: Simple Approaches to Complicated Data Analysis&nbsp;Deanna Needell (University of California Los Angeles)&nbsp;&nbsp;Afternoon Theme: Perspectives on Machine Learning and Artificial Intelligence for CI Tutorial on Python Programming for Computational Imaging&nbsp;Frank Ong (Stanford University)&nbsp;&nbsp;Tutorial Part II&nbsp;&nbsp;Computational Imaging with Deep Learning&nbsp;Orazio Gallo (NVIDIA Corporation)&nbsp;Poster Session and Reception&nbsp;Morning Theme: Perspectives on Machine Learning and Artificial Intelligence for CIJeff Fessler (University of Michigan)Coherent Optical Processing with Machine Learning&nbsp;Charles Bouman (Purdue University)&nbsp;Faster Guaranteed GAN-based recovery in Linear Inverse Problems&nbsp;Yoram Bresler (University of Illinois at Urbana-Champaign)&nbsp;&nbsp;Q&amp;A&nbsp;&nbsp;Geometry of Convolutional Neural Networks for Computational Imaging&nbsp;Jong Chul Ye (Korea Advanced Institute of Science and Technology (KAIST)) &nbsp;Afternoon Theme: X-Ray Computed Tomography Imaging&nbsp;Novel CT Data Acquisition and Processing&nbsp;Joseph Stayman (Johns Hopkins University)&nbsp;&nbsp;Machine Learning for Tomographic Imaging&nbsp;Ge Wang (Rensselaer Polytechnic Institute)&nbsp;&nbsp;Q&amp;A/Coffee Break&nbsp;&nbsp;Data and Image Domain Deep Learning for Tomographic Computational Imaging&nbsp;W. Clem Karl (Boston University)&nbsp;&nbsp;Q&amp;A&nbsp;&nbsp;IEEE Computational Imaging Technical Committee Meeting (open to all)&nbsp;Morning Theme: Signal Processing for CI&nbsp;Stanley Chan (Purdue University) Justin Haldar (University of Southern California)&nbsp;Three Short Stories About Image Denoising&nbsp;Mario Figueiredo (Instituto Superior Tecnico)&nbsp;&nbsp;Fourier Multispectral Imaging&nbsp;Keigo Hirakawa (University of Dayton)&nbsp;Q&amp;A&nbsp;Modeling and removal of correlated noise using nonlocal patch-based collaborative filters with applications to direct and inverse imaging&nbsp;Alessandro Foi (Tampere University of Technology)&nbsp;&nbsp;Afternoon Theme: Magnetic Resonance Imaging Computational Imaging: Beyond the limits imposed by lenses&nbsp;Ashok Veeraraghavan (Rice University) &nbsp;Q&amp;A / Coffee&nbsp;Computational Imaging: From structured low-rank methods to model based deep learning.&nbsp;Mathews Jacob (The University of Iowa)&nbsp;Q&amp;A/Coffee Break&nbsp;&nbsp;Rise of the machines (in MR image reconstruction)&nbsp;Florian Knoll (NYU Langone Medical Center)&nbsp;Q&amp;A&nbsp;Theme: Broad CI Modalities&nbsp;Ulugbek Kamilov (Washington University)&nbsp;Computational Radar Imaging&nbsp;Mujdat Cetin (University of Rochester)&nbsp;&nbsp;Q&amp;A&nbsp;Challenges and Opportunities in Magnetic Resonance Fingerprinting&nbsp;Nicole Seiberlich (University of Michigan)&nbsp;&nbsp;Q&amp;A/Closing Remarks&nbsp;Brendt Wohlberg (Los Alamos National Laboratory)Follow @NuitBlog&nbsp;or join the CompressiveSensing Reddit&nbsp;the Facebook page the Compressive Sensing group on&nbsp;LinkedIn&nbsp;&nbsp;or&nbsp;the Advanced Matrix Factorization group on&nbsp;LinkedInLiked this entry ? subscribe to Nuit Blanche's feed there's more where that came from.&nbsp;You can also subscribe to Nuit Blanche by Email.Other links:Paris Machine Learning:&nbsp;Meetup.com||@Archives||LinkedIn||Facebook|| @ParisMLGroup&lt; br/&gt;  About&nbsp;LightOn:&nbsp;Newsletter ||@LightOnIO|| on LinkedIn || on CrunchBase || our BlogAbout myself:&nbsp;LightOn || Google Scholar || LinkedIn ||@IgorCarron ||Homepage||ArXiv
2019,1,3,How to Deploy Artificial Intelligence in the Industrial World,https://prateekvjoshi.com/2019/01/03/how-to-deploy-artificial-intelligence-in-the-industrial-world/,Industrial companies want their assets to generate more revenue without investing further in infrastructure upgrades. It makes sense because the upgrade is extremely expensive! They need to leverage existing systems to achieve this. Large assets usually come with sensors that &#8230; Continue reading &#8594;
2017,12,5,Chasing Fermat’s Last Theorem,https://prateekvjoshi.com/2017/12/05/chasing-fermats-last-theorem/,Fermat&#8217;s Last Theorem is one of the most celebrated problems in mathematics. It is a problem in the field of Number Theory that&#8217;s very simple to understand yet extremely difficult to solve. In fact it&#8217;s so difficult that it remained &#8230; Continue reading &#8594;
2017,11,7,Dissecting Diophantine Equations,https://prateekvjoshi.com/2017/11/07/dissecting-diophantine-equations/,The word Diophantine refers to Diophantus the third century mathematician from Alexandria. Numbers are the fundamental building blocks of mathematics so he started thinking about problems for which we need integer solutions. For example how many horses do we need &#8230; Continue reading &#8594;
2017,10,9,Understanding Catalan’s Conjecture,https://prateekvjoshi.com/2017/10/09/understanding-catalans-conjecture/,Numbers are deceptively simple. We use them everyday in a variety of contexts. It&#8217;s beautiful how there are so many mysteries hidden inside these numbers. Catalan&#8217;s Conjecture is a mathematical problem that&#8217;s really easy to understand but extremely difficult to &#8230; Continue reading &#8594;
2017,8,19,What is Empirical Risk Minimization,https://prateekvjoshi.com/2017/08/19/what-is-empirical-risk-minimization/,Even though it has an ornate name the underlying concept is actually quite simple and intuitive. The concept of Empirical Risk Minimization becomes relevant in the world of supervised learning. The actual goal of supervised learning is to find a &#8230; Continue reading &#8594;
2017,7,8,Measuring the Stability of Machine Learning Algorithms,https://prateekvjoshi.com/2017/07/08/measuring-the-stability-of-machine-learning-algorithms/,When you think of a machine learning algorithm the first metric that comes to mind is its accuracy. A lot of research is centered on developing algorithms that are accurate and can predict the outcome with a high degree of &#8230; Continue reading &#8594;
2017,5,14,Ergodicity In The World Of IoT,https://prateekvjoshi.com/2017/05/14/ergodicity-in-the-world-of-iot/,Ergodicity is one of the most important concepts in statistics. More importantly it has a lot of real world applications. In this case it&#8217;s applicable to the staggering number of internet connected devices in the world of Internet of Things (IoT). &#8230; Continue reading &#8594;
2017,2,21,Cauchy Sequences In The Real World,https://prateekvjoshi.com/2017/02/21/cauchy-sequences-in-the-real-world/,Sequences occur everywhere in our daily life. Some of the examples include sensor data stock market quotes speech signals and many more. A sequence is a collection of elements where each element is indexed. Repetitions are allowed in this case which &#8230; Continue reading &#8594;
2016,11,30,Measuring The Memory Of Time Series Data,https://prateekvjoshi.com/2016/11/30/measuring-the-memory-of-time-series-data/,Time series data has memory. It remembers what happened in the past and avenge any wrongdoings! Can you believe it? Okay the avenging part may not be true but it definitely remembers the past. The &#8220;memory&#8221; refers to how strongly the &#8230; Continue reading &#8594;
2016,9,15,What Is Pareto Optimality,https://prateekvjoshi.com/2016/09/15/what-is-pareto-optimality/,Let&#8217;s consider a business deal where there are multiple parties negotiating the terms. In such a situation it&#8217;s usually not possible for every single party to get everything it wants. They need to optimize their demands so that everyone comes out &#8230; Continue reading &#8594;
2021,6,19,Real-Time Machine Learning: Why It’s Vital and How to Do It,https://www.predictiveanalyticsworld.com/blog/real-time-machine-learning-why-its-vital-and-how-to-do-it/,"By:  Eric Siegel Predictive Analytics World This article is sponsored by IBM. SUMMARY: Organizations often miss the greatest opportunities that machine learning has to offer because tapping them requires real-time predictive scoring. In order to optimize the very largest-scale processes – which is a vital endeavor for your business – predictive scoring must take place [&#8230;]
The post Real-Time Machine Learning: Why It&#8217;s Vital and How to Do It appeared first on Predictive Analytics World."
2021,5,28,Liftoff: The Basics of Predictive Model Deployment,https://www.predictiveanalyticsworld.com/blog/liftoff-the-basics-of-predictive-model-deployment/,"By: Eric Siegel Predictive Analytics World This article is based on the transcript of one of 142 videos in Eric Siegel’s online course Machine Learning Leadership and Practice – End-to-End Mastery. Developing a good predictive model with machine learning isn&#8217;t the end of the story &#8212; you also need to use it. Predictions don’t help [&#8230;]
The post Liftoff: The Basics of Predictive Model Deployment appeared first on Predictive Analytics World."
2021,4,22,Hot Video: More Accuracy Fallacies – Predicting Criminality and Psychosis,https://www.predictiveanalyticsworld.com/blog/hot-video-more-accuracy-fallacies-predicting-criminality-and-psychosis/,"By:  Eric Siegel Predictive Analytics World Check out this topical video from Predictive Analytics World founder Eric Siegel: &#160; Can AI &#8220;tell&#8221; if you&#8217;re a criminal? Or whether you&#8217;ll develop psychosis? These are perfect examples of the accuracy fallacy which misleads the public into believing that machine learning can distinguish between positive and negative cases [&#8230;]
The post Hot Video: More Accuracy Fallacies – Predicting Criminality and Psychosis appeared first on Predictive Analytics World."
2021,4,19,The Precondition for Machine Learning Success: Bridge the Quant/Business Culture Gap,https://www.predictiveanalyticsworld.com/blog/the-precondition-for-machine-learning-success-bridge-the-quant-business-culture-gap/,"By: Eric Siegel Predictive Analytics World Over the last few years I poured thousands of working hours and 25 years of consulting and teaching experience into making the online course Machine Learning Leadership and Practice – End-to-End Mastery. Why? I developed this training program because teaching is in my blood and I was dying to [&#8230;]
The post The Precondition for Machine Learning Success: Bridge the Quant/Business Culture Gap appeared first on Predictive Analytics World."
2021,4,10,Hot Video: The Accuracy Fallacy – Bogus Machine Learning Results,https://www.predictiveanalyticsworld.com/blog/hot-video-the-accuracy-fallacy-bogus-machine-learning-results/,"By:  Eric Siegel Predictive Analytics World Check out this topical video from Predictive Analytics World founder Eric Siegel: &#160; Can AI &#8220;tell&#8221; if you&#8217;re gay? When machine learning practitioners claim their model achieves &#8220;high accuracy&#8221; it&#8217;s often bogus. This video reveals the undeniable yet common &#8220;accuracy fallacy&#8221;.
The post Hot Video: The Accuracy Fallacy – Bogus Machine Learning Results appeared first on Predictive Analytics World."
2021,4,8,"Explainable Machine Learning, Model Transparency, and the Right to Explanation",https://www.predictiveanalyticsworld.com/blog/explainable-machine-learning-model-transparency-and-the-right-to-explanation/,"By: Eric Siegel Predictive Analytics World Check out this topical video from Predictive Analytics World founder Eric Siegel: &#160; A computer can keep you in jail or deny you a job a loan insurance coverage or housing – and yet you cannot face your accuser. The predictive models generated by machine learning to drive these [&#8230;]
The post Explainable Machine Learning Model Transparency and the Right to Explanation appeared first on Predictive Analytics World."
2021,3,20,Machine Learning’s Missing Link: Business Leadership,https://www.predictiveanalyticsworld.com/blog/machine-learnings-missing-link-business-leadership/,"Machine learning. Your team needs it your boss demands it and your career loves it. After all LinkedIn places it as one of the top few “Skills Companies Need Most” and as the very top emerging job in the U.S. But this number-crunching craze tends to tragically overlook one key point: Of all the ingredients [&#8230;]
The post Machine Learning&#8217;s Missing Link: Business Leadership appeared first on Predictive Analytics World."
2020,10,24,Six Ways Machine Learning Threatens Social Justice,https://www.predictiveanalyticsworld.com/blog/six-ways-machine-learning-threatens-social-justice/,"By: Eric Siegel Predictive Analytics World Originally published in Big Think When you harness the power and potential of machine learning there are also some drastic downsides that you&#8217;ve got to manage. Deploying machine learning you face the risk that it be discriminatory biased inequitable exploitative or opaque. In this article I cover six ways [&#8230;]
The post Six Ways Machine Learning Threatens Social Justice appeared first on Predictive Analytics World."
2020,9,15,Coursera’s “Machine Learning for Everyone” Fulfills Unmet Training Requirements,https://www.predictiveanalyticsworld.com/blog/courseras-machine-learning-for-everyone-fulfills-unmet-training-requirements/,"By: Eric Siegel Predictive Analytics World My new course series on Coursera Machine Learning for Everyone (free access) fulfills two different kinds of unmet learner needs. It’s a conceptually-complete end-to-end course series – its three courses amount to the equivalent of a college or graduate-level course – that covers both the technology side and the [&#8230;]
The post Coursera&#8217;s &#8220;Machine Learning for Everyone&#8221; Fulfills Unmet Training Requirements appeared first on Predictive Analytics World."
2020,9,3,How Machine Learning Works – in 20 Seconds,https://www.predictiveanalyticsworld.com/blog/how-machine-learning-works-in-20-seconds/,"By:  Eric Siegel Predictive Analytics World This transcript comes from Coursera’s online course series Machine Learning for Everyone with Eric Siegel. In 57 words here&#8217;s why machine learning’s important: Business needs prediction. Prediction requires machine learning. And machine learning depends on data. Putting that in reverse we have data we give it to machine learning [&#8230;]
The post How Machine Learning Works – in 20 Seconds appeared first on Predictive Analytics World."
2020,1,8,heatmaply 1.0.0 – beautiful interactive cluster heatmaps in R,https://www.r-statistics.com/2020/01/heatmaply-1-0-0-beautiful-interactive-cluster-heatmaps-in-r/,"I&#8217;m excited to announce that heatmaply version 1.0.0 has been published to CRAN! (getting started vignette is available here) What is heatmaply? heatmaply is an R package for easily creating interactive cluster heatmaps that can be shared online as a stand-alone HTML file. Interactivity includes a tooltip display of values when hovering over cells as &#8230; Continue reading ""heatmaply 1.0.0 &#8211; beautiful interactive cluster heatmaps in R""
The post heatmaply 1.0.0 – beautiful interactive cluster heatmaps in R first appeared on R-statistics blog."
2018,4,27,Registration for eRum 2018 closes in two days!,https://www.r-statistics.com/2018/04/registration-for-erum-2018-closes-in-two-days/,"Why I&#8217;m going to eRum this year instead of useR! I have attended the useR! conferences every year now for the past 9 years and loved it! However this year I&#8217;m saddened that I won&#8217;t be able to go. This is because this year the conference will be held in Australia and going there would require &#8230; Continue reading ""Registration for eRum 2018 closes in two days!""
The post Registration for eRum 2018 closes in two days! first appeared on R-statistics blog."
2018,4,24,R 3.5.0 is released! (major release with many new features),https://www.r-statistics.com/2018/04/r-3-5-0-is-released-major-release-with-many-new-features/,"R 3.5.0 (codename &#8220;Joy in Playing&#8221;) was released yesterday. You can get the latest binaries version from here. (or the .tar.gz source code from here). This is a major release with many new features and bug fixes the full list is provided below. Upgrading R on Windows and Mac If you are using Windows you can easily upgrade to the &#8230; Continue reading ""R 3.5.0 is released! (major release with many new features)""
The post R 3.5.0 is released! (major release with many new features) first appeared on R-statistics blog."
2017,12,8,R 3.4.3 is released (a bug-fix release),https://www.r-statistics.com/2017/12/r-3-4-3-is-released-a-bug-fix-release/,"R 3.4.3 (codename &#8220;Kite-Eating Tree&#8221;) was released last week. You can get the latest binaries version from here. (or the .tar.gz source code from here). As mentioned by David Smith R 3.4.3 is primarily a bug-fix release: It fixes an issue with incorrect time zones on MacOS High Sierra and some issues with handling Unicode characters. (Incidentally representing international and &#8230; Continue reading ""R 3.4.3 is released (a bug-fix release)""
The post R 3.4.3 is released (a bug-fix release) first appeared on R-statistics blog."
2017,10,30,heatmaply: an R package for creating interactive cluster heatmaps for online publishing,https://www.r-statistics.com/2017/10/heatmaply-an-r-package-for-creating-interactive-cluster-heatmaps-for-online-publishing/,"This post on the heatmaply package is based on my recent paper from the journal bioinformatics (a link to a stable DOI). The paper was published just last week and since it is released as CC-BY I am permitted (and delighted) to republish it here in full. My co-authors for this paper are Jonathan Sidi Alan O&#8217;Callaghan and Carson Sievert. Summary: heatmaply is an R &#8230; Continue reading ""heatmaply: an R package for creating interactive cluster heatmaps for online publishing""
The post heatmaply: an R package for creating interactive cluster heatmaps for online publishing first appeared on R-statistics blog."
2017,9,29,R 3.4.2 is released (with several bug fixes and a few performance improvements),https://www.r-statistics.com/2017/09/r-3-4-2-is-released-with-several-bug-fixes-and-a-few-performance-improvements/,"R 3.4.2 (codename &#8220;Short Summer&#8221;) was released yesterday. You can get the latest binaries version from here. (or the .tar.gz source code from here). As mentioned by David Smith R 3.4.2 includes a performance improvement for names: c() and unlist() are now more efficient in constructing the names(.) of their return value thanks to a proposal by Suharto Anggono. (PR#17284) The full list of bug &#8230; Continue reading ""R 3.4.2 is released (with several bug fixes and a few performance improvements)""
The post R 3.4.2 is released (with several bug fixes and a few performance improvements) first appeared on R-statistics blog."
2017,7,11,R 3.4.1 is released – with some Windows related bug-fixes,https://www.r-statistics.com/2017/07/r-3-4-1-is-released-with-some-windows-related-bug-fixes/,"R 3.4.1 (codename &#8220;Single Candle&#8221;) was released several days ago. You can get the latest binaries version from here. (or the .tar.gz source code from here). As mentioned last week by David Smith R 3.4.1 includes several Windows related bug fixed: including an issue sometimes encountered when attempting to install packages on Windows and problems displaying functions including Unicode characters &#8230; Continue reading ""R 3.4.1 is released &#8211; with some Windows related bug-fixes""
The post R 3.4.1 is released – with some Windows related bug-fixes first appeared on R-statistics blog."
2017,4,24,R 3.4.0 is released – with new speed upgrades and bug-fixes,https://www.r-statistics.com/2017/04/r-3-4-0-is-released-with-new-speed-upgrades-and-bug-fixes/,"R 3.4.0 (codename &#8220;You Stupid Darkness&#8221;) was released 3 days ago. You can get the latest binaries version from here. (or the .tar.gz source code from here). The full list of bug fixes and new features is provided below. As mentioned two months ago by David Smith R 3.4.0 indicates several major changes aimed at improving the performance of R in &#8230; Continue reading ""R 3.4.0 is released &#8211; with new speed upgrades and bug-fixes""
The post R 3.4.0 is released – with new speed upgrades and bug-fixes first appeared on R-statistics blog."
2017,3,28,shinyHeatmaply – a shiny app for creating interactive cluster heatmaps,https://www.r-statistics.com/2017/03/shinyheatmaply-a-shiny-app-for-creating-interactive-cluster-heatmaps/,"My friend Jonathan Sidi and I (Tal Galili) are pleased to announce the release of shinyHeatmaply (0.1.0): a new Shiny application (and Shiny gadget) for creating interactive cluster heatmaps. shinyHeatmaply is based on the heatmaply R package which strives to make it easy as possible to create interactive cluster heatmaps. The app introduces a functionality that saves to disk a self &#8230; Continue reading ""shinyHeatmaply &#8211; a shiny app for creating interactive cluster heatmaps""
The post shinyHeatmaply – a shiny app for creating interactive cluster heatmaps first appeared on R-statistics blog."
2017,3,7,R 3.3.3 is released!,https://www.r-statistics.com/2017/03/r-3-3-3-is-released/,"R 3.3.3 (codename &#8220;Another Canoe&#8221;) was released yesterday You can get the latest binaries version from here. (or the .tar.gz source code from here). The full list of bug fixes and new features is provided below. A quick summary by David Smith: R 3.3.3 fixes an issue related to attempting to use download.file on sites that automatically redirect from http &#8230; Continue reading ""R 3.3.3 is released!""
The post R 3.3.3 is released! first appeared on R-statistics blog."
2016,12,3,How to Write an Academic Teaching Statement,http://yyue.blogspot.com/2016/12/how-to-write-academic-teaching-statement.html,A while back I wrote a post on how to write an academic research statement.  This is a follow-up post on how to write an academic teaching statement and contains my thoughts on what makes for a good teaching statement when applying to computer science departments in US research universities.Like I said about research statements the teaching statement is not the most important part of your application package.  In fact for research universities the teaching statement is probably the least important part.  Nonetheless there are pitfalls that should be avoided because a bad teaching statement can hurt your application.  Being an ineffective teacher is grounds for not getting tenure at many schools and schools don't like to hire faculty that they don't think can get tenure.For reference here is my teaching statement from when I went on the job market in Fall 2012.  I want to emphasize a few points:(1) Do not blather on and on about how much you LOVE teaching. No one wants to read this just like no one wants to read in your grad school application about how you've loved computers since you were three years old.  Stuff like this lacks substance and makes you seem immature.(2) Keep it short. No one wants to read a long teaching statement and long teaching statements are usually just fluffed up anyways.  Just like for the research statement optimize the wording to be as concise as possible.  I'd recommend keeping the teaching statement to one page.(3) List the courses and topics you can teach. Some faculty search committees like to explicitly see what courses you can teach so this is an important thing to mention in the teaching statement.  I typically like to list this at the very end of the teaching statement. I would recommend being somewhat open-minded about what courses you can teach.  You can always negotiate these things later.(4) Briefly list your credentials. I typically like to do this at the very beginning. You don't need anything flashy here but you just can't count on your recommendation letters to talk about all of your teaching and mentorship experience.(5) Have a teaching philosophy. No one is expecting you to know exactly how you'd like to teach your courses but not having an explicit teaching philosophy of some kind can be an indicator of complete lack of preparation.  For reference this part took up the 2nd paragraph of my teaching statement.  One strategy is to connect this part to prior teaching and mentoring experience where you can talk about what teaching styles of have been successful for you.(6) Say something meaningful about outreach and interdisciplinary education. Computer science and data science are quickly becoming vital enabling disciplines for virtually all scientific and engineering disciplines.  Recognize this and have some kind of plan for how you want to contribute to it or why it matters to you.
2016,9,29,SoCal ML Symposium,http://yyue.blogspot.com/2016/09/socal-ml-symposium.html,Julian McAuley and I are organizing the Southern California Machine Learning Symposium on Friday November 18 at Caltech!http://dolcit.cms.caltech.edu/scmls/** CPF Deadline is October 4th!!The SoCal ML Symposium brings together students and faculty to promote machine learning in the Southern California region. The workshop serves as a forum for researchers from a variety of fields working on machine learning to share and discuss their latest findings.Topics to be covered at the symposium include but are not limited to:+ Machine learning with graphs social networks and structured data.+ Active learning reinforcement learning crowdsourcing.+ Learning with images and natural language.+ Learning with high-dimensional data.+ Neural networks deep learning and graphical models.+ Learning dynamic and streaming data.+ Applications to interesting new domains.+ Addressing each of these issues at scale.The majority of the workshop will be focused on student contributions in the form of contributed talks and posters.We invite submissions in the form of 1-2 page extended absracts to be presented as posters and oral presentations at the symposium. Submissions may be made on our easychair page:https://easychair.org/conferences/?conf=scmls16A $500 first-prize and a $250 runner-up prize sponsored by Google Research will be awarded for the best student presentations.Timeline:Oct 4: Abstract submissionOct 14: NotificationNov 11: Registration deadlineNov 18: SymposiumFor more details including submission and registration instructions visit our symposium webpage:http://dolcit.cms.caltech.edu/scmls/and please help distribute our flyer:http://dolcit.cms.caltech.edu/scmls/scmls.pdf
2016,1,2,Data Science Positions for Sports Analytics,http://yyue.blogspot.com/2016/01/data-science-positions-for-sports.html,I want to give a plug for STATS LLC which is building a data science team and has several openings for data scientist positions.  For those who don't know STATS is sports data company that provides the tracking data for the National Basketball Association amongst other sports and leagues. STATS also recently acquired Prozone which provides tracking data for many professional soccer leagues around the world. Sports analytics is definitely entering an exciting phase due to the rapid growth of new data sources that offer far greater granularity than was possible before.  See e.g. these papers that analyze tracking data provided by STATS and Prozone.Patrick Lucey is the new Director of Data Science.  I previously worked with Patrick at Disney Research and I can vouch for him being a great collaborator with lots of fantastic ideas and unbounded enthusiasm for sports analytics research.
2015,12,31,Thoughts on NIPS 2015 and OpenAI,http://yyue.blogspot.com/2015/12/thoughts-on-nips-2015-and-openai.html,"A few weeks ago I attended NIPS 2015 which turned out to be (by far) the largest machine learning conference ever. With nearly 4000 attendees the conference saw a roughly 50% increase from the previous year.  Much of this growth seems fueled by industry interest especially in topics such as deep learning and large scale learning.  Deep learning in particular seems to be all the rage these days at least in the public zeitgeist.  I think this is great for the field because this degree of interest will also percolate to the rest of machine learning more broadly.  There have been plenty of posts regarding NIPS already (see: Sebastien Bubeck Neil Lawrence John Langford Paul Mineiro and Hal Daume) with plenty of great pointers to interesting NIPS papers that I'll hopefully get around to reading soon.  On my end I didn't get a chance to see too many papers in part because I was helping presenting a poster during one poster session and a demo during another.  But I did very much enjoy many of the talks especially during the workshops.OpenAIPerhaps the biggest sensation at NIPS was the announcement of OpenAI which is a non-profit artificial intelligence research company with $1B in endowment donated by people such as Sam Altman Elon Musk Peter Thiel and others.  The core ideal of OpenAI is to promote open research in Artificial Intelligence.  For the most part not much is known about how OpenAI will operate (and from what I've gathered the people at OpenAI haven't fully decided on a strategy yet either).  One thing that I do know on good authority is that OpenAI will NOT be patenting their research.  Nonetheless there have already been many reactions to OpenAI from the usual ""robots will steal our jobs"" trope to nuanced concerns voiced by machine learning expert Neil Lawrence observing that open access to data is just as important as open access to research and systems.  I do very much agree with Neil's point and I think that one of the best things that OpenAI can do for the research community is to generate interesting new datasets and testbeds.  There have also been concerns voiced that the founding team is overwhelmingly deep learning people.  I don't think this is much of an issue at the moment because representation learning has been the biggest practical leap forward and giving broader access to learned representations is a great thing.  The announcement has even caught the attention of rationalists such as Scott Alexander who voiced concerns about whether AI research should be open at all for risk of losing control of the technology and potentially leading to the catastrophic results.  Scott's concern is a meta-concern about the current mentality of AI research being an arms race and institutions such as OpenAI not focusing on ""controlling"" access to AI that could become dangerous.  These meta-concerns are predicated on the assumptions that hard takeoff of AGI is a legitimate existential threat to humanity (which I agree with) and that existing institutions such as OpenAI could directly lead to that happening (which I strongly disagree with).  I realize that OpenAI ponders about human-level intelligence in their opening blog post but that's just a mission statement of sorts.  For instance Google while awesome has (thus far) fallen quite short of their mission to ""organize the world's information and make it universally accessible and useful"".  Likewise I don't expect OpenAI to succeed in their mission statement anytime soon.Most machine learning experts probably do take an overly myopic view of machine learning progress which is partly due to the aforementioned research arms race but also just due to how research works (i.e. it is REALLY hard to make tangible progress on something that you can't even begin to rigorously and precisely reason about).  However from what I've read rationalist non-experts conversely tend to phrase things in such imprecise terms that it's hard to have a substantive discussion between the two communities.  I imagine the ""truth"" such as it is is somewhere in the middle. Perhaps one should gather both camps together for a common discussion.What is definitely going to happen in the near term is that access to AI technologies will be an increasingly important competitive advantage moving forward.  And it's great that institutions such as OpenAI will help promote open access to those technologies.I am optimistic that the crew at OpenAI will explore alternative mechanisms in contrast to NSF-style funding of research and how places like the Allen Institute engages in research.  I think it'll be exciting to see what comes out of that process.  Hopefully OpenAI will also engage with places like the Future of Humanity Institute and maybe even create forums that bring together people like Stuart Russell Eric Horvitz Scott Alexander and Eliezer Yudkowsky.Cynthia Dwork on Universal Adaptive Data AnalysisCynthia Dwork gave a great talk on using differential privacy to guard against overfitting when re-using a validation set multiple times.  See this Science paper for more details.  The basic idea is that when you use your validation set to evaluate the performance of a model do so in a differentially private way so that you don't overfit to the idiosyncrasies of the validation set.  See for instance this paper describing an application to Kaggle-style competitions. This result demonstrates a great instance of (unexpected?) convergence between different areas of study: privacy-preserving computation and machine learning.Jerry Zhu on Machine TeachingJerry Zhu has been doing very interesting work on Machine Teaching which he talked about at the NIPS workshop on adaptive machine learning.  Roughly speaking machine teaching is the computational and statistical problem of how to select training examples to teach a learner as quickly as possible.  One can think of machine teaching as the converse of active learning where instead of the learner actively querying for training examples a teacher actively provides them.  Machine teaching has a wide range of applications but the one that I'm most interested in is when the learner is a human.  As models become necessarily more complex in the quest for predictive accuracy it is important that we devise methods to keep these models somehow interpretable to humans.  One way is to use a machine teaching approach to quickly show the human what concepts the trained model has learned.  For instance this approach would have  applications debugging complicated machine learning models.Rich Caruana on Interpretable Machine Learning for Health CareOn the flip side Rich Carauna talked about training models that are inherently interpretable by domain experts such as medical professionals.  Of course these models are only applicable in restricted domains such as when there is a ""sufficient"" set of hand-crafted features such that a generalized additive model can accurately capture the phenomenon of interest.  The approach was applied to two settings: predicting the risk of pneumonia and 30-day re-admission.   One interesting consequence of this study was that these interpretable models could be used to tease out biases in the data collection process.  For instance the model predicted that patients with asthma are at lower risk of dying from pneumonia.  Consulting with medical experts revealed that historically patients with asthma are more closely monitored for signs of pneumonia and so the disease is detected much earlier than for the general populace.  Nonetheless it's clear that one wouldn't want a predictive model to predict a lower risk of pneumonia for patients with asthma -- that was simply a consequence of how the historical data was collected.  See this paper for details.Zoubin Ghahramani on Probabilistic ModelsZoubin Ghahramani gave a keynote talk on probabilistic models.  During this deep learning craze it's important keep in mind that properly quantifying uncertainty is often a critical component as well.  We are rarely given perfect information and so we can rarely make perfect predictions.  In order to make informed decisions our models should make calibrated probabilities regarding so that we can properly weigh different tradeoffs.  Recall that one of the critical aspects of the Jeopardy! winning IBM Watson machine was being able to properly calibrate its own confidence in the right answer (or question).  Another point that Zoubin touched on was rational allocation of computational resources under uncertainty.  See also this great essay on the interplay between machine learning and statistics by Max Welling.Interesting PapersAs I mentioned earlier I didn't get a chance to check out too many posters but here are a few that I did see which I found quite interesting.Generalization in Adaptive Data Analysis and Holdout Reuseby Cynthia Dwork Vitaly Feldman Moritz Hardt Toniann Pitassi Omer Reingold Aaron RothThis paper generalizes previous work on adaptive data analysis by: 1) allow the query to the validation set be adaptive to the result of previous queries and 2) provide a more general definition of adaptive data analysis.Logarithmic Time Online Multiclass Prediction by Anna Choromanska John LangfordThis paper studies how to quickly construct multiclass classifiers whose running time is logarithmic in the number of classes.  This approach is especially useful for settings where the number of classes is enormous which is also known as Extreme Multiclass Classification.Spatial Transformer Networksby Max Jaderberg Karen Simonyan Andrew Zisserman Koray KavukcuogluThis paper studies how to incorporate more invariants into convolutional neural networks beyond just shift invariance.  The most obvious cases are being invariant to rotation and skew.  See also this post.Optimization as Estimation with Gaussian Processes in Bandit Settingsby Zi Wang Bolei Zhou Stefanie JegelkaA preliminary version of this paper was presented at the Women in Machine Learning Workshop at NIPS and will be formally published at AISTATS 2016.  This is a really wonderful paper that unifies to some extent two of the most popular views in Bayesian optimization: UCB-style bandit algorithms and probability of improvement (PI) algorithms.  One obvious future direction is to also unify with expected improvement (EI) algorithms as well.Fast Convergence of Regularized Learning in Gamesby Vasilis Syrgkanis Alekh Agarwal Haipeng Luo Robert E. SchapireThis paper won a best paper award at NIPS and analyzed the setting of learning in a repeated game.  Previous results showed a regret convergence of O(T-1/2) and this paper demonstrates an asymptotic improvement to O(T-3/4) for individual regret and O(T-1) for the sum of utilities. Data Generation as Sequential Decision Makingby Philip Bachman Doina PrecupThis paper takes the view of sampling from sequential generative models as sequential decision making.  For instance can we view sequential sampling as an Markov decision process?  In particular this paper focuses on the problem of data imputation or filling in missing values.  This style of research has been piquing my interest recently since it can offer the potential to dramatically speed up computation when sampling or prediction can be very computationally intensive.Sampling from Probabilistic Submodular Modelsby Alkis Gotovos S. Hamed Hassani Andreas KrauseAndreas's group has been working on a general class of probabilistic models called log-submodular and log-supermodular models. These models generalize models such as determinantal point processes. This paper studies how to do inference on these models via MCMC sampling and establish conditions for fast mixing.The Self-Normalized Estimator for Counterfactual Learningby Adith Swaminathan Thorsten JoachimsThis paper addresses a signficant limitation of previous work on counterfactual risk minimization which is overfitting to hypotheses that match or avoid the logged (bandit) training data which the authors call propensity overfitting.  The authors propose a new risk estimator which deals with this issue."
2015,9,7,Thoughts on KDD 2015,http://yyue.blogspot.com/2015/09/thoughts-on-kdd-2015.html,"Last month I attended KDD 2015 in beautiful Sydney Australia.  For those who don't know KDD is the premier international conference for applied machine learning & data mining and is often the venue for some of the most interesting data analysis research projects.  Despite concerns that KDD 2015 would be a let down after KDD 2014 was such a great success in New York City overall KDD 2015 was a fantastic conference with an excellent lineup of invited speakers and plenty of interesting papers.  Congratulations also to my PhD advisor Thorsten Joachims who not only did a great job as PC Co-Chair but also was the recipient of a Test of Time Award for his work on Optimizing Search Engines using Clickthrough Data.Data Science for ScienceOne of the biggest themes at KDD 2015 was applying data science to support the sciences which is something that's been on my mind a lot recently.  Hugh Durrant-White gave a great keynote on applying machine learning to discovery processes in geology and ecology.  One thing that jumped out of his talk was how challenging it is to develop models that are interpretable to domain experts.  This issue is ameliorated in his settings because he largely focused on spatial models which are easier to visualize and interpret.  Susan Athey gave another keynote on the interplay between machine learning and causal inference in policy evaluation which is an important issue for the sciences as well.  I must admit most of the talk went over my head but there was some interesting debate after the talk about whether causality should be the goal or rather just more ""robust"" correlations (whatever that might mean).I also really enjoyed the Data-Driven Science Panel where the debate got quite heated at times.  Two issues in particular  stood out.  First what should be the role of machine learning and data mining experts in the ecosystem of data-driven science?  One the one hand computer scientists have historically had a large impact by developing systems and platforms that abstract away low-level complexity and empower the end user to be more productive.  However how to achieve such a solution in a data-rich world is a much messier (or at least different) type of endeavor.  There are of course plenty of startups that address aspects of this problem but a genuine scalable solution for science remains elusive. A second issue that was raised was whether computational researchers have made much of a direct impact on the sciences.  The particular area raised by Tina Eliassi-Rad is the social sciences.   Machine learning and data mining have taken great interest in computational social science via studying large social networks.  However it is not clear to what extent computational researchers have directly made an impact to traditional social science fields.  Of course this issue is tied back to what the role of computational researchers should be.  On the one hand many social scientists do use tools made by computational people so the indirect impact is quite clear.  Does it really matter that there hasn't been much direct impact?Update on MOOCsDaphne Koller gave a great keynote on the state of MOOCs and Coursera in particular.  It seems that MOOCs nowadays are much smarter about their consumer base and have diversified the way they deliver content and measure success for a wide range of students.  For example people now understand much better the different needs of college aspirants (who use MOOCs to supplicant high school & college education) versus young professionals (who use MOOCs to get ahead in their careers) versus those seeking vocational skills (which is very popular in less developed countries).  One striking omission that was pointed out during the Q&A was that MOOCs have mostly abandoned the pre-college demographic especially before high school.  In retrospect this is not too surprising in large part due to the very different requirements for primary and secondary education across different states and school districts.  But it does put a damper on the current MOOC enthusiasm since many problems with education start much earlier than college.Lessons Learned from Large-Scale A/B TestingRon Kohavi gave a keynote on lessons learned from online A/B testing.  The most interesting aspect of his talk was just how well-tuned the existing systems are.  One symptom of a highly tuned system is that it becomes very difficult to intuit about whether certain modifications will increase or decrease the performance of the system (or have no effect). For example he gave the audience a number of questions to the audience such as: ""Does increasing the description of the sponsored advertisements lead to increased overall clicks on ads?""  Basically the audience could not guess better than random.  So the main lesson is to basically to follow the data and don't be to (emotionally) tied to your own intuitions when it comes to optimizing large complex industrial systems.Sports Analytics WorkshopI co-organized the 2nd workshop on Large-Scale Sports Analytics.  I tried to get more eSports into the workshop this year but alas fell a bit short.  Thorsten did give an interesting talk that used eSports data although the phenomenon he was studying was not specific to eSports.  In many ways eSports is an even better test bed for sports analytics than traditional sports because game replays track literally everything.Within the more traditional sports regimes it's clear that access to data remains a large bottleneck. Many professional leagues are hoarding their data like gold but sadly do not have the expertise leverage the data effectively.  The situation actually seems better in Europe where access to tracked soccer (sorry futbol) games are relatively common.  In the US it seems like the data is only available to a select few sports analytics companies such as Second Spectrum.  I'm hopeful that this situation will change in the near future as the various stake holders become more comfortable with the idea that it's not the raw data that has value but the processed artifacts built on top of that data.Interesting PapersThere were plenty of interesting research papers at KDD of which I'll just list a few that I particularly liked.  A Decision Tree Framework for Spatiotemporal Sequence Prediction by Taehwan Kim Yisong Yue Sarah Taylor and Iain MatthewsI'll start with a shameless piece of self-advertising.  In collaboration with Disney Research we trained a model to generate visual speech i.e. animate the lower face in response to audio or phonetic inputs.  See the demo video below:More details here.Inside Jokes: Identifying Humorous Cartoon Captionsby Dafna Shahaf Eric Horvitz and Robert MankoffProbably the most interesting application at KDD was on studying the anatomy of a joke.  While the results may not seem too surprising in retrospect (e.g. the punchline should be at the end of the joke) what was really cool was that the model could quantify if one joke was funnier than another joke (i.e. rank jokes).  Cinema Data Mining: The Smell of Fearby Jörg Wicker Nicolas Krauter Bettina Derstorff Christof Stönner Efstratios Bourtsoukidis Thomas Klüpfel Jonathan Williams and Stefan KramerThis was a cool paper that studied how the exhaled organic particles vary in response to different emotions.  The authors instrumented a movie theater's air circulation system with chemical sensors and found that the chemicals you exhale are indicative of various emotions such as fear or amusement.  The author repeatedly lamented the fact that they didn't do this for any erotic films and so they don't know what the cinematic chemical signature of arousal would look like. Who supported Obama in 2012? Ecological inference through distribution regressionby Seth Flaxman Yu-Xiang Wang and Alex SmolaThis paper presents a new solution to the ecological inference problem of inferring individual level preferences from aggregate data.  The primary data testbed were county-wise election outcomes and demographic data that reported at a different granularity or overlay.  The main issue is how to estimate e.g. female preference for one presidential candidate using just these kinds of aggregate data.Certifying and removing disparate impactby Michael Feldman Sorelle Friedler John Moeller Carlos Scheidegger and Suresh VenkatasubramanianMany people assume that because algorithms are ""objective"" then they can't be biased or discriminatory.  This assumption is invalid because the data or features themselves can be biased (cf. this interview with Cynthia Dwork).  The authors of this paper propose a way to detect & remove bias in machine learning models that is tailored to the US legal definition of bias.  The work is of course preliminary but this paper was arguably the most thought provoking of the entire conference.Edge-Weighted Personalized PageRank: Breaking A Decade-Old Performance Barrierby Wenlei Xie David Bindel Alan Demers and Johannes GehrkeThis paper proposes a reduction approach to personalized PageRank that yields a computational boost by several orders of magnitude thus allowing for the first time personalized PageRank to be computed at interactive speeds.  This paper was also the recipient of the best paper award."
2015,4,9,KDD 2015 Workshop on Large-Scale Sports Analytics,http://yyue.blogspot.com/2015/04/kdd-2015-workshop-on-large-scale-sports.html,We are pleased to announce that the KDD Workshop on Large-Scale Sports Analytics will be taking place in Sydney this year on August the 10th at KDD 2015. Similar to last year it will be a full day workshop consisting of invited speakers as well as poster sessions for submitted papers. A call for paper submissions is below. === Call for Submissions === When: August 10th 2015Where: Sydney AustraliaWebsite: http://large-scale-sports-analytics.org/Description: Virtually every aspect of sports analytics is now entering the “Big Data” phase and the interest in effectively mining modeling and learning from such data has also been correspondingly growing. Relevant data sources include detailed play-by-play game logs tracking data physiological sensor data to monitor the health of players social media and text-based content and video recordings of games.The objective of this workshop is to bring together researchers and analysts from academia and industry who work in sports analytics data mining and machine learning. We hope to enable meaningful discussions about state-of-the-art in sports analytics research and how it might be improved upon. We seek poster submissions (which can be both preliminary research as well as recently published work) on topics including but not limited to:* Spatiotemporal modeling* Video text and social media analysis* Feature selection and dimensionality reduction* Feature learning and latent factor models* Computational rationality* Real-time predictive modeling* Interactive analysis &amp; visualization tools* Sensor technology and reliability* Labeling and annotation of events/activities/tactics* Real-time/deployed analytical systems* Knowledge discovery of player/team/league behaviors* Game Theory* eSportsSubmission Details:Poster submissions should be extended abstracts no more than 4 pages in length (in KDD format do not need to be anonymous).  Extended abstracts should be submitted by June 5th 11:59 PM PDT. Details can be found at: http://www.large-scale-sports-analytics.org/Large-Scale-Sports-Analytics/Submissions.htmlImportant Dates:Submission - 5th June 2015 11:59 PM PDT Notification - 30th June 2015Workshop - 10th August 2015Organizers:Patrick Lucey (Disney Research) (patrick.lucey@disneyresearch.com)Yisong Yue (Caltech) (yyue@caltech.edu)Jenna Wiens (University of Michigan) (wiensj@umich.edu)Stuart Morgan (Australian Institute of Sport) (stuart.morgan@ausport.gov.au)
2015,1,14,A Brief Overview of Deep Learning,http://yyue.blogspot.com/2015/01/a-brief-overview-of-deep-learning.html,"(This is a guest post by Ilya Sutskever on the intuition behind deep learning as well as some very useful practical advice.  Many thanks to Ilya for such a heroic effort!)Deep Learning is really popular these days.  Big and small companies are getting into it and making money off it.  It’s hot.  There is some substance to the hype too: large deep neural networks achieve the best results on speech recognition visual object recognition and several language related tasks such as machine translation and language modeling.But why?   What’s so special about deep learning? (from now on we shall use the term  Large Deep Neural Networks --- LDNN ---  which is what the vaguer term “Deep Learning” mostly refers to).  Why does it work now and how does it differ from neural networks of old?   Finally suppose you want to train an LDNN.  Rumor has it that it’s very difficult to do so that it is “black magic” that requires years of experience.  And while it is true that experience helps quite a bit the amount of “trickery” is surprisingly limited ---- one needs be on the lookout for only a small number well-known pitfalls.   Also there are many open-source implementations of various state-of-the-art neural networks (c.f. Caffe cuda-covnet Torch Theano) which makes it much easier to learn all the details needed to make it work.Why Does Deep Learning Work?It is clear that to solve hard problems we must use powerful models.  This statement is obvious.  Indeed if a model is not powerful then there is absolutely no chance that it can succeed in solving a hard problem no matter how good the learning algorithm is.   The other necessary condition for success is that our model is trainable.   That too is obvious for if we cannot train our model then its power is useless --- it will never amount to anything and great results will not be achieved.   The model will forever remain in a state of unrealized potential. Fortunately LDNNs are both trainable and powerful.    Why Are LDNNs Powerful?When I talk about LDNNs I’m talking about 10-20 layer neural networks (because this is what can be trained with today’s algorithms).   I can provide a few ways of looking at LDNNs that will illuminate the reason they can do as well as they do.Conventional statistical models learn simple patterns or clusters.  In contrast LDNNs learn computation albeit a massively parallel computation with a modest number of steps.  Indeed this is the key difference between LDNNs and other statistical models.To elaborate further:  it is well known that any algorithm can be implemented by an appropriate very deep circuit (with a layer for each timestep of the algorithm’s execution -- one example).   What’s more the deeper the circuit the more expensive are the algorithms that can be implemented by the circuit (in terms of runtime).    And given that neural networks are circuits as well deeper neural networks can implement algorithms with more steps ---- which is why depth = more power.N.B.: It is easy to see that a single neuron of a neural network can compute the conjunction of its inputs or the disjunction of its inputs by simply setting their connections to appropriate values. Surprisingly neural networks are actually more efficient than boolean circuits.  By more efficient I mean that a fairly shallow DNN can solve problems that require many more layers of boolean circuits.    For a specific example consider the highly surprising fact that a DNN with 2 hidden layer and a modest number of units can sort N  N-bit numbers!   I found the result shocking when I heard about it so I implemented a small neural network and trained it to sort 10  6-bit numbers which was easy to do to my surprise.    It is impossible to sort N   N-bit numbers with a boolean circuit that has two hidden layers and that are not gigantic.The reason DNNs are more efficient than boolean circuits is because neurons perform a threshold operation which cannot be done with a tiny boolean circuit.Finally human neurons are slow yet humans can perform lots of complicated tasks in a fraction of a second.   More specifically it is well-known that a human neuron fires no more than 100 times per second.  This means that if a human can solve a problem in 0.1 seconds then our neurons have enough time to fire only 10 times --- definitely not much more than that.   It therefore follows that a large neural network with 10 layers can do anything a human can in 0.1 seconds.  This is not scientific fact since it is conceivable that real neurons are much more powerful than artificial neurons but real neurons may also turn out to be much less powerful than artificial neurons.  In any event the above is certainly a plausible hypothesis.This is interesting because humans can solve many complicated perception problems in 0.1 seconds --- for example humans can recognize the identity of an object that’s in front of them recognize a face recognize an emotion and understand speech in a fraction of a second.    In fact if there exists even just one person in the entire world who has achieved an uncanny expertise in performing a highly complex task of some sort in a fraction of a second then this is highly convincing evidence that a large DNN could solve the same task --- if only its connections are set to the appropriate values. But won’t the neural network need to be huge?   Maybe.  But we definitely know that it won’t have to be exponentially large ---- simply because the brain isn’t exponentially large!   And if human neurons turn out to be noisy (for example) which means that many human neurons are required to implement a single real-valued operation that can be done using just one artificial neuron then the number of neurons required by our DNNs to match a human after 0.1 seconds is greatly diminished. These four arguments suggest (strongly in my opinion) that for a very wide variety of problems there exists a setting of the connections of a LDNN that basically solves the problem.    Crucially the number of units required to solve these problems is far from exponential --- on the contrary the number of units required is often so “small” that it is even possible using current hardware to train a network that achieves super-high performance on the task of interest.   It is this last point which is so important and requires additional elaboration:We know that most machine learning algorithms are consistent: that is they will solve the problem given enough data.   But consistency generally requires an exponentially large amount of data.  For example the nearest neighbor algorithm can definitely solve any problem by memorizing the correct answer to every conceivable input.  The same is true for a support vector machine --- we’d have a support vector for almost every possible training case for very hard problems.  The same is also true for a neural network with a single hidden layer:  if we have a neuron for every conceivable training case so that neuron fires for that training case and but not for any other then we could also learn and represent every conceivable function from inputs to outputs.  Everything can be done given exponential resources but it is never ever going to be relevant in our limited physical universe.And it is in this point that LDNNs differ from previous methods:  we can be reasonably certain that a large but not huge LDNN will achieve good results on a surprising variety of problems that we may want to solve.   If a problem can be solved by a human in a fraction of a second then we have a very non-exponential  super-pessimistic upper bound on the size of the smallest neural network that can achieve very good performance.   But I must admit that it is impossible to predict whether a given problem will be solvable by a deep neural network ahead of time although it is often possible to tell whenever we know that a similar problem can be solved by an LDNN of a manageable size. So that’s it then.  Given a problem such as visual object recognition all we need is to train a giant convolutional neural network with 50 layers.  Clearly a giant convnet with 50 layers can be configured to achieve human-level performance on object recognition --- right?   So we simply need to find these weights.  Once once we do the problem is solved.Learning.What is learning?   Learning is the problem of finding a setting of the neural network’s weights that achieves the best possible results on our training data.   In other words we want to “push” the information from the labelled data into the parameters so that the resulting neural network will solve our problem. The success of Deep Learning hinges on a very fortunate fact:  that well-tuned and carefully-initialized stochastic gradient descent (SGD) can train LDNNs on problems that occur in practice.   It is not a trivial fact since the training error of a neural network as a function of its weights is highly non-convex.  And when it comes to non-convex optimization we were taught that all bets are off.  Only convex is good and non-convex is bad.   And yet somehow SGD seems to be very good at training those large deep neural networks on the tasks that we care about.   The problem of training neural networks is NP-hard and in fact there exists a family of datasets such that the problem of finding the best neural network with three hidden units is NP-hard.  And yet SGD just solves it in practice.  This is the main pillar of deep learning.We can say fairly confidently that successful LDNN training relies on the “easy” correlation in the data which allows learning to bootstrap itself towards the more “complicated” correlations in the data.   I have done an experiment that seems to support this claim:   I found that training a neural network to solve the parity problem is hard.  I was able to train the network to solve parity for 25 bits 29 bits but never for 31 bits  (by the way I am not claiming that learning parity is impossible for over 30 bits --- only that I didn’t succeed in doing so).   Now we know that parity is a highly unstable problem that doesn’t have any linear correlations:  every linear function of the inputs is completely uncorrelated with the output which is a problem for neural networks since they are mostly linear at initialization time (so perhaps I should’ve used larger initial weights?  I will discuss the topic of weight initialization later in the text).   So my hypothesis (which is shared by many other scientists) is that neural networks start their learning process by noticing the most “blatant” correlations between the input and the output and once they notice them they introduce several hidden units to detect them which enables the neural network to see more complicated correlations.  Etc.  The process goes on.  I imagine some sort of a “spectrum” of correlations --- both easy and hard and the network jumps from a correlation to a more complicated correlation much like an opportunistic mountain climber.Generalization.While it is very difficult to say anything specific about the precise nature of the optimization of neural networks (except near a local minimum where everything becomes convex and uninteresting) we can say something nontrivial and specific about generalization.And the thing we can say is the following:  in his famous 1984 paper called ""A Theory of the Learnable"" Valiant proved roughly speaking that if you have a finite number of functions say N then every training error will be close to every test error once you have more than log N training cases by a small constant factor.   Clearly if every training error is close to its test error then overfitting is basically impossible  (overfitting occurs when the gap between the training and the test error is large).    (I am also told that this result was given in Vapnik’s book as small exercise).    This theorem is easy to prove but I won’t do it here.  But this very simple result has a genuine implication to any implementation of neural networks.  Suppose I have a neural network with N parameters.  Each parameter will be a float32.  So a neural network is specified with 32N bits which means that we have no more than 232N distinct neural networks and probably much less.   This means that we won’t overfit much once we have more than 32N training cases.  Which is nice.  It means that it’s theoretically OK to count parameters.  What’s more if we are quite confident that each weight only requires 4 bits (say) and that everything else is just noise then we can be fairly confident that the number of training cases will be a small constant factor of 4N rather than 32N.The Conclusion:If we want to solve a hard problem we probably need a LDNN which has many parameters.   So we need a large high-quality labelled training set to make sure that it has enough information to specify all the network’s connections.  And once we get that training set we should run SGD on it until the network solves the problem.  And it probably will if our neural network is large and deep.What Changed Since the 80s?In the old days people believed that neural networks could “solve everything”.  Why couldn’t they do it in the past?  There are several reasons.Computers were slow. So the neural networks of past were tiny.  And tiny neural networks cannot achieve very high performance on anything.   In other words small neural networks are not powerful.Datasets were small.   So even if it was somehow magically possible to train LDNNs there were no large datasets that had enough information to constrain their numerous parameters.   So failure was inevitable.Nobody knew how to train deep nets.   Deep networks are important.  The current best object recognition networks have between 20 and 25 successive layers of convolutions.  A 2 layer neural network cannot do anything good on object recognition.  Yet back in the day everyone was very sure that deep nets cannot be trained with SGD since that would’ve been too good to be true!It’s funny how science progresses and how easy it is to train deep neural networks especially in retrospect.Practical Advice.Ok.  So you’re sold.  You’re convinced that LDNNs are the present and the future and you want to train it.   But rumor has it that it’s so hard so difficult… or is it?   The reality is that it used to be hard but now the community has consolidated its knowledge and realized that training neural networks is easy as long as you keep the following in mind.   Here is a summary of the community’s knowledge of what’s important and what to look after:Get the data: Make sure that you have a high-quality dataset of input-output examples that is large representative and has relatively clean labels.   Learning is completely impossible without such a dataset.Preprocessing:  it is essential to center the data so that its mean is zero and so that the variance of each of its dimensions is one.   Sometimes when the input dimension varies by orders of magnitude it is better to take the log(1 + x) of that dimension.  Basically it’s important to find a faithful encoding of the input with zero mean and sensibly bounded dimensions.  Doing so makes learning work much better.   This is the case because the weights are updated by the formula: change in wij \propto  xidL/dyj (w denotes the weights from layer x to layer y and L is the loss function). If the average value of the x’s is large (say 100) then the weight updates will be very large and correlated which makes learning bad and slow.   Keeping things zero-mean and with small variance simply makes everything work much better.Minibatches: Use minibatches.  Modern computers cannot be efficient if you process one training case at a time.  It is vastly more efficient to train the network on minibatches of 128 examples because doing so will result in massively greater throughput.  It would actually be nice to use minibatches of size 1 and they would probably result in improved performance and lower overfitting;  but the benefit of doing so is outweighed the massive computational gains provided by minibatches.    But don’t use very large minibatches because they tend to work less well and overfit more.   So the practical recommendation is:  use the smaller minibatch that runs efficiently on your machine. Gradient normalization:  Divide the gradient by minibatch size.   This is a good idea because of the following pleasant property:  you won’t need to change the learning rate (not too much anyway) if you double the minibatch size (or halve it).Learning rate schedule:  Start with a normal-sized learning rate (LR) and reduce it towards the end.A typical value of the LR is 0.1.  Amazingly 0.1 is a good value of the learning rate for a large number of neural networks problems.   Learning rates frequently tend to be smaller but rarely much larger.Use a validation set ---- a subset of the training set on which we don’t train --- to decide when to lower the learning rate and when to stop training (e.g. when error on the validation set starts to increase).A practical suggestion for a learning rate schedule:   if you see that you stopped making progress on the validation set divide the LR by 2 (or by 5) and keep going.  Eventually the LR will become very small at which point you will stop your training.   Doing so helps ensure that you won’t be (over-)fitting the training data at the detriment of validation performance which happens easily and often.   Also lowering the LR is important and the above recipe provides a useful approach to controlling via the validation set.But most importantly worry about the Learning Rate.   One useful idea used by some researchers (e.g. Alex Krizhevsky) is to monitor the ratio between the update norm and the weight norm.  This ratio should be at around 10-3.  If it is much smaller then learning will probably be too slow and if it is much larger then learning will be unstable and will probably fail.Weight initialization.  Worry about the random initialization of the weights at the start of learning.   If you are lazy it is usually enough to do something like 0.02 * randn(num_params).    A value at this scale tends to work surprisingly well over many different problems.   Of course smaller (or larger) values are also worth trying.  If it doesn’t work well (say your neural network architecture is unusual and/or very deep) then you should initialize each weight matrix with the init_scale / sqrt(layer_width) * randn.   In this case init_scale should be set to 0.1 or 1 or something like that.   Random initialization is super important for deep and recurrent nets.  If you don’t get it right then it’ll look like the network doesn’t learn anything at all.  But we know that neural networks learn once the conditions are set.Fun story:  researchers believed for many years that SGD cannot train deep neural networks from random initializations.   Every time they would try it it wouldn’t work.  Embarrassingly they did not succeed because they used the “small random weights” for the initialization which works great for shallow nets but simply doesn’t work for deep nets at all.  When the nets are deep the many weight matrices all multiply each other so the effect of a suboptimal scale is amplified.But if your net is shallow you can afford to be less careful with the random initialization since SGD will just find a way to fix it.You’re now informed.  Worry and care about your initialization.  Try many different kinds of initialization.  This effort will pay off.   If the net doesn’t work at all (i.e. never “gets off the ground”) keep applying pressure to the random initialization. It’s the right thing to do.If you are training RNNs or LSTMs use a hard constraint over the norm of the gradient (remember that the gradient has been divided by batch size).  Something like 15 or 5 works well in practice in my own experiments.   Take your gradient divide it by the size of the minibatch and check if its norm exceeds 15 (or 5).  If it does then shrink it until it is 15 (or 5).  This one little trick plays a huge difference in the training of RNNs and LSTMs where otherwise the exploding gradient can cause learning to fail and force you to use a puny learning rate like 1e-6 which is too small to be useful.Numerical gradient checking:  If you are not using Theano or Torch you’ll be probably implementing your own gradients.   It is easy to make a mistake when we implement a gradient so it is absolutely critical to use numerical gradient checking.   Doing so will give you a complete peace of mind and confidence in your code.   You will know that you can invest effort in tuning the hyperparameters (such as the learning rate and the initialization) and be sure that your efforts are channeled in the right direction.If you are using LSTMs and you want to train them on problems with very long range dependencies you should initialize the biases of the forget gates of the LSTMs to large values.  By default the forget gates are the sigmoids of their total input and when the weights are small the forget gate is set to 0.5 which is adequate for some but not all problems.   This is the one non-obvious caveat about the initialization of the LSTM.Data augmentation:  be creative and find ways to algorithmically increase the number of training cases that are in your disposal.   If you have images then you should translate and rotate them;  if you have speech you should combine clean speech with all types of random noise;   etc.   Data augmentation is an art (unless you’re dealing with images).    Use common sense.Dropout.  Dropout provides an easy way to improve performance. It’s trivial to implement and there’s little reason to not do it.   Remember to tune the dropout probability and to not forget to turn off Dropout and to multiply the weights by (namely by 1-dropout probability) at test time.  Also be sure to train the network for longer.  Unlike normal training where the validation error often starts increasing after prolonged training dropout nets keep getting better and better the longer you train them.  So be patient.Ensembling.  Train 10 neural networks and average their predictions.  It’s a fairly trivial technique that results in easy sizeable performance improvements.   One may be mystified as to why averaging helps so much but there is a simple reason for the effectiveness of averaging.  Suppose that two classifiers have an error rate of 70%.   Then when they agree they are right.  But when they disagree one of them is often right so now the average prediction will place much more weight on the correct answer.  The effect will be especially strong whenever the network is confident when it’s right and unconfident when it’s wrong.I am pretty sure that I haven’t forgotten anything.  The above 13 points cover literally everything that’s needed in order to train LDNNs successfully.So to Summarize:LDNNs are powerful.LDNNs are trainable if we have a very fast computer.So if we have a very large high-quality dataset we can find the best LDNN for the task.Which will solve the problem or at least come close to solving it.The End.But what does the future hold?     Predicting the future is obviously hard but in general models that do even more computation will probably be very good.   The Neural Turing Machine is a very important step in this direction.   Other problems include unsupervised learning which is completely mysterious and incomprehensible in my opinion as of 8 Jan 2015.   Learning very complicated “things” from data without supervision would be nice.  All these problems require extensive research."
2014,12,20,Thoughts on NIPS 2014,http://yyue.blogspot.com/2014/12/thoughts-on-nips-2014.html,"NIPS 2014 happened last week and what a great conference it was.  Lots of great papers workshops and invited talks.  The NIPS ExperimentIn contrast to previous years the most talked about thing from NIPS this year was not any new machine learning approach but rather a reviewing experiment called the NIPS Experiment.In a nutshell about 10% of submissions were reviewed independently by two sets of reviewers (including two different Area Chairs).  The goal of the NIPS Experiment was to assess to what extent reviewers agreed on accept/reject decisions.  The outcome of the experiment has been a challenge to interpret properly.  The provocative and thought provoking blog post by Eric Price has garnered the most attention from the broader scientific community. Basically one reasonable way of interpreting the NIPS Experiment results is that of the papers accepted for publication at NIPS 2014 roughly half of them would be rejected if they were reviewed again by a different set of reviewers.  This of course highlights the degree of subjectivity and randomness (likely exacerbated by sub-optimal reviewing) inherent in reviewing for a such a broad field as machine learning.The most common way to analyze this is from a certain viewpoint about fairness.  I.e. if we had a budget for K papers did the top K submissions get published?  From that standpoint the answer seems to be a resounding no no matter how you slice it.  One can argue about the degree of unfairness which is a much murkier subject.Alternative Viewpoint via Regret MinimizationHowever as echoed in a blog post by Bert Huang NIPS was AWESOME this year.  The poster sessions had lots of great papers and the oral presentations were good. So I'd like to offer a different viewpoint about NIPS one based on regret minimization.  Let's assume that the accepted papers that were more likely to be rejected in a second review are ""borderline"" papers (seems like a reasonable assumption but perhaps there are arguments against it).  Then had we swapped out a bunch of borderline papers with other borderline papers that got rejected would the quality of the conference have been that much better?In other words given a budget of K papers to accept what is the collective quality of K papers actually accepted versus the quality of the ""optimal"" set of K papers we should've accepted?  It's conceivable that the regret on quality difference could be quite low despite the paper overlap being substantially different.One might even argue as alluded to here that long-term regret minimization (i.e. reviewing for NIPS over many years) requires some amount of randomness and/or disagreement between reviewers.  Otherwise there could be a more serious risk of group-think or intellectual inbreeding that can cause the field to stagnate.Not sure to what extent this viewpoint is appropriate.  For instance NIPS is also a venue by which junior researchers become established in the field.  Having a significant amount of randomness in the reviewing process can definitely be detrimental to the morale and career prospects of junior researchers. On to the Actual PapersThere were many great papers at NIPS this year.  Here are a few that caught my eye:Sequence to Sequence Learning with Neural Networks by Ilya Sutskever Oriol Vinyals & Quoc Le.Ilya gave hands down the best talk at NIPS this year.  Ever since it started becoming popular Deep Learning has carried with it the idea that only Geoff Hinton & company could make them work well. Ilya spent most of his talk describing how this is not the case anymore.  He also showed how to incorporate a type of gradient momentum called Long Short-Term Memory in order to do sequence-to-sequence prediction with deep neural networks.Learning Neural Network Policies with Guided Policy Search under Unknown Dynamicsby Sergey Levine & Pieter Abbeel.This paper combined reinforcement learning and neural networks in order to do policy search.  What's shocking about this approach is how few training examples they needed to train a neural network.  Granted the neural network wasn't very deep but still the low amount of training data is quite surprising.  Learning to Optimize via Information-Directed Samplingby Dan Russo & Benjamin Van Roy.Dan Russo has been doing some great work recently on analyzing bandit/MDP algorithms and proposing new algorithms. This paper proposes the first (mostly) fundamentally new bandit algorithm design philosophy that I've seen in a while.  It's not clear yet how to make this algorithm practical in a wide range of complex domains but it's definitely exciting to think about.Submodular meets Structured: Finding Diverse Subsets in Exponentially-Large Structured Item Setsby Adarsh Prasad Stefanie Jegelka & Dhruv Batra.This paper deals with how to do submodular maximization when the ground set is exponentially large.  This paper exploits specific structure in the ground set e.g. it can be solved via cooperative cuts in order to arrive an efficient solution.  It would be interesting to try to learn the diversity/submodular objective function rather than hand-craft a relatively simple one (from a modeling perspective).From MAP to Marginals: Variational Inference in Bayesian Submodular Modelsby Josip Djolonga & Andreas Krause.Log submodular models are a new family of probabilistic models that generalizes things like associative Markov random fields.  This paper shows how to perform variational marginal inference on log submodular functions which might be wildly intractable when viewed through the lens of conventional graphical models (e.g. very large factors that obey a submodular structure).  Very cool stuff.Non-convex Robust PCAby Praneeth Netrapalli Niranjan U N Sujay Sanghavi Animashree Anandkumar & Prateek Jain.This paper gives a very efficient and provably optimal approach for robust PCA where a matrix is assumed to be low-rank but except for a few sparse components. This optimization problem is non-convex and convex relaxations can often give sub-optimal results.  They also have a cool demo.How transferable are features in deep neural networks?by Jason Yosinski Jeff Clune Yoshua Bengio & Hod Lipson.Along with a scientific study on the transferability of neural network features Jason Yosinski also developed a cool demo that can visualize the various hidden layers of a deep neural network.  Conditional Random Field Autoencoders for Unsupervised Structured Predictionby Waleed Ammar Chris Dyer & Noah A. Smith.This paper gives a surprisingly efficient approach for learning unsupervised auto-encoders that avoids making overly restrictive independence assumptions.  The approach is based off CRFs.  I wonder if one can do this with a more expressive model class such as structured decision trees.A* Samplingby Chris J. Maddison Daniel Tarlow & Tom Minka.I admit that I don't really understand what's going on in this paper.  But it seems like it's doing something quite new so there are perhaps many interesting connections to be made here.  This paper also won one of the Outstanding Paper Awards at NIPS this year."
2014,10,13,How to Write an Academic Research Statement,http://yyue.blogspot.com/2014/10/how-to-write-academic-research-statement.html,"It's that time of year when junior researchers are preparing applications for academic positions.   One of the largest uncertainties that many people have is how to properly write a research statement that is typically part of the application package.  This post contains my thoughts on what a good Computer Science research statement should look like when applying to US and Canadian universities.  Please keep mind though that everyone's research profile is different so what worked for me may not exactly work for you.  To be perfectly honest the research statement is not the most important part of your application package -- the letters of recommendation are.  Your letter writers are accomplished researchers in your field of study and can place your work in context as well as compare you to other researchers (when they were at your current career stage).  Whether or not a hiring committee seriously considers you for an onsite interview is largely a function of your recommendation letters and any other research reputation you've managed acquire while disseminating your work (**).Nonetheless the research statement is still important especially once the hiring committee gets down to a short list and are basically trying to figure out which of the strong candidates seem like they would be the most interesting and impactful additions to the department.For reference here's my research statement when I was on the job market in Fall 2012.  I want to emphasize a few points:(1) As my history teacher Dr. Skinner would always say: ""Pithy and Erudition!"" In other words keep it short and to the point.  You have to optimize for the case when someone with very limited time is doing a quick read of your research statement.  No convoluted sentences and no long paragraphs.  As a general rule I'd say it's probably too long if it's more than 3 pages.  Optimize your wording to be as concise as possible.(2) Tell a Story.  Academics like to get excited by the potential of new research directions -- after all that's why many of us chose to pursue this line of work.  So make sure you have an overarching vision in mind.  For me I chose to talk about machine learning with humans in the loop as my central theme.  During my onsite interviews I was repeatedly asked to describe what my NSF CAREER proposal would look like.  The purpose of the question is so that the interviewer can get a sense of my research vision.  I quickly realized that I can just re-emphasize various aspects of my research statement as my answer.  This also helps create a consistent image of who you are as a researcher.(3) Don't Regurgitate Your CV.  Your letter writers will do a far better job of describing your previous accomplishments than you will in your research statement.  Trust in them to do that. Only describe your previous work to support the story you're trying to tell.  For me I used my previous work to demonstrate that machine learning with humans in the loop is both a broadly practical and an intellectually deep research area.  But I kept it to a bare minimum -- previous work took up just under 1 page in my research statement.  Your research statement is your one chance in your application package to describe your vision to the hiring committee.  Don't waste it all on dwelling in the past.  (4) It's OK to Stretch the Truth a Little Bit.  Because you're trying to keep the research statement concise you can't accurately describe all the details of your previous work.  For instance when I described my prior work I did not include all the caveats that necessarily come with any such research result.  That is OK; everyone understands that your research results have caveats.  People not in your area don't want to read a laundry list of assumptions and conditions that your result must be couched in.  And people who are interested will read your actual research papers.  You can explicitly highlight the more interesting limitations of your previous work when you talk about future research directions.(5) Don't Bullshit Too Much.  Of course you must be somewhat speculative when you're laying out your research vision and describing future research directions.  But make sure that your speculations are grounded in some kind of sound reasoning.  The easiest way to do this is to demonstrate that you've already done some preliminary work in the future directions you want to pursue.  For my research statement I listed one piece of preliminary work that I've done for each future direction.  This is also a nice way to incorporate the more interesting peripheral parts of your CV into your research vision.  (6) Get Lots of Feedback and Iterate.  I had  many great mentors and colleagues who contributed significantly in helping to sharpen the wording and focus of my research statement.  Again not all of these points may work for everyone and I'm sure there are plenty of other good tips that I didn't mention (examples here and here).  But hopefully this was useful to some.  Best of luck everyone!(**) This is not to say that your actual accomplishments are not important.  If there is no substance to your work then your letter writers won't write you strong letters and your research won't have garnered you much recognition and reputation.  Having done substantial work is assumed by default in this post."
2014,10,3,Caltech CMS Faculty Opening 2014,http://yyue.blogspot.com/2014/10/caltech-cms-faculty-opening-2014.html,The CMS department is growing!  We have an tenure-track faculty opening -- see the official ad here.  We are interested in outstanding candidates from all areas of applied math and computer science. We value high-impact and cross-cutting fundamental research more than the specific area or discipline.  However I personally would be delighted if we developed a stronger presence in computational linguistics and/or network science (with a healthy dose of machine learning sprinkled in of course).  Please apply!
2014,8,26,NIPS 2014 Workshop on Personalization,http://yyue.blogspot.com/2014/08/nips-2014-workshop-on-personalization.html,"Call for PapersPersonalization: Methods and Applications a workshop in conjunction with the 28th Annual Conference on Neural Information Processing Systems (NIPS 2014)December 12 or 13 2014 – Montreal Canadahttps://sites.google.com/site/nips2014personalizationDeadline for Submissions: October 9 2014Overview--------From online news to online shopping to scholarly research we are inundated with a torrent of information on a daily basis. With our limited time money and attention we often struggle to extract actionable knowledge from this deluge of data. A common approach for addressing this challenge is personalization where results are automatically filtered to match the tastes and preferences of individual users.  This workshop aims to bring together researchers from industry and academia in order to describe recent advances and discuss future research directions pertaining to the personalization of digital systems broadly construed.  We aim to highlight new and emerging research opportunities for the machine learning community that arise from the evolving needs for personalization.Format and Submissions----------------------This is a one-day workshop. The program will feature five invited talks poster spotlights a poster session and a panel discussion.We welcome the following types of papers:1. Research papers that introduce new models or methodology or apply established models/methods to novel domains and data sets; or2. Research papers that explore theoretical and computational issues.We encourage submissions from a wide range of disciplines from machine learning to HCI to the social sciences.  Topics of interest include (but are not limited to):- Learning of fine-grained representations of user preferences- Large-scale personalization- Interpreting observable human behavior- Interactive algorithms for ""on-the-fly"" personalization- Learning to personalize using rich user interactions- Modeling complex sensemaking goals- Applications beyond conventional recommender systemsSubmissions should be 4-8 pages long and adhere to the NIPS format (http://nips.cc/Conferences/2014/PaperInformation/StyleFiles).Please make the author information visible on submissions. Submissions will be accepted through the following website:https://easychair.org/conferences/?conf=nipspersonalization2For up-to-date information on the workshop please check:https://sites.google.com/site/nips2014personalizationDeadline for Submissions: October 9 2014 [11:59pm Honolulu time]Notification of Decision: October 23 2014Organizers:-----------------Khalid El-Arini FacebookYisong Yue CaltechDilan Gorur MicrosoftContact: kelarini@fb.com"
2014,8,6,Resnick Sustainability Institute Postdoctoral Fellowship,http://yyue.blogspot.com/2014/08/resnick-sustainability-institute.html,"The Resnick Sustainability Institute has openings for postdoctoral fellowships.  The Resnick Sustainability Institute is Caltech’s ""studio"" for sustainability science and is dedicated to supporting the cutting-edge science and creative problem solving necessary to change the balance of the world’s sustainability.The postdoctoral fellowship was created to attract outstanding recent graduates to Caltech working on projects that explore new directions in sustainability focused science and engineering research. The Resnick fellows will have support for up to two years to work on creative cross-catalytic research that complements the existing work of the Caltech faculty or that creates new research directions within the mission areas of the Resnick Sustainability Institute. Eligible candidates will have completed their PhD within five years of the start of the appointment and must have secured a commitment from one or more Caltech faculty member to serve as a mentor and provide office/lab space for the length of the fellowship. Candidates can come from any country provided they are proficient in English. Applications consisting of a research proposal cover letter recommendations and CV can be submitted electronically here.  Applications are due by October 13th 2014. Any questions can be directed to rpd@caltech.edu."
2014,5,22,Large-Scale Sports Analytics Workshop (KDD 2014),http://yyue.blogspot.com/2014/05/large-scale-sports-analytics-workshop.html,I'm delighted to be co-organizing the Large-Scale Sports Analytics Workshop at KDD 2014 in August later this year in New York City.  We have a great lineup of invited speakers and we're also inviting poster submissions for preliminary and recently published research.  See the Call for Papers below. In general KDD this year looks to be amazing so I encourage everyone to attend.KDD 2014 Workshop on Large-Scale Sports Analytics=== Call for Submissions === When: August 24th Where: New York City NYWebsite: http://large-scale-sports-analytics.org/Description: Virtually every aspect of sports analytics is now entering the “Big Data” phase and the interest in effectively mining modeling and learning from such data has also been correspondingly growing. Relevant data sources include detailed play-by-play game logs tracking data physiological sensor data to monitor the health of players social media and text-based content and video recordings of games.The objective of this workshop is to bring together researchers and analysts from academia and industry who work in sports analytics data mining and machine learning. We hope to enable meaningful discussions about state-of-the-art in sports analytics research and how it might be improved upon. We seek poster submissions (which can be both preliminary research as well as recently published work) on topics including but not limited to:* Spatiotemporal modeling* Video text and social media analysis* Feature selection and dimensionality reduction* Feature learning and latent factor models* Computational rationality* Real-time predictive modeling* Interactive analysis & visualization tools* Sensor technology and reliability* Labeling and annotation of events/activities/tactics* Real-time/deployed analytical systems* Knowledge discovery of player/team/league behaviors* Game theorySubmission Details:Poster submissions should be extended abstracts no more than 4 pages in length (in KDD format do not need to be anonymous).  Extended abstracts should be submitted by June 17th 11:59 PM PDT and can be submitted electronically via: https://cmt.research.microsoft.com/LSSA2014Important Dates:Submission - 17th June 2014 11:59 PM PDT Notification - 8th July 2014Workshop - 24th August 2014Organizers:Yisong Yue (Disney Research) Patrick Lucey (Disney Research) Peter Carr (Disney Research) Jenna Wiens (MIT) 
2014,3,17,Grad School Decisions,http://yyue.blogspot.com/2014/03/grad-school-decisions.html,"In the computer science PhD world prospective students are just about wrapping up their school visits and they will soon make a decision on where to spend the next 5-6 years of their lives.  The decision can often be a difficult one and typically must be made with imperfect information conflicting advice from others and uncertainty about how to weigh the various influencing factors.  In my opinion the **single biggest indicator** of grad school success is how well you work/fit with your advisor. Ben Barres recently wrote a great article expounding on this very issue.  I won't bother with repeating all the great points he made but I want to emphasize some things from a computer science perspective.It's important to work with an advisor who has an exciting vision rather than just doggedly work someone whose research area matches the narrow scope of what you worked on as an undergrad.  Keep in mind that a significant fraction of students (perhaps as many as 50%) switch areas after starting grad school from what they declared in their applications.  Of course most switches are to neighboring areas that have significant overlap in the technical foundations.  But the lesson to take away here is that you should keep an open mind about what research you might find interesting.  It's important to have a good working relationship with your advisor.  Research is a very unpredictable process with a lot of highs and lows. You will almost invariably hit some bumps while working with your advisor and it's important to have a good rapport with your advisor when working through those bumps.  Assessing this somewhat intangible ""fit"" can be hard to do a priori but simply talking to a potential advisor can often yield some warning signs or illuminate that there's a good match.  Talking to other students who currently work or have previously worked with the potential advisor can also be useful.  But beware of students who have no first hand experience with the potential advisor -- sometimes students just like to say things =)Some professors are more hands on than others.  Some professors like their students to work in groups while others like their students to work alone.  Some professors want their students to take a deep dive for 1-2 years on a grand project while others want their students to be more focused on the next 4-6 month project.  All of these work styles can lead to successful research outcomes but it has to work for both the advisor and the student. And of course some professors are more flexible to adapt their advising style to match the student than others. Working with the right advisor is usually more important than quibbles over funding.  Most schools guarantee funding for PhD students.  But if your advisor is low on funds then you may have to spend a few extra terms as a teaching assistant rather than be funded directly by your advisor as a research assistant.  In my opinion this is completely worth it if it's for the right advisor (the one caveat being that the school should have reasonable TA workloads).  However if your advisor is completely broke and can't fund anything (including machines and travel) then that can become problematic.Good luck to everyone making decisions!"
2014,1,23,New Caltech PhD Program,http://yyue.blogspot.com/2014/01/new-caltech-phd-program.html,"I'm excited to announce that Caltech will be starting a new Ph.D. option next year called the Computing and Mathematical Sciences (CMS) option.  This program will emphasize data-intensive algorithmic thinking broadly construed. Students will study topics such as optimization statistics machine learning economics privacy network science and optimal control.  Students will also be expected to perform cross-cutting research applied to various scientific disciplines such as geology chemistry biology and astronomy.  This program is similar in spirit to many other interdisciplinary ""data science"" and ""systems science & engineering"" programs now popping up in universities around the world.  One distinguishing feature of the Caltech program is that there will be a very heavy emphasis on the mathematical foundations as well as a close collaboration with scientists (which is possible due to Caltech's small size).  Students who find themselves conflicted between computer science economics and engineering should find this program particularly appealing. We hope this program will help bring about a unification of the mathematical foundations underpinning the many data-intensive science and engineering problems that we face today.See this post by Adam Wierman for more details.  Special thanks to Adam Katrina Ligett Venkat Chandrasekaran and Joel Tropp for doing all the heavy-lifting to make this program happen."
2013,12,17,Caltech Electrical Engineering Faculty Opening,http://yyue.blogspot.com/2013/12/caltech-electrical-engineering-faculty.html,The Electrical Engineering department at Caltech has a faculty opening and is targeting machine learning as a key area of interest.  Reviewing of applications has just begun (started December 15) but all applications received by January 15th will be reviewed.Caltech is an extremely small school (~300 faculty total) and departmental boundaries are almost meaningless.  I anticipate having many connections to the Electrical Engineering department so any machine learning person who joins will definitely not feel alone.  So please apply if you're interested!
2013,12,14,Thoughts on NIPS 2013,http://yyue.blogspot.com/2013/12/thoughts-on-nips-2013.html,I recently attended NIPS 2013 at South Lake Tahoe which was an absolutely fantastic conference (the limited dining choices and general casino debauchery notwithstanding).  Highlights included listening to an awesome talk by Daphne Koller on online education (and Coursera in particular) and meeting Mark Zuckerberg and learning about the new Facebook AI research lab. I also attended the Ben Taskar Memorial Session which while very saddening was also inspiring to myself and hopefully many others who attended.  Ben Taskar was a truly exceptional researcher and human being and it was very touching listening to his friends and collaborators talk about and celebrate his life and work.  Ben leaves behind a wife and daughter who requires around-the-clock care.  I encourage everyone who can to donate to his memorial fund.*EDIT* -- you can read more about my thoughts on this Quora answer.The NIPS workshops have consistently been my favorite machine learning event over the past several years.  This year I was privileged to be an invited speaker at the Discrete and Combinatorial Problems in Machine Learning workshop. The guest of honor at the workshop was Kazuo Murota who gave a great overview on his work on discrete convexity (slides here).  As was usual I jumped around between several workshops including Bayesian Optimization Machine Learning for Sustainability and Machine Learning for Clinical Data and Healthcare.  Daniel Russo presented a very interesting result at the Bayesian Optimization workshop showing a very simple way of analyzing Thompson sampling based on existing analysis for UCB-style algorithms.  He mainly showed results in a Bayesian regret setting and I wonder if one could extend his result to show account for the full spectrum between Bayesian expected regret and worst-case regret. There were many great papers at NIPS this year (more than I could get around to reading it seems like).  Here are a few that I found particularly interesting:Sequential Transfer in Multi-armed Bandit with Finite Set of Models by Mohammad Azar Alessandro Lazaric and Emma Brunskill.  This paper shows how to use knowledge learned from previous online-learning problems to learn faster for new online learning problems (i.e. transfer learning).  They leverage a recent result on learning tensor decompositions in order to arrive at a provably data-efficient approach.  Such problems are particularly relevant in many online systems which need to continuously personalize to new users and domains.High-Dimensional Gaussian Process Bandits by Josip Djolonga Andreas Krause and Volkan Cevher.  Like the above-mentioned paper this paper addresses how to transfer learning across different actions.  Rather than focusing on sequential transfer learning like above this paper instead treats all the actions (e.g.  items) and contexts (e.g. users) jointly as points in very high dimensional space.  This paper leverages a different result on low-rank matrix recovery.  I've thought about this type of approach a little while back and am really excited that the authors got this method to work.  One thing that kind of bugs me is that the first stage requires a very restrictive random sampling technique which makes the result less useful in practice.Eluder Dimension and the Sample Complexity of Optimistic Exploration by Daniel Russo and Benjamin Van Roy. This paper proposes a new measure of complexity for analyzing online learning problems.  This seems to be a refreshingly unique perspective on the problem although the relationship with more understood measures such as information gain is currently unclear.  But it does have the potential to help us analyze more complex prediction settings.Multi-Task Bayesian Optimization by Kevin Swersky Jasper Snoek and Ryan Adams.  This paper is very cool.  Hyper-parameter tuning is a big problem in machine learning. This method proposes a way to use a smaller dataset to extrapolate likely outcomes when running a learning algorithm (with a certain hyper-parameter setting) on the larger actual dataset.  The experiments show that it can offer SIGNIFICANT speedups compared to standard approaches.BIG & QUIC: Sparse Inverse Covariance Estimation for a Million Variables by Cho-Jui Hsieh Matyas Sustik Inderjit Dhillon Pradeep Ravikumar and Russell Poldrack.  Estimating high-dimensional inverse covariance matrices is an increasingly more important problem in machine learning. This method absolutely blows the competition out of the water.Learning Adaptive Value of Information for Structured Prediction by David Weiss and Ben Taskar.  This paper is also very cool.  It is essentially a meta-learning algorithm that trains an adaptive policy on which features to compute for the base classifier to use.  For certain applications computing features can be very expensive and this paper shows how to achieve almost identical performance while using only a small fraction of the features.  Furthermore certain features can be harmful for certain classification instances and the authors show how this method can actually predict to avoid computing such features depending on the problem instance.  There's a lot about this setup that I don't quite understand from a theoretical perspective but it seems very powerful.Latent Structured Active Learning by Wenjie Luo Alex Schwing and Raquel Urtasun.  Active learning for structured prediction models is an important problem.  Most active learning results address only the unstructured setting e.g. binary labels.  This issue is even more important in structured models because acquiring structured labels is typically much more expensive than binary labels.  The authors propose a method for actively eliciting labels on components of the full structured prediction problem (thus leaving much of the structure latent).  One thing that's worth exploring here is a mixed-initiative approach rather than a purely system-driven approach.
2013,11,19,Remembering Ben Taskar,http://yyue.blogspot.com/2013/11/remembering-ben-taskar.html,"Ben Taskar passed away recently.  It's a sudden and tragic loss for the machine learning community and our deepest condolences go out to his friends and family.  I've only known Ben professionally; we've chatted a few times over the years.  By all accounts he was a wonderful and kind person.  More information can be found here.I knew Ben mostly through his research which has been exceptional throughout his career as well as an inspiration for my own work.  The term ""rising star"" is reserved for a select few in the research community and Ben most definitely deserved that moniker. In graduate school I literally began my machine learning career reading Ben Taskar's papers.  Ben was a pioneer in the area of Structured Prediction.  His thesis work on Max Margin Markov Networks was a revelation and has stood the test of time as one of those foundational papers that people refer to over and over again. He's also done some great follow-up work extending it as well.But Ben was only just getting started. More recently his group has done some extremely elegant work extending the limits of what structured prediction models can be applied to.  For example his work on Structured Prediction Cascades (with David Weiss) was one of the first principled approaches for discriminatively learning efficient approximations of complex structured prediction models with learning-theoretic guarantees.  As another example his work on Structured Determinantal Point Processes (with Alex Kulesza) is quite possibly the most elegant way of building probabilistic models of redundancy that I've encountered thus far. Every year for the past several years I would regularly browse his website in anticipation of finding interesting new papers that his group has recently published. Although the shock I'm feeling must pale dramatically in comparison to that felt by his family and friends it is nonetheless profoundly saddening that this great star in the machine learning community has seen his chapter end so abruptly and prematurely."
2013,10,26,Caltech CMI Postdoc Openings,http://yyue.blogspot.com/2013/10/caltech-cmi-postdoc-openings.html,Caltech's Center for the Mathematics of Information (CMI) announces openings in the CMI Postdoctoral Fellowship Program starting in fall 2014. The CMI is dedicated to fundamental mathematical research with an eye to the roles of information and computation throughout science and engineering. Areas of interest include algorithms complexity algorithmic game theory applied combinatorics applied probability statistics machine learning information and coding theory control optimization networked systems geometry processing multi-resolution methods and molecular programming.Please apply and have three reference letters sent directly as instructed at:http://www.ist.caltech.edu/joinus/positions.htmlAll candidate materials are due by&nbsp; Friday December 20 2013 &nbsp;and reference letters are due by Monday December 23 2013.Positions are contingent upon completion of the PhD. Caltech is an affirmative action/equal opportunity employer; women minorities veterans and disabled persons are encouraged to apply.
2013,7,31,"Disney, Caltech",http://yyue.blogspot.com/2013/07/disney-caltech.html,"As many of you know I was kept quite busy on the faculty job market this past spring when I spent three months traveling almost non-stop. My time as a postdoc at Carnegie Mellon University was a very stimulating and productive one but it is now coming to a close. Many thanks to my supervisors Carlos and Krishnan as well as all my other colleagues and collaborators. CMU truly is an amazing place with its sheer scale of high-impact research activity.I'm delighted to announce that I've accepted an assistant professor position in the Computing and Mathematical Sciences Department at the California Institute of Technology.When visiting Caltech I was very drawn to the technical rigor as well as the intense interdisciplinary focus that permeate the entire faculty. For instance I was pleasantly surprised to find common ground with geologists economists and biologists there.This interdisciplinary culture is in part due to Caltech's extremely small size (only ~300 faculty total). In fact I will be one of the very few machine learning researchers at Caltech and I'm looking forward to interacting and collaborating with a wide range of researchers there.However I won't be starting at Caltech just yet. Starting next week I will be spending one year at Disney Research&nbsp;(and defer my start at Caltech to fall 2014). I'm treating this as an chance to do a pre-sabbatical in industry before starting my tenure-track job.Disney Research tackles an impressive range of interesting data mining and modeling challenges (including advanced video analysis data-driven animation and cyber-physical sensor systems) and also currently do not have an overly large machine learning presence. I hope to use this opportunity to find ""untouched"" ground to push machine learning in new directions that I find interesting.Finally I'd like to thank all of my reference letter writers&nbsp;Thorsten&nbsp;Carlos&nbsp;Bobby&nbsp;and Krishnan&nbsp;for all their support and guidance throughout my job search process. It's been a hectic transitional year for me and I'm very much looking forward to diving back into research!"
2013,7,17,Interpretable Predictive Models,http://yyue.blogspot.com/2013/07/interpretable-predictive-models.html,"I recently attended a workshop on media at the NYU Center for Urban Science and Progress.  Many thanks to Arun Sundararajan and Maria Liakata for planning the workshop which facilitated a very interesting exchange of ideas from people in very different areas.One issue that was discussed extensively during the workshop is the need for interpretable predictive models.    Since the actual definition of ""interpretable"" is up for debate I concluded that (at least for now) it's more useful to talk about when and why a model isn't interpretable and what problems that might cause.The primary over-arching use-case is when an end-user is using the model not only for superior predictive power but also for deriving insight from a large dataset or problem domain. Examples include: Investigative analysis that aims to use a predictive model for policy making. Debugging/building a complicated predictive model for commercial purposes.In both examples the decision-making process of the model needs to be somehow understandable (at least at a high level) and thus trustworthy.  Unfortunately effective predictive models are typically very complex with potentially billions of parameters makes them difficult to interpret/understand using conventional means (i.e. inspecting the individual parameters). For many settings one effective approach could be to explain the behavior of the model on specific problem instances rather than the model as a whole.  There appears to be (at least) two types of approaches for this.Sensitivity AnalysisOne approach that Foster Provost mentioned at the workshop is to perform a form of sensitivity analysis to understand which alternative scenarios would cause the model predict something different.  For example weather patterns such as tropical storms are notoriously difficult to model and any prediction on the future behavior of such storms also come with a so-called cone of uncertainty.  One type of useful analysis would be to understand what factors might case the hurricane to fall within different regions of the cone of uncertainty (assuming the model isn't just using simple context-free stochastic process to explain uncertainty).  This type of sensitivity analysis can be done by varying the different input attributes into the model and then summarizing evaluating the likely outcomes.  Typically analysts would do a significant chunk of this work manually which significantly limits the scalability of such techniques.  It would be interesting to develop automated ""meta-analysis"" algorithms that can perform large-scale sensitivity analysis and summarization for large classes of predictive models.Structured ModelsAnother way for models to explain or justify their predictions is to actually build such capabilities into the predictive model.  Many prediction domains require structured models that can make complex predictions.  For example recent work by Yun Jiang use a model of hallucinated humans to understand a scene (such as a room) and how to predict where objects should be placed.  In this case the hallucinated human serves as a way for the model to explain that e.g.  a sofa should be placed opposite of a television.  When analyzing any particular scene it would be interesting to develop useful ways of exposing salient aspects of the hallucinated humans (which is typically referred to as a hidden or latent part of the model) to the end user.  Another example is my previous work on sentiment analysis with Ainur Yessenalina where we built a model to predict the sentiment of a movie review or congressional speech.  The model justifies its predictions by also extracting the sentiments that best explain the overall prediction.These models make the (sometimes implicit) assumption that for any particular problem instance a small set of factors contribute the bulk of reasoning behind the model's prediction for that instance.  Note that the set of contributing factors can vary for different problem instances.  Such ""structured"" models are essentially modeling the data at a level of granularity that is more expressive than a simple prediction (e.g. the likely human poses or the most opinionated sentences) but also less complex than the raw data.  It would be interesting to develop these types of models to be more amenable to human inspection and modification."
2013,5,13,Two Thoughts on the Academic Job Market,http://yyue.blogspot.com/2013/05/two-thoughts-on-academic-job-market.html,"As some of you know I'm on the academic job market this year. The whole process for me isn't over yet (so I can't divulge any details regarding my situation) but I wanted to talk about two issues about the academic job search that have been bothering me.Head Count Capped By EnrollmentAt almost every university the tenure-track faculty head count is more-or-less capped by undergraduate enrollment (or a professional school enrollment if you're looking for a job in say a business school).  Yet the dean of EVERY university I interviewed at told me that the primary responsibility of (new) tenure-track faculty is to engage in basic research.I'm sure that if one thought hard enough about the issue (and maybe perused some history books) then one could reverse engineer how this whole situation came to be.  Here are how things look from where I stand:  Tenure-track positions have historically been prestigious because of the job-security and this continues to be the case. The budget for tenure-track salaries have historically been computed based on student enrollment. For most schools this means primarily undergraduate enrollment.  It's unclear to what extent tenure-track salaries are currently paid for out of student tuition revenues but I can imagine that was the case in the past. Ever since the end of World War II the US federal government has invested in basic research (through funding agencies such as the National Science Foundation).  Intellectual scholarship is the first-order bit that dictates prestige in academia. These days most of that is based on scientific research.Not only are tenure-track positions few in number but most other research positions are significantly lacking (by comparison) in the way of salaries and benefits.  My sense is that it's difficult to find comfortable non-tenured (i.e. ""soft money"") research positions.  This situation strikes me as rather odd (and wrong) and I definitely think that the number of comfortable positions available for basic research is too low.The End GamePart 1: Short Fuse OffersCandidates spend months preparing their application.  Job search committees also spend a long time reviewing applications and interviewing short-listed candidates.  Yet the final few weeks of the academic job market are filled with rapid-fire offers that have short fuses (i.e. quick expiration dates).  For example someone I know received an offer with a one-week deadline well before he'd finished all his interviews.  From the schools' perspective an early-deadline offer puts pressure on candidates to make a decision.  Part of the rationale behind this strategy is because schools typically have a list of candidates they'd like to make offers to (and they often aren't allowed to make many offers in parallel).  Of course the problem with this strategy is that most candidates (including myself) don't like feeling pressured into making a decision that has such a large impact on our lives.  Part 2: DeadlockIn the end many candidates push back on (or at least they should) the few schools they're serious about claiming that they need more time to make a decision about each school.  There might be several plausible reasons why candidates need more time including: Managing added complexities to one's personal life Wanting to be absolutely sure given the life-changing nature of the decision Waiting for other schools to get back to them before making a final decisionSchools will typically allow deadlines to be extended if they realize that the candidate is serious about them. Of course knowing this fact didn't change the squirmy feeling I got when I asked for deadlines to be pushed back. Many candidates will get wait-listed by schools that they'd rather hear back from before making a final decision.  This situation arises since virtually all departments can only make a small number of outstanding offers even if they'd ideally like to make more (typically the dean limits headcount based on enrollment or other factors). Should one of the department's outstanding offers get declined then the department is free to make a second offer (hence the short-fuse deadlines from Part 1).  In most cases everyone is just waiting on a few candidates to make up their minds after which everything cascades and falls into place.  However a deadlock arises when a cycle is created: Candidate 1 gets an offer from School A but prefers School B Candidate 2 gets an offer from School B but prefers School A School A intends to give a second-round offer to Candidate 2 School B intends to give a second-round offer to Candidate 1In essence both candidates get offers from their second choice which causes them to push back their deadlines indefinitely (well not actually indefinitely but you get the idea). Given the limited amount of information sharing and cooperation between schools and candidates this type of situation can easily drag on for weeks. This process is in my opinion severely flawed.  However it's unclear if there's a good solution out there that doesn't require schools (and candidates) to more openly share information and cooperate in some fashion."
2013,4,23,Human Supremacy Bias,http://yyue.blogspot.com/2013/04/human-supremacy-bias.html,"Every once in a while I'm reminded of the implicit (or sometimes explicit) human bias that humans are at (or near) the proverbial center of the universe.For instance Thomas Nagel's recently published book Mind & Cosmos argues that the theory of evolution is false (and will soon be replaced by a more comprehensive theory). More notably the book argues that the main failing of current evolutionary science is that:It fails to account for how consciousness fits into the natural order. Instead it regards it as an afterthought an accidental quirk a trinket on the tree of life less important to life’s story than the random physical mutations of genes.The thing that I find notable about this argument is its elevation of [human] consciousness to the forefront of the argument.  This whole thing reminds me of a conversation I had a few years ago.  While discussing what rights a hypothetical general AI agent should have someone (let's call this person Person A) told me that even if one could construct an artificial intelligence whose cognitive capacity was indistinguishable from humans that Person A could not accept that being as an ""equal"".  I then rephrased the query supposing this general AI agent were to be (at least superficially) indistinguishable from real humans (think Bicentennial Man).  In that setting Person A felt it would be OK to grant such a being equal rights as humans.  Person A noted that in retrospect some kind of low-level emotional response played a significant role in his/her thought process.I suspect that a similar type of emotional response (e.g. regarding the supremacy of human consciousness) is driving Thomas Nagel's viewpoint.  The above linked article further describes Nagel's other views which lends credence to this suspicion -- e.g. talking about how human consciousness is ""part of the lengthy process of the universe gradually waking up and becoming aware of itself.""  I'll not waste time talking about my personal views on this issue (I am a materialist).  But I think it's important to realize when one's (subconscious) biases and emotional responses may be driving an entire intellectual agenda -- i.e. ""the theory of evolution must be wrong because it currently explains human consciousness as the result of unguided mutations and natural selection."""
2013,3,15,Reviewing Season,http://yyue.blogspot.com/2013/03/reviewing-season.html,"It's reviewing season for many machine learning conferences (and for conferences in other fields as well). &nbsp;My personal sample size is rather small but I must say that the quality of reviewing has improved noticeably over the recent years.I've been especially pleased with the ICML reviewing system this year which has taken steps towards becoming more of a hybrid conference/journal system. &nbsp;Being conference-driven (rather than journal-driven) once yielded significant benefits in terms of rapid dissemination of new research results. &nbsp;However given the rapidly growing number of submissions (as well as the fact that everything is submitted and reviewed electronically these days) it no longer makes sense to constrain submitting and reviewing to a few brief time intervals every year. &nbsp;This paper by H V Jagadish&nbsp;provides a nice discussion of the benefits of a fully hybridized conference/journal system.If I were to nitpick one thing it would be the lack of information regarding the levels of expertise and interest that reviewers have for various submissions. &nbsp;For example in ICML most reviewers seem to only express interest in a small number of submissions thus often requiring an Area Chair to ""infer"" whether a reviewer would be a good fit for reviewing any given submission. &nbsp;Although most conferences require submissions to enter in keywords or subjects this often does not provide well-calibrated information.One approach that I've enjoyed using in the past is the&nbsp;Toronto Matching System which is able to learn a matching between my areas of expertise and the technical areas contained in a submission. &nbsp;I hope that most conferences adopt something similar in the near-future because it reduces the amount of human effort needed to find good matchings between reviewers and submissions."
2012,12,12,Thoughts on NIPS 2012,http://yyue.blogspot.com/2012/12/thoughts-on-nips-2012.html,I've recently returned from this year's NIPS conference which is the largest annually held machine learning conference.  The program was excellent but the venue left something to be desired.NIPS is currently undergoing a transition period.  During the previous decade NIPS was always held in Vancouver (main conference) and Whistler (workshops) which were excellent locations.  However because of increased costs (in part due to the 2010 winter Olympics) the two recent NIPS conferences were held in Grenada (last year) and South Lake Tahoe (this year) respectively.This year's conference was held jointly at the Harveys and Harrah's hotels.  The two hotels are located across the street from each other and are joined via an underground floor filled with restaurants casinos arcades and the like. I suspect the rather confusing path from one venue to the other was done quite deliberately by the hotels in an effort to promote gambling.  Unfortunately for them rather few of the 1000+ NIPS attendees participated in the smoke-filled debauchery (which I heard peeved the hotels quite a bit).  Nonetheless the winding maze was successful at limiting my exposure to the outdoors. I actually spent my first 36 hours completely indoors after which I started feeling claustrophobic. In fact one fellow attendee spent 5 days (that's 120 hours) straight indoors before finally venturing outside.  The conference program was fantastic. Here are some of the papers that I found particularly interesting:Discriminative Learning of Sum-Product Networks -- sum-product networks are a new deep learning architecture that yields tractable inference.  Deep architectures are the most expressive machine learning models in existence but are notoriously difficult to train.  This paper shows how to discriminatively train sum-product networks (this is a bit of misnomer -- max-product networks really) which leads to significantly improved prediction accuracy. Imitation Learning by Coaching -- imitation learning is a learning approach where a human expert teaches a computer program how to behave within some environment.  Often times the actions of a human expert are too difficult for a computer program to initially learn and so the program might be better served by first learning how to perform easier actions.  This paper proposes such an approach (which they call Coaching) and demonstrates improved theoretical guarantees and empirical performance.Near-Optimal MAP Inference for Determinantal Point Processes -- determinental point processes (DPPs) have recently gained visibility in the machine learning community.  A DPP is a probabilistic model that encourages predictions to be diverse.  This paper shows how to perform MAP inference with DPPs which were previously difficult to do well.  One thing I'm not sold on is why one would want to do MAP inference with a DPP. DPPs spend considerable model capacity learning a probability distribution which MAP inference throws away.  If one wanted to do MAP inference why not just use a discriminatively trained model instead?  Practical Bayesian Optimization of Machine Learning Algorithms -- let's face it parameter tuning is a pain in the ass. The naive thing to do (which I have been guilty of on several occasions) is parameter grid search which scales exponentially with the number of tuning parameters. This paper shows how to frame parameter tuning as a goal-oriented active learning problem (my terminology).  The main difference with classical active learning is that the final performance of the model is evaluated on the best action the model can take (i.e. predicting the best parameter setting) rather than predicting the performance of all actions (i.e. predicting the response variable of all inputs).  This style of approach could potentially be useful for more structured active learning problems as well.A Spectral Algorithm for Latent Dirichlet Allocation -- spectral algorithms have become increasingly popular for learning various latent variable models in machine learning (started by this paper).  In contrast to the common alternative Expectation Maximization spectral algorithms are exact learning algorithms subject to there being sufficient training data.  This paper shows how to do spectral learning for Latent Dirichlet Allocation which is pretty cool.  This particular spectral learning approach is largely a theoretical result so it will be interesting to see how practical it is (or if it could be made sufficiently practical).And as usual the NIPS workshops were fantastic.  I particularly enjoyed the Bayesian Optimization & Decision Making and Personalizing Education With Machine Learning workshops. Andrew Ng gave an inspiring presentation on Coursera and the potential of online education. Online education (in its current form) certainly has its flaws but it definitely seems that they will be an integral part of the education process moving forward.
2021,6,9,AzureR update: new in May/June,https://blog.revolutionanalytics.com/2021/06/mayjune-azurer-update.html,by Hong Ooi This is a summary of the updates to AzureR family of packages in May and June 2021. AzureAuth Change the default caching behaviour to disable the cache if running inside Shiny. Update Shiny vignette to clean up redirect page after authenticating (thanks to Tyler Littlefield). Add a create_AzureR_dir function to create the caching directory manually. This can be useful not just for non-interactive sessions but also Jupyter and R notebooks which are not technically interactive in the sense that they cannot read user input from a console prompt. AzureGraph Add enhanced support for the paging API. Many...
2021,4,15,Microsoft365R 2.1.0 with Outlook support now on CRAN,https://blog.revolutionanalytics.com/2021/04/microsoft365r-210-with-outlook-support-now-on-cran.html,by Hong Ooi I’m happy to announce that Microsoft365R 2.1.0 is now on CRAN with Outlook email support! Here’s a quick summary of the new features: Send reply to and forward emails optionally composed with blastula or emayili Copy and move emails between folders Create delete copy and move folders Add remove and download attachments Here’s a sample of how to write an email using blastula: library(Microsoft365R) # 1st one is for your personal Microsoft account # 2nd is for your work & school account outl <- get_personal_outlook() outlb <- get_business_outlook() # compose an email with blastula library(blastula) bl_body <-...
2021,3,30,Outlook client support in Microsoft365R available for beta test,https://blog.revolutionanalytics.com/2021/03/outlook-client-in-microsoft365r-beta.html,"by Hong Ooi This is an announcement that a beta Outlook email client is now part of the Microsoft365R package. You can install it from the GitHub repository with: devtools::install_github(""Azure/Microsoft365R"") The client provides the following features: Send reply to and forward emails optionally composed with blastula or emayili Copy and move emails between folders Create delete copy and move folders Add remove and download attachments The plan is to submit this to CRAN sometime next month after a period of public testing. Please give it a try and give me your feedback: either via email or by opening an issue..."
2021,3,10,Teams support in Microsoft365R,https://blog.revolutionanalytics.com/2021/03/teams-support-in-microsoft365r.html,"by Hong Ooi I’m happy to announce that version 2.0 of Microsoft365R the R interface to Microsoft 365 is now on CRAN! This version adds support for Microsoft Teams a much-requested feature. To access a team in Microsoft Teams use the get_team() function and provide the team name or ID. You can also list the teams you’re in with list_teams(). These return objects of R6 class ms_team which has methods for working with channels and drives. list_teams() team <- get_team(""My team"") # list the channels in a team (including your private channels) team$list_channels() # get the primary channel for a..."
2021,2,9,Microsoft365R: an R interface to the Microsoft 365 suite,https://blog.revolutionanalytics.com/2021/02/microsoft365r.html,"I’m very happy to announce Microsoft365R a package for working with the Microsoft 365 (formerly known as Office 365) suite of cloud services. Microsoft365R extends the interface to the Microsoft Graph API provided by the AzureGraph package to provide a lightweight yet powerful interface to SharePoint and OneDrive with support for Teams and Outlook soon to come. Microsoft365R is now available on CRAN or you can install the development version from GitHub with devtools::install_github(""Azure/Microsoft365R""). Authentication The first time you call one of the Microsoft365R functions (see below) it will use your Internet browser to authenticate with Azure Active Directory (AAD)..."
2021,1,21,AzureCosmosR: interface to Azure Cosmos DB,https://blog.revolutionanalytics.com/2021/01/azurecosmosr-interface-to-azure-cosmos-db.html,by Hong Ooi Last week I announced AzureCosmosR an R interface to Azure Cosmos DB a fully-managed NoSQL database service in Azure. This post gives a short rundown on the main features of AzureCosmosR. Explaining what Azure Cosmos DB is can be tricky so here’s an excerpt from the official description: Azure Cosmos DB is a fully managed NoSQL database for modern app development. Single-digit millisecond response times and automatic and instant scalability guarantee speed at any scale. Business continuity is assured with SLA-backed availability and enterprise-grade security. App development is faster and more productive thanks to turnkey multi region...
2021,1,13,What's new with AzureR,https://blog.revolutionanalytics.com/2021/01/whats-new-with-azurer.html,by Hong Ooi This is an update on what’s been happening with the AzureR suite of packages. First you may have noticed that just before the holiday season the packages were updated on CRAN to change their maintainer email to a non-Microsoft address. This is because I’ve left Microsoft for a role at Westpac bank here in Australia; while I’m sad to be leaving I do intend to continue maintaining and updating the packages. To that end here are the changes that have recently been submitted to CRAN or will be shortly: AzureAuth now allows obtaining tokens for the “organizations”...
2020,12,17,Azure Functions with R and plumber,https://blog.revolutionanalytics.com/2020/12/azure-functions-with-r.html,Azure Functions is a cloud service that allows you to deploy “serverless” microservices that are triggered by events (timers HTTP POST events etc) and automatically scale to serve demand while minimizing latency. The service natively supports functions written in C# Java JavaScript PowerShell Python and TypeScript and now supports other languages as well thanks to the launch last week of custom handlers for Azure Functions. A new tutorial walks you through the process of creating a custom handler for a “hello world” R function. The process is fairly straightforward: use a couple of Azure CLI commands to set up a...
2020,12,11,R at Microsoft,https://blog.revolutionanalytics.com/2020/12/r-at-microsoft.html,"I was my great pleasure yesterday to be a presenter in the ""Why R Webinar"" series on the topic R at Microsoft. In the talk (which you can watch below) I recounted the history of Microsoft's acquisition of Revolution Analytics and the various way the Microsoft supports R: its membership of the R Consortium integration with many products (including demos of Azure ML Service with GitHub Actions and Azure Functions) and how Microsoft has driven adoption of R in large organizations by making R ""IT approved"". Many thanks to the Why R Foundation for hosting the talk and to everyone..."
2020,11,25,"Attend the Create:Data free online event, December 7",https://blog.revolutionanalytics.com/2020/11/createdata-free-online-event-december-7.html,The Microsoft Create: series is back again now with Create: Data! Join us for a half-day of conversations at Microsoft Create: Data and connect with the experts and community to learn and discuss everything data - from the upcoming trends to best practices and data for good. Join our virtual event Create: Data Date: 7 December 2020 Time: 8:00AM – 11:10AM PDT / 4:00PM – 7:10PM GMT Join us! In this event you will hear from our keynote speakers Arun Ulagaratchagan and Heather Newman share how to drive a data culture in a world of remote everything. Learn about Apache...
2020,1,21,Blazing Trails in Teaching Talend: Meet Rick Sherman,https://athena-solutions.com/blazing-trails-in-teaching-talend-meet-rick-sherman/,From Talend:&#160;Rick&#160;Sherman is an author educator and a managing partner of Athena IT Solutions.&#160; His book&#160;Business Intelligence Guidebook: From Data Integration to Analytics&#160;was published by Morgan […]
2019,2,8,How to make a self-service BI tools deployment less painful,https://athena-solutions.com/how-to-make-a-self-service-bi-tools-deployment-less-painful/,Why does self-service BI have to hurt so much? In theory it should be the answer to all your problems. But so many enterprises end up […]
2018,9,18,Tableau vs. Power BI vs. Qlik: How the BI rivals stack up,https://athena-solutions.com/tableau-vs-power-bi-vs-qlik-how-the-bi-rivals-stack-up/,Tableau Power BI and Qlik Sense offer similar self-service BI functionality consultant Rick Sherman says. But he points to some of their strengths and weaknesses. Purchasing […]
2018,6,20,Modern Data Architecture—Avoiding  the ‘One-Tool-Fits-All Trap,https://athena-solutions.com/modern-data-architecture-avoiding-the-one-tool-fits-all-trap/,A modern data architecture for business intelligence and analytics has to support structured unstructured and semi-structured sources as well as hybrid integration and also meet the […]
2017,12,28,"For BI, you must know the data integration process",https://athena-solutions.com/know-data-integration/,Understanding the data integration process is central to self-service BI and data architecture design consultant Rick Sherman says in an end-of-year look at data management trends. […]
2017,11,13,"Govern first, then ask many questions",https://athena-solutions.com/govern-first-then-ask-many-questions/,Join me for this webinar hosted by Information Management Live. Thursday November 16 2017 1 PM ET/10 AM PT The path to fruitful analytics starts with good […]
2017,11,9,Self-Service BI on DM Radio,https://athena-solutions.com/self-service-bi-on-dm-radio/,Join me on DM Radio DM Radio / Leave IT Alone &#8211; The Vast Value of Self-Service DATE: November 9 2017 TIME: 3 PM Eastern / […]
2017,10,26,Dataversity 2017,https://athena-solutions.com/dataversity-2017/,I’m excited to share with you that I’ll be presenting two unique sessions at DATAVERSITY’s Data Architecture Summit in Chicago IL on November 13-16 2017.  I’m also […]
2017,7,25,Azure SQL Data Warehouse turns up the heat,https://athena-solutions.com/azure-sql-data-warehouse-turns-up-the-heat/,Rick Sherman is quoted extensively in this new TechTarget article: Azure SQL Data Warehouse turns up the heat expands processing power Abstract: Spirited competition is under way among […]
2017,6,28,A Firm Foundation for Modernized BI & Analytics,https://athena-solutions.com/foundation-for-modernization/,You wouldn’t modernize a house without fixing its rickety foundation first. Similarly your BI and analytics environment needs a firm foundation before you can update it. […]
2016,12,6,Properties of Interpretability,https://shapeofdata.wordpress.com/2016/12/05/properties-of-interpretability/,In my last two posts I wrote about model interpretability with the goal of trying to understanding what it means and how to measure it. In the first post I described the disconnect between our mental models and algorithmic models &#8230; Continue reading &#8594;
2016,11,17,Goals of Interpretability,https://shapeofdata.wordpress.com/2016/11/17/goals-of-interpretability/,In my last post I looked at the gap that arises when we delegate parts of our thought processes to algorithmic models rather than incorporating the rules they identify directly into our mental models like we do with traditional statistics. I &#8230; Continue reading &#8594;
2016,10,26,Interacting with ML Models,https://shapeofdata.wordpress.com/2016/10/26/interacting-with-ml-models/,The main difference between data analysis today compared with a decade or two ago is the way that we interact with it. Previously the role of statistics was primarily to extend our mental models by discovering new correlations and causal &#8230; Continue reading &#8594;
2016,6,4,LSTMs,https://shapeofdata.wordpress.com/2016/06/04/lstms/,In past posts I&#8217;ve described how Recurrent Neural Networks (RNNs) can be used to learn patterns in sequences of inputs and how the idea of unrolling can be used to train them. It turns out that there are some significant &#8230; Continue reading &#8594;
2016,4,28,Rolling and Unrolling RNNs,https://shapeofdata.wordpress.com/2016/04/27/rolling-and-unrolling-rnns/,A while back I discussed Recurrent Neural Networks (RNNs) a type of artificial neural network in which some of the connections between neurons point &#8220;backwards&#8221;. When a sequence of inputs is fed into such a network the backward arrows feed &#8230; Continue reading &#8594;
2016,1,20,Continuous Bayes’ Theorem,https://shapeofdata.wordpress.com/2016/01/20/continuous-bayes-theorem/,Bayes&#8217; Rule is one of the fundamental Theorems of statistics but up until recently I have to admit I was never very impressed with it. Bayes&#8217; gives you a way of determining the probability that a given event will occur or &#8230; Continue reading &#8594;
2015,11,30,The TensorFlow perspective on neural networks,https://shapeofdata.wordpress.com/2015/11/30/the-tensorflow-perspective-on-neural-networks/,A few weeks ago Google announced that it was open sourcing an internal system called TensorFlow that allows one to build neural networks as well as other types of machine learning models. (Disclaimer: I work for Google.) Because TensorFlow is designed &#8230; Continue reading &#8594;
2015,11,9,"Neural networks, linear transformations and word embeddings",https://shapeofdata.wordpress.com/2015/11/09/neural-networks-linear-transformations-and-word-embeddings/,In past posts I&#8217;ve described the geometry of artificial neural networks by thinking of the output from each neuron in the network as defining a probability density function on the space of input vectors. This is useful for understanding how &#8230; Continue reading &#8594;
2015,10,20,Recurrent Neural Networks,https://shapeofdata.wordpress.com/2015/10/20/recurrent-neural-networks/,So far on this blog we&#8217;ve mostly looked at data in two forms &#8211; vectors in which each data point is defined by a fixed set of features and graphs in which each data point is defined by its connections &#8230; Continue reading &#8594;
2015,7,14,GPUs and Neural Networks,https://shapeofdata.wordpress.com/2015/07/14/gpus-and-neural-networks/,Artificial neural networks have been around for a long time &#8211; since either the 1940s or the 1950s depending on how you count. But they&#8217;ve only started to be used for practical applications such as image recognition in the last &#8230; Continue reading &#8594;
2017,12,31,Thank You,https://www.svds.com/thankyou/,"We thank our customers partners investors friends and family for their support over the years. And most importantly we thank our employees for their hard work and dedication to building a great company!
The post Thank You appeared first on Silicon Valley Data Science."
2017,12,28,Happy Holidays from SVDS,https://www.svds.com/happy-holidays-svds/,"Happy holidays from SVDS! We wish you peace prosperity and happiness this season and in the year ahead.
The post Happy Holidays from SVDS appeared first on Silicon Valley Data Science."
2017,12,21,Crossing the Development to Production Divide,https://www.svds.com/tbt-crossing-development-production-divide/,"In this post we'll give an overview of obstacles we’ve faced (you may be able to relate) and talk about solutions to overcome these obstacles.
The post Crossing the Development to Production Divide appeared first on Silicon Valley Data Science."
2017,12,7,Q&A: On Being Data-Driven,https://www.svds.com/qa-data-driven/,"The best way to spread data-driven thinking through an organization is by proving that you can use data to solve a real business problem.
The post Q&#038;A: On Being Data-Driven appeared first on Silicon Valley Data Science."
2017,11,30,Managing Uncertainty,https://www.svds.com/managing-uncertainty/,"Being data-driven is the best way to manage uncertainty—but achieving that is about far more than bringing a bunch of numbers to your latest meeting.
The post Managing Uncertainty appeared first on Silicon Valley Data Science."
2017,11,21,Analyzing Sentiment in Caltrain Tweets,https://www.svds.com/analyzing-sentiment-caltrain-tweets/,"As a first step to using Twitter activity as one of the data sources for train prediction we start with a simple question: How do Twitter users currently feel about Caltrain?
The post Analyzing Sentiment in Caltrain Tweets appeared first on Silicon Valley Data Science."
2017,11,16,Learning from Imbalanced Classes,https://www.svds.com/tbt-learning-imbalanced-classes/,"For this month's Throwback Thursday a post that provides insight and concrete advice on how to tackle imbalanced data. 
The post Learning from Imbalanced Classes appeared first on Silicon Valley Data Science."
2017,11,9,Exploring the Possibilities of Artificial Intelligence,https://www.svds.com/exploring-the-possibilities-of-artificial-intelligence/,"In this interview Paco Nathan discusses making life more livable AI fears and more.
The post Exploring the Possibilities of Artificial Intelligence appeared first on Silicon Valley Data Science."
2017,11,2,Merging Data Science and Business,https://www.svds.com/merging-data-science-and-business/,"Business leaders cannot afford to ignore their organization&#8217;s data—rather that data should be used to make informed decisions. In this post Principal Data Scientist Tom Fawcett and Professor of Data Science Foster Provost discuss how businesses can make the most of their analytical teams. Tom and Foster are the authors of Data Science for Business. What aspect [&#8230;]
The post Merging Data Science and Business appeared first on Silicon Valley Data Science."
2017,10,26,Handling Small Files in MapR-FS,https://www.svds.com/handling-small-files-in-mapr-fs/,"In this post we will discuss how dealing with small files is different if you are using MapR-FS rather than the traditional HDFS installation.
The post Handling Small Files in MapR-FS appeared first on Silicon Valley Data Science."
2021,3,31,Streamline - tidy data as a service,https://simplystatistics.org/2021/03/31/streamline-data-science/,"Tldr: We started a company called Streamline Data Science https://streamlinedatascience.io/ that offers tidy data as a service. We are looking for customers partnerships and employees as we scale up after closing our funding round!

Most of my career I have worked in the muck of data cleaning. In the world of genomics a lot of my research has focused on batch effects synthesizing big genomic data into usable formats and generally making data easier to use. A couple of years ago we also started a company called Problem Forward Data Science. Problem Forward offered fractional data science services to a variety of businesses around the country from the very big corporations to startups just getting started. We were asked to do a lot of different types of data work everything from turning spreadsheets into dashboards to building complicated forecasting models. But no matter the project whether in government academia or industry we always ended up with the same problem.


We needed to clean the data before we could do the data science.


This will be no surprise to anyone who has worked in data science or analytics but the data almost always led to setbacks and frustration when we were working with our clients. Customers wanted complex AI insightful dashboards or easy reports but the data just weren’t ready for that yet. And we wasted a huge amount of time cleaning the data over and over again.

We realized that the most common challenge companies have is that their data processing and management pipelines aren’t ready for analytics. Or as Google so eloquently puts it:


“Everyone wants to do the model work not the data work”


We realized that this was a service that many businesses needed. They needed someone who could come in and set up a data processing pipeline for them manage it and make sure the data were up to date. Some people call this Extract Load Transform (ELT) but we found it goes a bit beyond that. It is figuring out what format is most useful for the people who rely on data and working backward to create a customized and unique data pipeline that gets the data ready to use.

The ELT pipeline we set up is designed to consistently output “tidy data” that makes it easy for our customers to use BI tools like Tableau or Looker and to ingest their data without having to do all the ugly data work that is painful and time-consuming.

We piloted this for one of our startup customers - we built their data pipeline and provided ongoing management maintenance and upkeep. When they hired their first data scientist they were able to quickly create dashboards for their whole business because they already had easy-to-use tidy data.

We got so excited about this data dry cleaning idea that we started a new company called Streamline Data Science that solely focuses on tidy data as a service. We just closed our seed round and are now working with our first set of customers to set up their data pipelines. The cool thing is we found that our most excited customers were the ones that already had a data scientist on the team. This seems a little counter-intuitive until you realize that we handle the painful/boring bits of data management so they can focus on the fun part.

The interesting thing about Streamline is that it isn’t a product. There are a ton of complicated tools out there that you can use to set up your own data pipeline. Streamline is a service that handles all your data issues for you so the data “just works”. It can often be a lot cheaper than building out a full stack data engineering team in house.

If you are a company that is worried about the state of your data - they are difficult to share to manage and to quality control - we’d love to hear from you! We would also love to hear from you if you are a data scientist or analyst at a company that is frustrated about how much time you are spending on data management and cleaning.

I’ll write more in the future about how we figured out setting up a data pipeline efficiently and the problems Streamline solves. We will also be releasing our first public data Streamlines that you can play with. In the meantime I wanted to share how excited I am to finally be working on solving the first mile of data science and building a company that can help Baltimore grow its data science community."
2020,11,24,The Four Jobs of the Data Scientist,https://simplystatistics.org/2020/11/24/the-four-jobs-of-the-data-scientist/,"In 2019 I wrote a post about The Tentpoles of Data Science that tried to distill the key skills of the data scientist. In the post I wrote:


When I ask myself the question “What is data science?” I tend to think of the following five components. Data science is (1) the application of design thinking to data problems; (2) the creation and management of workflows for transforming and processing data; (3) the negotiation of human relationships to identify context allocate resources and characterize audiences for data analysis products; (4) the application of statistical methods to quantify evidence; and (5) the transformation of data analytic information into coherent narratives and stories.

My contention is that if you are a good data scientist then you are good at all five of the tentpoles of data science. Conversely if you are good at all five tentpoles then you’ll likely be a good data scientist.


I still feel the same way about these skills but my feeling now is that actually that post made the job of the data scientist seem easier than it is. This is because it wrapped all of these skills into a single job when in reality data science requires being good at four jobs. In order to explain what I mean by this we have to step back and ask a much more fundamental question.

What is the Core of Data Science?

This is a question that everyone is asking and I think struggling to answer. With any field there’s always a distinction between the questions of


What is the core of the field?
What do people in that field do on a regular basis?


In case it’s not clear these are not the same question. For example in Statistics based on the curriculum from most PhD program the core of the field involves statistical methods statistical theory probability and maybe some computing. Data analysis is generally not formally taught (i.e. in the classroom) but rather picked up as part of a thesis or research project. Many classes labeled “Data Science” or “Data Analysis” simply teach more methods like machine learning clustering or dimension reduction. Formal software engineering techniques are also not generally taught but in practice are often used.

One could argue that data analysis and software engineering is something that statisticians do but it’s not the core of the field. Whether that is correct or incorrect is not my point. I’m only saying that a distinction has to be made somewhere. Statisticians will always do more than what would be considered the core of the field.

With data science I think we are collectively taking inventory of what data scientists tend to do. The problem is that at the moment it seems to be all over the map. Traditional statistics does tend to be central to the activity but so does computer science software engineering cognitive science ethics communication etc. This is hardly a definition of the core of a field but rather an enumeration of activities.

The question then is can we define something that all data scientists do? If we had to teach something to all data science students without knowing where they might end up afterwards what would it be? My opinion is that at some point all data scientists have to engage in the basic data analytic iteration.

Data Analytic Iteration

The basic data analytic iteration comes in four parts. Once a question has been established and a plan for obtaining or collecting data is available we can do the following:


Construct a set of expected outcomes
Design/Build/Apply a data analytic system to the data
Diagnose any anomalies in the analytic system output
Make a decision about what to do next


While this iteration might be familiar or obvious to many its familiarity masks the complexity involved. In particular each step of the iteration requires that the data scientist play a different role involving very different skills. It’s like a one-person play where the data scientist has to change costumes when going from one step to the next. This is what I refer to as the the four jobs of the data scientist.

The Four Jobs

Each of the steps in the basic data analytic iteration requires the data scientist to be four different people: (1) Scientist; (2) Statistician; (3) Systems Engineer; and (4) Politician. Let’s take a look at each job in greater detail.

Scientist

The scientist develops and asks the question and is responsible for knowing the state of the science and what the key gaps are. The scientist also designs any experiments for collecting new data and executes the data collection. The scientist must work with the statistician to design a system for analyzing the data and ultimately construct a set of expected outcomes from any analysis of the data being collected.

The scientist plays a key role in developing the system that results in our set of expected outcomes. Components of this system might include a literature review meta-analysis preliminary data or anecdotal data from colleagues. I use the term “Scientist” broadly here. In other settings this could be a policy-maker or product manager.

Statistician

The statistician in concert with the scientist designs the analytic system that will analyze the data generated by any data collection efforts. They specify how the system will operate what outputs it will generate and obtain any resources needed to implement the system. The statistician draws on statistical theory and personal experience to choose the different components of the analytic system that will be applied.

The statistician’s role here is to apply the data analytic system to the data and to produce the data analytic output. This output could be a regression coefficient a mean a plot or a prediction. But there must be something produced that we can compare to our set of expected outcomes. If the output deviates from our set of expected outcomes then the next task is to identify the reasons for that deviation.

Systems Engineer

Once the analytic system is applied to the data there are only two possible outcomes:


The outputs meet our expectations or
The output does not meet our expectations (an anomaly).


In the case of an anomaly the systems engineer’s responsibility is to diagnose the potential root causes of the anomaly based on knowledge of the data collection process the analytic system and the state of scientific knowledge.

Recently Emma Vitz wrote on Twitter:


How do you teach debugging to people who are more junior? I feel like it’s such an important skill and yet we seem to have no structured framework for teaching it


For software and for data analysis alike the challenge is that bugs or unexpected behavior can originate from anywhere. Any complex system is composed of multiple components some of which may be your responsibility and many of which are someone else’s. But bugs and anomalies do not respect those boundaries! There may be an issue that occurs in one component that only becomes known when you see the data analytic output.

So if you are responsible for diagnosing a problem it is your responsibility to investigate the behavior of each component of the system. If it is something you are not that familiar with then you need to become familiar with it either by learning on your own or (more likely) talking to the person who is in fact responsible.

A common source of unexpected behavior in data analytic output is the data collection process but the statistician who analyzes the data may not be responsible for that aspect of the project. Nevertheless the systems engineer who identifies an anomaly has to go back through and talk to the statistician and the scientist to figure out exactly how each component works.

Ultimately the systems engineer is tasked with taking a broad view of all the activities that affect the output from a data analysis in order to identify any deviations from what we would expect. Once those root causes have been explained we can then move on to decide how we should act on this new information.

Politician

The politician’s job is to make decisions while balancing the needs of the various constituents to achieve a reasonable outcome. Most statisticians and scientist that I know would recoil at the idea of being considered a politician or that politics in any form would play a role in doing any sort of science. However my thinking here is a bit more basic: In any data analysis iteration we are constantly making decisions about what to do keeping in mind a variety of conflicting factors. In order to resolve these conflicts and come to a reasonable agreement one has to engage a key skill which is negotiation.

At various stages of the data analytic iteration the politician must negotiate about (1) the definition of success in the analysis; (2) resources for executing the analysis; and (3) the decision for what to do after we have seen the output from the analytic system and have diagnosed the root causes of any anomalies. Decisions about what to do next fundamentally involve factors outside the data and the science.

Politicians have to identify who the stakeholders of the problem are and what is it that they ultimately want (as opposed to what their position is). For example an investigator might say “We need a p-value less than 0.05”. That’s their position. But what they want is more likely “a publication in a high profile journal”. Another example  might be an investigator who needs to meet a tight publication deadline while another investigator who wants to run a time-consuming (but more robust) analysis. Clearly the positions conflict but arguably both investigators share the same goal which is a rigorous high-impact publication.

Identifying positions versus underlying needs is a key task in negotiating a reasonable outcome for everyone involved. Rarely in my experience does this process have to do with the data (although data may be used to make certain arguments). The dominating elements of this process tend to be the nature of relationships between each constituent and the constraints on resources (such as time).

Applying the Iteration

If you’re reading this and find yourself saying “I’m not an X” where X is either scientist statistician systems engineer or politician then chances are that is where you are weak at data science. I think a good data scientist has to have some skill in each of these domains in order to be able to complete the basic data analytic iteration.

In any given analysis the iteration may be applied anywhere from once to dozens if not hundreds of times. If you’ve ever made two plots of the same dataset you’ve likely done two iterations. While the exact details and frequency of the iterations may vary widely across applications the core nature and the skills involved do not change much."
2020,8,26,Palantir Shows Its Cards,https://simplystatistics.org/2020/08/26/palantir-shows-its-cards/,"File this under long-term followup but just about four years ago I wrote about Palantir the previously secretive but now soon to be public data science company and how its valuation was a commentary on the value of data science more generally. Well just recently Palantir filed to go public and therefore submitted a registration statement (S-1) describing its business. It&rsquo;s a fascinating read if you&rsquo;re into that kind of stuff.

But the important thing is that Palantir itself summarized the question I asked more than 4 years ago. In their enumeration of risk factors one risk factor they highlight is


If our customers are not able or willing to accept our product-based business model instead of a labor-based business model our business and results of operations could be negatively impacted. [emphasis added]


In my original post I wrote about the &ldquo;Data Science Spectrum&rdquo; which has consulting on one end and software on the other.



The point of the diagram was that businesses on the right hand side have huge valuations while businesses on the left side have merely large valuations. The people running Palantir clearly understand this and are trying to push the company in a software-based productized direction.

Here&rsquo;s the rest of their summary of this risk factor:


Our platforms are generally offered on a productized basis to minimize our customers’ overall cost of acquisition maintenance and deployment time of our platforms. Many of our customers and potential customers are instead generally familiar with the practice of purchasing or licensing software through labor contracts where custom software is written for specific applications the intellectual property in such software is often owned by the customer and the software typically requires additional labor contracts for modifications updates and services during the life of that specific software. Customers may be unable or unwilling to accept our model of commercial software procurement. Should our customers be unable or unwilling to accept this model of commercial software procurement our growth could be materially diminished which could adversely impact our business financial condition results of operations and growth prospects.


Those of us who do data analysis for a living already know this to be true. Custom consulting is not scalable and therefore not as valuable as a piece of boxed up software which is infinitely scalable.

Show Me The Numbers

So how is Palantir doing?

At first glance it seems their doing pretty well. Their gross profit (Revenue - Cost of Revenue) suggests about a 72% gross margin percentage for 2018 and 67% for 2019 which both seem high. These gross margin percentages are in software company territory. (For comparison Facebook&rsquo;s percentage runs around 80%.) This suggests that each dollar of Palantir&rsquo;s revenue does not have a lot of direct costs associated with it.

But ulimately Palantir has posted net losses every year indicating there are significant indirect costs to generating that revenue. In particular their Sales and marketing costs almost equal their entire gross profit. Reading through the S-1 this ultimately is not surprising. Palantir itself admits that


Our sales efforts involve considerable time and expense and our sales cycle is often long and unpredictable.


Alas there is some consulting to do after all. My guess is that much of the up front &ldquo;sales&rdquo; work comes down to Palantir having to


Figure out a customer&rsquo;s problem and what question they&rsquo;re asking;
Figure out how a customer&rsquo;s data are organized;
Figure out how to their existing software products to the customer&rsquo;s specific situation.


This should sound familiar to seasoned data scientists. Indeed this is almost all the work of the data scientist. This is expensive because it requires humans to do it and there&rsquo;s typically not much to generalize from customer to customer. Implementing the software and deploying it is work too but is often more straightforward and their are often existing solutions that can be employed.

The Road to Profits

So here&rsquo;s the problem: Palantir&rsquo;s route to profitability involves making these costs go down&hellip;a lot. Maybe not to zero but substantially because each new customer&mdash;with their different problems and different data&mdash;costs a lot to acquire. If they can do this they&rsquo;ve cracked the nut of data science scalability.

Another big expense is Research and Development which the company describes as developing new methods for data analysis (machine learning tools etc.). While it&rsquo;s nice to have room to do open-ended research on new data science tools my guess is that this line item goes down a lot in the near future as it often does at companies that start off with large R&amp;D budgets. Ultimately it would save Palantir ~$300 million a year.

See you in another four years?"
2020,4,30,Asymptotics of Reproducibility,https://simplystatistics.org/2020/04/30/asymptotics-of-reproducibility/,"Every once in a while I see a tweet or post that asks whether one should use tool X or software Y in order to “make their data analysis reproducible”. I think this is a reasonable question because in part there are so many good tools out there! This is undeniably a good thing and quite a contrast to just 10 years ago when there were comparatively few choices.

The question of toolset though is not a question worth focusing on too much because it’s the wrong question to ask. Of course you should choose a tool/software package that is reasonably usable by a large percentage of your audience. But the toolset you use will not determine whether your analysis is reproducible in the long-run.

I think of the choice of toolset as kind of like asking “Should I use wood or concrete to build my house?” Regardless of what you choose once the house is built it will degrade over time without any deliberate maintenance. Just ask any homeowner! Sure some materials will degrade slower than others but the slope is definitely down.

Discussions about tooling around reproducibility often sound a lot like “What material should I use to build my house so that it never degrades*?” Such materials do not exist and similarly toolsets do not exist to make your analysis permanently reproducible.

I’ve been reading some of the old web sites from Jon Claerbout’s group at Stanford (thanks to the Internet Archive) the home of some of the original writings about reproducible research. At the time (early 90s) the work was distributed on CD-ROMs which totally makes sense given that CDs could store lots of data were relatively compact and durable and could be mailed or given to other people without much concern about compatibility. The internet was not quite a thing yet but it was clearly on the horizon.

But ask yourself this: If you held one of those CD-ROMs in your hand right now would you consider that work reproducible? Technically yes but I don’t even have a CD-ROM reader in my house so I couldn’t actually read the data. And a larger problem is that a CD from the 90s probably degraded to the point where it is likely unreadable anyway.

Claerbout’s group obviously knew about the web and were transitioning in that direction but such a transition costs money. As does keeping a keen eye on emerging trends and technology usage.

Hilary Parker and I recently discussed the how the economics of academic research are not well-suited to support the reproducibility of scientific results. The traditional model is that a research grant pays for the conduct of research over a 3-5 year period after which the grant is finished and there is no more funding. During (or after) that time scientific results are published. While the funding can be used to prepare materials (data software and code) to make the published findings reproducible at the instant of publication there is no funding afterwards for dealing with two key tasks:


Ensuring that the work continues to be reproducible given changes to the software and computing environment (maintenance)
Fielding questions or inquiries from others interested in reproducing the results or in building upon the published work (support)


These two activities (maintenance and support) can continue to be necessary in perpetuity for every study that an investigator publishes. The mismatch between how the grant funding system works and the requirements of reproducible research is depicted in the diagram below.



When I say “value” in the drawing above what I really mean is the “reproducibility value”. In the old model of publishing science there was no reproducibility value because the work was generally not reproducible in the sense that data and code were available. Hence this whole discussion would be moot.

Traditional paper publications held their value because the text on the page did not generally degrade much over time and copies could easily be made. Scientists did have to field the occasional question about the results but it was not the same as maintaining access to software and datasets and answering technical questions therein. As a result the traditional economic model for funding academic research really did match the manner in which research was conducted and then published. Once the results were published the maintenance and support costs were nominal and did not really need to be paid for explicitly.

Fast forward to today and the economic model has not changed but the “business” of academic research has. Now every publication has data and code/software attached to it which come with maintenance and support costs that can extend for a substantial period into the future. While any given publication may not require significant maintenance and support the costs for an investigator’s publications in aggregate can add up very quickly. Even a single paper that turns out to be popular can take up a lot of time and energy.

If you play this movie to the end it becomes soberingly clear that reproducible research from an economic stand point is not really sustainable. To see this it might help to use an analogy from the business world. Most businesses have capital costs where they buy large expensive things &ndash; machinery buildings etc. These things have a long life but are thought to degrade over time (accountants call it depreciation). As a result most businesses have “maintenance capital expenditure” costs that they report to show how much money they are investing every quarter to keep their equipment/buildings/etc. up to shape. In this context the capital expenditure is worth it because every new building or machine that is purchased is designed to ultimately produce more revenue. As long as the revenue generated exceeds the cost of maintenance the capital costs are worth it (not to oversimplify or anything!).

In academia each new publications incurs some maintenance and support costs to ensure reproducibility (the “capital expenditure” here) but it’s unclear how much each new publication brings in more “revenue” to offset those costs. Sure more publications allow one to expand the lab or get more grant funding or hire more students/postdocs but I wouldn’t say that’s universally true. Some fields are just constrained by how much total funding there is and so the available funding cannot really be increased by “reaching more customers”. Given that the budgets for funding agencies (at least in the U.S.) have barely kept up with inflation and the number of publications increases every year it seems the goal of making all research reproducible is simply not economically supportable.

I think we have to concede that at any given moment in time there will always be some fraction of published research for which there is no maintenance or support for reproducibility. Note that this doesn’t mean that people don’t publish their data and code (they should still do that!) it just means they don’t support or maintain it. The only question is which fraction should *no*t be supported or maintained? Most likely it will be older results where the investigators simply cannot keep up with maintenance and support. However it might be worth coming up with a more systematic approach to determining which publications need to maintain their reproducibility and which don’t.

For example it might be more important to maintain the reproducibility of results from huge studies that cannot be easily replicated independently. However for a small study conducted a decade ago that has subsequently been replicated many times we can probably let that one go. But this isn’t the only approach. We might want to preserve the reproducibility of studies that collect unique datasets that are difficult to re-collect. Or we might want to consider term-limits on reproducibility so an investigator commits to maintaining and supporting the reproducibility of a finding for say 5 years after which either the maintenance and support is dropped or longer-term funding is obtained. This doesn’t necessarily mean that the data and code suddenly disappear from the world; it just means the investigator is no longer committed to supporting the effort.

Reproducibility of scientific research is of critical importance perhaps now more than ever. However we need to think harder about how we can support it in both the short- and long-term. Just assuming that the maintenance and support costs of reproducibility for every study are merely nominal is not realistic and simply leads to investigators not supporting reproducibility as a default."
2020,4,29,Amplifying people I trust on COVID-19,https://simplystatistics.org/2020/04/29/amplifying-people-i-trust-on-covid-19/,"Like a lot of people I&rsquo;ve been glued to various media channels trying to learn about the latest with what is going on with COVID-19. I have also been frustrated - like a lot of people - with misinformation and the deluge of preprints and peer reviewed material. Some of this information is critically important and some is hard to trust.

As a biostatistician at a very visible school of public health I have also had a number of media outreaches but I&rsquo;ve been hesitant to do any interviews or talk about COVID-19. The reason is that even thought I have a PhD in Biostatistics and I work in a School of Public Health I actually know very little about infectious disease modeling and response. I think if you aren&rsquo;t really deep in the field its difficult to know the difference between someone like me and someone with real expertise.

While I&rsquo;m not an expert in the area I know many of the real experts professionally or by reputation. So I thought I&rsquo;d make a brief list of people and organizations I find credible and have been following for good information in case it is helpful to others. Many of these folks have already been found by audiences much bigger than ours but I just thought it would be useful to amplify further their work.

Paper review


JHU Novel Coronavirus Research Compendium - Hopkins experts rapidly reviewing preprints and peer reviewed literature to find the gems.


Infectious disease modeling


Trevor Bedford - Fred Hutchinson Cancer Research center expert in phylogenetic modeling of infectious disease his viz work and sober analysis is one of my go-tos.
Justin Lessler - infectious disease professor and epidemiologist at Hopkins who did some of the earliest studies of contact tracing in China.
Kate Grabowski - infectious disease professor and epidemiologist at Hopkins
Nicholas Reich - UMass expert in infectious disease modeling doing a great job of aggregating and evaluating disease models.
Natalie Dean - University of Florida expert statistician in vaccine clinical trials - also one of my favorite pragmatic reviewers of big papers.


Vaccine development


Derek Lowe - drug discovery chemist and blogger who is one of the best out there at distilling progress on vaccines.


Scicom and public outreach


Ellie Murray - Boston University expert epidemiologist professor and communicator providing clear understandable breakdowns of the best practices.
Lucy D&rsquo;Agostino McGowan - Vanderbilt statistics professor and communicator who does an amazing job of breaking down difficult stats and causal inference issues.
Carl Bergstrom - UW Biology Professor and infectious disease expert providing sober reviews and interactions around many of the papers coming out.


Policy


Tom Ingelsby - Professor and director of Johns Hopkins Center for Health Security has been producing solid analysis and policy recommendations on when to re-open.
Caitlin Rivers - Professor at the Hopkins Center for Health Security outbreak specialist also producing solid analysis and policy recommendations.
Andy Slavitt - Ex-Obama health care head and providing solid policy reviews and ideas.
Josh Sharfstein - Professor of the Practice at Johns Hopkins Bloomberg School of Public Health has a great public health podcast with lots of experts on it.
Keshia Pollack-Porter - Professor of Health Policy and Management at Johns Hopkins Bloomberg School of Public Health who has a great take on mobility issues associated with Covid-19.
Lisa Cooper - Bloomberg Professor at the Johns Hopkins Bloomberg School of Public Health who has great content on inequality of impact.


I&rsquo;m sure I&rsquo;ve missed great people to mention as I&rsquo;ve dashed this off pretty quickly so apologies if I missed you!"
2019,12,4,Is Artificial Intelligence Revolutionizing Environmental Health?,https://simplystatistics.org/2019/12/04/is-artificial-intelligence-revolutionizing-environmental-health/,"NOTE: This post was written by Kevin Elliott Michigan State University; Nicole Kleinstreuer National Institutes of Health; Patrick McMullen ScitoVation; Gary Miller Columbia University; Bhramar Mukherjee University of Michigan; Roger D. Peng Johns Hopkins University; Melissa Perry The George Washington University; Reza Rasoulpour Corteva Agriscience and Elizabeth Boyle National Academies of Sciences Engineering and Medicine. The full summary for the workshop on which this post is based can be obtained here.

On June 6 and 7 2019 the National Academy of Sciences Engineering and Medicine (NASEM) hosted a workshop on the use of artificial intelligence (AI) in the field of Environmental Health. Rapid advances in machine learning are demonstrating the ability of machines to carry out repetitive “smart” tasks requiring discreet judgments. Machine learning algorithms are now being used to analyze large volumes of complex data to find patterns and make predictions often exceeding the accuracy and efficiency of people attempting the same task. Driven by tremendous growth in data availability as well as computing power and accessibility artificial intelligence and machine learning applications are rapidly growing in various sectors of society including retail such as predicting consumer purchases; the automotive industry as demonstrated by self-driving cars and in health care with advances in automated medical diagnoses.

Building upon the major themes of the NASEM workshop in this blog post we address the following questions:


How might AI advance environmental health?

Does AI change the standards used for conducting environmental health research?

Does the use of AI allow us to change our established research principles?

How does AI impact our training programs for the next generation of environmental health scientists?

Are there barriers within the current academic incentive structures that are hindering the full potential of AI and how might those barriers be overcome?


How might AI advance environmental health?

Environmental health is the study of how the environment affects human health. Due to the complexity of both human biology and the multiplicity of environmental factors that we encounter daily studying environmental impacts on human health presents many data challenges. Due to the data boom we have seen in recent years we now have a multitude of individualized data including genetic sequencing and wearable health and activity monitors.  We have also seen exponential growth in the availability of data on individual environmental exposures.  Wearable sensors and personal chemical samplers are allowing for more detailed exposure models whereas advancements in exposure biomonitoring in a variety of matrices including blood and urine is giving more granular detail about actual chemical body burdens. We have also seen an increase in available population level data on dietary factors the social and built environment climate and many other variables affected by environmental and genetic factors. Concurrently while population data are booming toxicology is creating a variety of experimental models to advance our understanding of how chemicals and environmental exposures may pose risks to human health. Large-scale high-throughput chemical safety screening efforts can now generate data on tens of thousands of chemicals in thousands of biological targets. Integrating these diverse data streams represents a new level of complexity.

AI and machine learning provide many opportunities to make this complexity more manageable such as highly accurate prediction methods to better assess exposures and flexible approaches to allow incorporation of exposure to complex mixtures in population health analyses. Incorporating artificial intelligence and machine learning methods in environmental health research offers the potential to transform how we analyze environmental exposures and our understanding of how these myriad factors influence our health and contribute to disease.

Does AI change the standards used for conducting environmental health research?

While we think the use of AI and machine learning techniques clearly hold great promise for the advancement of environmental health research we also believe such techniques introduce new challenges and magnify existing ones.  While the major standards by which we conduct scientific research do not change our ability to adhere to them will require some adaptation. Transparency and repeatability are key.  We must ensure that the computational reproducibility and replicability of our scientific findings do not suffer at the hands of complex algorithms and poorly assembled data pipelines. Complex data analyses that incorporate more diverse data types from varied sources stretch our ability to track curate and validate these data without robust data curation tools.  Although some data curation tools that establish standard approaches for creating managing and maintaining data are available they are usually field-specific and currently there are no incentives or strict requirements to ensure that investigators use them.

Machine learning and artificial intelligence algorithms have demonstrated themselves to be very powerful. At the same time we also recognize their complexity and general opacity can be cause for concern. While investigators may be willing to overlook the opacity of these algorithms when predictions are highly accurate and precise all is well until it isn’t. When an algorithm does not work as expected it is critical to know why it didn’t work. With transparency and reproducibility of utmost importance machine learning algorithms must ensure that investigators and data analysts have accountability in their analyses and that regulators have confidence in applying AI generated results to inform public health decisions.

Does the use of AI allow us to change our established research principles?

AI does not change established research principles such as sound study designs and understanding threats of bias. However there is a need to create updated guidelines and implement best practices for choosing cleaning structuring and sharing the data used in AI applications. Creating appropriate training datasets engaging in ongoing processes of validation and assessing the domain of applicability for the models that are generated are also important. As in all areas of science it is crucial to clarify whether models solely provide accurate predictions or whether they also provide understanding of relevant mechanisms. The current Open Science movement’s emphasis on transparency is particularly relevant to the use of AI and machine learning. Users of these methods in environmental health should be looking for ways to be open about the model training data to clarify validation methods to create interpretable “models of the models” where possible and to clarify their domains of applicability. Recent innovations like model cards or short documents that go alongside machine learning models to share information that everyone impacted by the model should know is one example of a way model developers can communicate their models’ strengths and weaknesses in a way that is accessible.

How does AI impact our training programs for the next generation of environmental health scientists?

As complex AI methods are increasingly applied to environmental health research it is important to consider effective training of the workforce and its future leaders. Currently training in the application of data science is unstandardized as trainees learn how to apply methods to a specific research application through an apprenticeship type model where a trainee works with a mentor. Classroom training standardizes theory and methods but the mentor teaches the fine details of analyzing data in a specific research area which introduces heterogeneity into the ways in which scientists analyze data. The lack of training standards leads to a worry that analysts may apply cutting-edge computational/algorithmic approaches to data analysis without consideration of fundamental biostatistical and epidemiologic principles such as statistical design sampling and inference.
Fundamental questions taught in biostatistics and epidemiology courses such as &ldquo;Who is in my sample?&rdquo; and &ldquo;What is my target population of inference?&rdquo; are even more relevant in our current era of algorithms and machine learning. Now analysts are agnostically querying databases not designed for population-based research such electronic health records medical claims Twitter Facebook and Google searches for new discoveries in environmental health. It is important to recognize that a lack of proper consideration of issues related to sampling selection bias correlation of multiple exposures exposure and outcome misclassification could lead to erroneous results and false conclusions.  Training programs will need to evolve so that we do not just teach scientists and analysts how to program models and interpret their results but also emphasize how to recognize human biases that can be inadvertently built into the data and model approaches and the continuous need for rigor responsibility and reproducibility.

An increased focus on mathematical theory may also improve training in the application of AI to environmental health. A greater effort in developing standardized theory about how and why a specific research area analyses data in a certain way may help adapt approaches from one research area to another. In addition deeper mathematical exploration of AI methods will help data scientists understand when and why AI methods work well and when they don’t.

Are there barriers within the current academic incentive structures that are hindering the full potential of AI and how might those barriers be overcome?

Rigorous data science requires a team science approach to achieve a variety of functions such as developing algorithms formalizing common data platforms and testing protocols and properly maintaining and curating data sources. Over recent decades we have witnessed how the power of team science has improved the understanding of critical health problems of our time such as in unlocking the human genome and achieving major advancements in cancer treatment.  These advances have demonstrated the payoff of interdisciplinary transdisciplinary and multidisciplinary investigations. Despite these successes there are still barriers to large team science projects because these projects often have goals that do not sit precisely within a single funding agency.  In order for AI to truly advance environmental health federal agencies and institutions that fund environmental health research need to create pathways to support large multi-disciplinary and multi-institutional teams that are conducting this research. An example could be a multi-agency/multi-institute funding consortia. A ten-year investment in a well-coordinated initiative that harnesses AI data opportunities could accelerate new findings in not only the environmental causes of disease but also in informing interventions that can prevent environmentally mediated disease and improve population health.

Final thoughts

We believe machine learning and AI methods have tremendous potential but we also believe they cannot be used in a way that overlooks limitations or relaxes data integrity standards. With these considerations in mind we have tempered enthusiasm for the promises of these approaches. We have to make sure that environmental health scientists stay out in front of these considerations to avoid potential pitfalls such as the allure of hype or chasing after the next new thing because it is novel rather than truly meaningful.  We can do this by fostering ongoing conversations about the challenges and opportunities AI provides for environmental health research. An intentional union of the two cultures of careful (and often overly cautious) stochastic and bold (and often overly optimistic) algorithmic modeling can help to ensuring we are not abandoning principles of proper study design when a new technology comes along but explore how to use the new technology to better understand the myriad ways the environment affects health and disease."
2019,8,28,You can replicate almost any plot with R,https://simplystatistics.org/2019/08/28/you-can-replicate-almost-any-plot-with-ggplot2/,"Although R is great for quickly turning data into plots it is not widely used for making publication ready figures. But with enough tinkering you can make almost any plot in R. For examples check out the flowingdata blog or the Fundamentals of Data Visualization book.
Here I show five charts from the lay press that I use as examples in my data science courses. In the past I would show the originals but I decided to replicate them in R to make it possible to generate class notes with just R code (there was a lot of googling involved).
Below I show the original figures followed by R code and the version of the plot it produces. I used the ggplot2 package but you can achieve similar results using other packages or even just with R-base. Any recommendations on how to improve the code or links to other good examples are welcomed. Please at to the comments or @ me on twitter: @rafalab.

Example 1
The first example is from this ABC news article. Here is the original:

Here is the R code for my version. Note that I copied the values by hand.
library(tidyverse)
library(ggplot2)
library(ggflags)
library(countrycode)

dat &lt;- tibble(country = toupper(c(&quot;US&quot; &quot;Italy&quot; &quot;Canada&quot; &quot;UK&quot; &quot;Japan&quot; &quot;Germany&quot; &quot;France&quot; &quot;Russia&quot;))
              count = c(3.2 0.71 0.5 0.1 0 0.2 0.1 0)
              label = c(as.character(c(3.2 0.71 0.5 0.1 0 0.2 0.1)) &quot;No Data&quot;)
              code = c(&quot;us&quot; &quot;it&quot; &quot;ca&quot; &quot;gb&quot; &quot;jp&quot; &quot;de&quot; &quot;fr&quot; &quot;ru&quot;))

dat %&gt;% mutate(country = reorder(country -count)) %&gt;%
  ggplot(aes(country count label = label)) +
  geom_bar(stat = &quot;identity&quot; fill = &quot;darkred&quot;) +
  geom_text(nudge_y = 0.2 color = &quot;darkred&quot; size = 5) +
  geom_flag(y = -.5 aes(country = code) size = 12) +
  scale_y_continuous(breaks = c(0 1 2 3 4) limits = c(04)) +   
  geom_text(aes(6.25 3.8 label = &quot;Source UNODC Homicide Statistics&quot;)) + 
  ggtitle(toupper(&quot;Homicide Per 100000 in G-8 Countries&quot;)) + 
  xlab(&quot;&quot;) + 
  ylab(&quot;# of gun-related homicides\nper 100000 people&quot;) +
  ggthemes::theme_economist() +
  theme(axis.text.x = element_text(size = 8 vjust = -16)
        axis.ticks.x = element_blank()
        axis.line.x = element_blank()
        plot.margin = unit(c(1111) &quot;cm&quot;)) 



Example 2
The second example from everytown.org. Here is the original:

Here is the R code for my version. As in the previous example I copied the values by hand.
dat &lt;- tibble(country = toupper(c(&quot;United States&quot; &quot;Canada&quot; &quot;Portugal&quot; &quot;Ireland&quot; &quot;Italy&quot; &quot;Belgium&quot; &quot;Finland&quot; &quot;France&quot; &quot;Netherlands&quot; &quot;Denmark&quot; &quot;Sweden&quot; &quot;Slovakia&quot; &quot;Austria&quot; &quot;New Zealand&quot; &quot;Australia&quot; &quot;Spain&quot; &quot;Czech Republic&quot; &quot;Hungry&quot; &quot;Germany&quot; &quot;United Kingdom&quot; &quot;Norway&quot; &quot;Japan&quot; &quot;Republic of Korea&quot;))
              count = c(3.61 0.5 0.48 0.35 0.35 0.33 0.26 0.20 0.20 0.20 0.19 0.19 0.18 0.16
                        0.16 0.15 0.12 0.10 0.06 0.04 0.04 0.01 0.01))

dat %&gt;% 
  mutate(country = reorder(country count)) %&gt;%
  ggplot(aes(country count label = count)) +   
  geom_bar(stat = &quot;identity&quot; fill = &quot;darkred&quot; width = 0.5) +
  geom_text(nudge_y = 0.2  size = 3) +
  xlab(&quot;&quot;) + ylab(&quot;&quot;) + 
  ggtitle(toupper(&quot;Gun Murders per 100000 residents&quot;)) + 
  theme_minimal() +
  theme(panel.grid.major =element_blank() panel.grid.minor = element_blank() 
        axis.text.x = element_blank()
        axis.ticks.length = unit(-0.4 &quot;cm&quot;)) + 
  coord_flip() 



Example 3
The next example is from the Wall Street Journal. The original is interactive but here is a screenshot:

Here is the R code for my version. Note I matched the colors by hand as the original does not seem to follow a standard palette.
library(dslabs)
data(us_contagious_diseases)
the_disease &lt;- &quot;Measles&quot;
dat &lt;- us_contagious_diseases %&gt;%
  filter(!state%in%c(&quot;Hawaii&quot;&quot;Alaska&quot;) &amp; disease == the_disease) %&gt;%
  mutate(rate = count / population * 10000 * 52 / weeks_reporting) 

jet.colors &lt;- colorRampPalette(c(&quot;#F0FFFF&quot; &quot;cyan&quot; &quot;#007FFF&quot; &quot;yellow&quot; &quot;#FFBF00&quot; &quot;orange&quot; &quot;red&quot; &quot;#7F0000&quot;) bias = 2.25)

dat %&gt;% mutate(state = reorder(state desc(state))) %&gt;%
  ggplot(aes(year state fill = rate)) +
  geom_tile(color = &quot;white&quot; size = 0.35) +
  scale_x_continuous(expand = c(00)) +
  scale_fill_gradientn(colors = jet.colors(16) na.value = &#39;white&#39;) +
  geom_vline(xintercept = 1963 col = &quot;black&quot;) +
  theme_minimal() + 
  theme(panel.grid = element_blank()) +
        coord_cartesian(clip = &#39;off&#39;) +
        ggtitle(the_disease) +
        ylab(&quot;&quot;) +
        xlab(&quot;&quot;) +  
        theme(legend.position = &quot;bottom&quot; text = element_text(size = 8)) + 
        annotate(geom = &quot;text&quot; x = 1963 y = 50.5 label = &quot;Vaccine introduced&quot; size = 3 hjust = 0)



Example 4
The next example is from the New York Times. Here is the original:

Here is the R code for my version:
data(&quot;nyc_regents_scores&quot;)
nyc_regents_scores$total &lt;- rowSums(nyc_regents_scores[-1] na.rm=TRUE)
nyc_regents_scores %&gt;% 
  filter(!is.na(score)) %&gt;%
  ggplot(aes(score total)) + 
  annotate(&quot;rect&quot; xmin = 65 xmax = 99 ymin = 0 ymax = 35000 alpha = .5) +
  geom_bar(stat = &quot;identity&quot; color = &quot;black&quot; fill = &quot;#C4843C&quot;) + 
  annotate(&quot;text&quot; x = 66 y = 28000 label = &quot;MINIMUM\nREGENTS DIPLOMA\nSCORE IS 65&quot; hjust = 0 size = 3) +
  annotate(&quot;text&quot; x = 0 y = 12000 label = &quot;2010 Regents scores on\nthe five most common tests&quot; hjust = 0 size = 3) +
  scale_x_continuous(breaks = seq(5 95 5) limit = c(099)) + 
  scale_y_continuous(position = &quot;right&quot;) +
  ggtitle(&quot;Scraping By&quot;) + 
  xlab(&quot;&quot;) + ylab(&quot;Number of tests&quot;) + 
  theme_minimal() + 
  theme(panel.grid.major.x = element_blank() 
        panel.grid.minor.x = element_blank()
        axis.ticks.length = unit(-0.2 &quot;cm&quot;)
        plot.title = element_text(face = &quot;bold&quot;))



Example 5
This last one is from fivethirtyeight.

Below is the R code for my version. Note that in this example I am essentially just drawing as I don’t estimate the distributions myself. I simply estimated parameters “by eye” and used a bit of trial and error.
my_dgamma &lt;- function(x mean = 1 sd = 1){
  shape = mean^2/sd^2
  scale = sd^2 / mean
  dgamma(x shape = shape scale = scale)
}

my_qgamma &lt;- function(mean = 1 sd = 1){
  shape = mean^2/sd^2
  scale = sd^2 / mean
  qgamma(c(0.10.9) shape = shape scale = scale)
}

tmp &lt;- tibble(candidate = c(&quot;Clinton&quot; &quot;Trump&quot; &quot;Johnson&quot;) 
              avg = c(48.5 44.9 5.0) 
              avg_txt = c(&quot;48.5%&quot; &quot;44.9%&quot; &quot;5.0%&quot;) 
              sd = rep(2 3) 
              m = my_dgamma(avg avg sd)) %&gt;%
  mutate(candidate = reorder(candidate -avg))

xx &lt;- seq(0 75 len = 300)

tmp_2 &lt;- map_df(1:3 function(i){
  tibble(candidate = tmp$candidate[i]
         avg = tmp$avg[i]
         sd = tmp$sd[i]
         x = xx
         y = my_dgamma(xx tmp$avg[i] tmp$sd[i]))
})

tmp_3 &lt;- map_df(1:3 function(i){
  qq &lt;- my_qgamma(tmp$avg[i] tmp$sd[i])
  xx &lt;- seq(qq[1] qq[2] len = 200)
  tibble(candidate = tmp$candidate[i]
         avg = tmp$avg[i]
         sd = tmp$sd[i]
         x = xx
         y = my_dgamma(xx tmp$avg[i] tmp$sd[i]))
})
         
tmp_2 %&gt;% 
  ggplot(aes(x ymax = y ymin = 0)) +
  geom_ribbon(fill = &quot;grey&quot;) + 
  facet_grid(candidate~. switch = &quot;y&quot;) +
  scale_x_continuous(breaks = seq(0 75 25) position = &quot;top&quot;
                     label = paste0(seq(0 75 25) &quot;%&quot;)) +
  geom_abline(intercept = 0 slope = 0) +
  xlab(&quot;&quot;) + ylab(&quot;&quot;) + 
  theme_minimal() + 
  theme(panel.grid.major.y = element_blank() 
        panel.grid.minor.y = element_blank()
        axis.title.y = element_blank()
        axis.text.y = element_blank()
        axis.ticks.y = element_blank()
        strip.text.y = element_text(angle = 180 size = 11 vjust = 0.2)) + 
  geom_ribbon(data = tmp_3 mapping = aes(x = x ymax = y ymin = 0 fill = candidate) inherit.aes = FALSE show.legend = FALSE) +
  scale_fill_manual(values = c(&quot;#3cace4&quot; &quot;#fc5c34&quot; &quot;#fccc2c&quot;)) +
  geom_point(data = tmp mapping = aes(x = avg y = m) inherit.aes = FALSE) + 
  geom_text(data = tmp mapping = aes(x = avg y = m label = avg_txt) inherit.aes = FALSE hjust = 0 nudge_x = 1) 

"
2019,8,27,So You Want to Start a Podcast,https://simplystatistics.org/2019/08/27/so-you-want-to-start-a-podcast/,"Podcasting has gotten quite a bit easier over the past 10 years due in part to improvements to hardware and software. I wrote about both how I edit and record both of my podcasts about 2 years ago and while not much has changed since then I thought it might be helpful if I organized the information in a better way for people just starting out with a new podcast.

One frustrating problem that I find with podcasting is that the easy methods are indeed easy and the difficult methods are indeed difficult but the methods that are just above easy which other markets might label as “prosumer” or something like that are&hellip;kind of hard. One of the reasons is that once you start buying better hardware everything kind of snowballs because the hardware becomes more modular. So instead of just using your phone headphones to record you might buy a microphone that connects to a stand that connects to a USB interface using an XLR cable that connects to your computer. Similarly on the software side there’s really not much out there that’s free. As a result of both phenomena costs start to go up pretty quickly as soon as you step up just a little bit.

I can’t do anything about costs but I thought I could help a little bit on sorting out what’s out there and what’s genuinely valuable. There are two versions here: the free and easy plan if you’re just starting out and the next level up which is basically what I use.

The three things I’ll cover here that you need for podcasting are:


Hardware - this includes all recording equipment like microphones stands cables etc.
Recording Software - Unless you live in a recording booth you’ll need some software for your computer (which I assume you have!)
Editing Software - the more complicated your podcast gets the more you’ll need to edit (beyond just trimming the beginning and end of the audio files)
Hosting - Unless you plan on running your own server (which is an option but I don’t recommend it) you’ll need someone to host your audio files.


Free and Easy

There are in fact ways to podcast for free and many people stay at this level for a long time because the quality is acceptable and cost is zero. If you want to just get started quickly here’s what you can do:


Hardware - just use the headphones/microphone that came with your mobile phone.
Recording Software - If you are doing a podcast by yourself you can just use whatever app your phone has to record things like voice memos. On your computer there should be a built-in app that just lets you record sound through the headphones.
Editing Software - For editing I recommend either not editing (simpler!) or using something like Audacity to just trim the beginning and the end.
Hosting - SoundCloud offers free hosting for up to 3 hours of content. This is plenty for just starting out and seeing if you like it but you will likely use it up.


If you are working with a partner it gets a little more complicated and there are some additional notes on the recording software. My go-to recommendation for recording with a partner is to use Zencastr. Zencastr has a free plan that lets you record high-quality audio for a max of 2 people. (If you need to record more than 2 people you can’t use the free option.) The nice thing about Zencastr is that it uses WebRTC to record directly off your microphone so you don’t need to worry too much about the quality of your internet connection. What you get is separate audio files one for each speaker that are synched together. Occasionally there are some synching glitches but usually it works out. The files are automatically uploaded to a Dropbox account so you’ll need one of those. Because Zencastr automatically goes to MP3 format the files are relatively small. Also if you have a guest who is less familiar with audio hardware/software you can just send them a link that they can click on and they’re recording.

Note that even if your partner is sitting right next to you it’s often simpler to just go to separate spaces and record “remotely”. The primary benefit of doing this is that you can cleanly record separate/independent audio tracks. This can be useful in the editing process.

If you prefer an all-in-one solution there are services like Cast and Anchor that offer recording hosting and distribution. Cast only has a free 1-month trial and so you have to pay eventually. Anchor appears to be free (I’ve never used it) but it was recently purchased by Spotify so it’s not immediately clear to me if anything will change. My guess is they’ll likely stay free because they want as many people making podcasts as possible. Anchor didn’t exist when I started podcasting but if it had I might have used it first. But it always makes me a little nervous when I can’t figure out how a company makes money.

To summarize here’s the “free and easy” workflow that I recommend:


Record your podcast using Zencastr (especially if you have a partner) which then puts audio files on Dropbox
Trim beginning/ending of audio file with Audacity
Upload audio to SoundCloud and add episode metadata


And here are the pros and cons:

Pros


It’s free


Cons


Audio quality is acceptable but not great. Earbud type microphones are not designed for high quality and you can usually tell when someone has used them to record. Given that podcasts are all about audio it’s hard for me to trade off audio quality.
Hosting limitations mean you can only get a few episodes up. But that’s a problem for down the road right?
Editing is generally a third-order issue but there is one scenario where it can be critical&mdash;when you have a bad internet connection. Bad internet connections can introduce delays and cross-talk. These problems can be mitigated when editing (I give an example here) but only with better software.


Beyond Free

Beyond the free workflow there are a number of upgrades that you can make and you can easily start spending a lot of money. But the only real upgrade that I think you need to make is to buy a good microphone. Surprisingly this does not need to cost much money. The best podcasting microphone for the money out there is the Audio Technica ATR2100 USB micrphone. This is the microphone that Elizabeth uses on the The Effort Report and Hilary uses on Not So Standard Deviations. As of this writing it’s \$65 on Amazon but I’ve seen it for as low as \$40. The benefits of this microphone are:


The audio quality is high
It isolates vocal audio really well and doesn’t pick up a lot of background audio (good for noisy rooms like my office).
It connects directly to a computer via USB so you don’t need to buy a separate USB interface.
It’s cheap


The problem with getting “better” (i.e. more expensive) microphones as that they tend to be more sensitive which means they pick up more high-frequency background noise. Professional microphones are designed for you to be working in a sound-proof recording studio environment in which you want to pick up as much sound as possible. But podcasting in general tends to take place wherever. So you want a microphone that will only pick up your voice right in front of it. Technically you lose a little quality this way but it’s equally annoying to have a lot of background noise.

Now that you’ve got a microphone you need to stick it somewhere. While you can always just hold the microphone I’d recommend an adjustable stand of some sort. Desk stands like this one are nice because they’re adjustable but they do require you to have a semi-permanent office where you can just keep it. The main point here is that podcasting requires you to sit still and talk for a while and you don’t want to be uncomfortable while you’re doing it.

The last upgrade you’ll likely need to make is the hosting provider. SoundCloud itself offers an unlimited plan but I don’t recommend it as it’s not really designed for podcasting. I use Libsyn which has a $5 a month plan that should be enough for a monthly podcast. They also provide some decent analytics that you can download and read into R. What I like about Libsyn is that they do one job and they do it really well. I give them money and they provide me a service in return. How simple is that?

That’s it for now. I’m happy to make more recommendations regarding software and hardware (feel free to tweet me @rdpeng) but I think what I’ve got here should get you 99% of the way there."
2019,7,23,The data deluge means no reasonable expectation of privacy - now what?,https://simplystatistics.org/2019/07/23/the-data-deluge-means-no-reasonable-expectation-of-privacy-no-what/,"Today a couple of different things reminded me about something that I suppose many people are talking about but has been on my mind as well.

The idea is that many of our societies social norms are based on the reasonable expectation of privacy. But the reasonable expectation of privacy is increasingly a thing of the past. Three types of data I&rsquo;ve been thinking about are:


Obviously identifying data: Data like cellphone GPS traces and public social media posts are obviously information that is indentifiable and reduce privacy.
Data that can be inferred from public data: We can also now infer a lot about people given the data that is public. For example a couple of years ago I challenged the students in my advanced data science class to predict the Gail score - one of the most widely used measures of breast cancer risk  - using only the information available from a person&rsquo;s public Facebook profile. While not all of the information was available a good fraction of it was. This is an example of something you might not think that posting pictures of your family your birthday celebrations and family life events could enable. I was reminded of this when hearing about this paper that claims to be able to deidentify up to 99.98\% of Americans using only 15 pieces of demographic information.
Data other people share about us: The stories around the capture of the Golden Gate Killer using genealogy data make it clear that even when you personally don&rsquo;t share your data someone else may be sharing it for you. The same can be said of photos of you that were tagged on Facebook even if you aren&rsquo;t on the platform.


I don&rsquo;t think these types of data are going to magically disappear. So like a lot of other people I&rsquo;ve been wondering how we should individually and as a society adapt to the world where privacy is no longer an expectation."
2019,7,19,More datasets for teaching data science: The expanded dslabs package,https://simplystatistics.org/2019/07/19/more-datasets-for-teaching-data-science-the-expanded-dslabs-package/,"
Introduction
We have expanded the dslabs package which we previously introduced as a package containing realistic interesting and approachable datasets that can be used in introductory data science courses.
This release adds 7 new datasets on climate change astronomy life expectancy and breast cancer diagnosis. They are used in improved problem sets and new projects within the HarvardX Data Science Professional Certificate Program which teaches beginning R programming data visualization data wrangling statistics and machine learning for students with no prior coding background.
You can install the dslabs package from CRAN:
install.packages(&quot;dslabs&quot;)
If you already have the package installed you can add the new datasets by updating the package:
update.packages(&quot;dslabs&quot;)
You can load the package into your workspace normally:
library(dslabs)
Let’s preview these new datasets! To code along use the following libraries and options:
# install packages if necessary
if(!require(&quot;tidyverse&quot;)) install.packages(&quot;tidyverse&quot;)
if(!require(&quot;ggrepel&quot;)) install.packages(&quot;ggrepel&quot;)
if(!require(&quot;matrixStats&quot;)) install.packages(&quot;matrixStats&quot;)


# load libraries
library(tidyverse)
library(ggrepel)
library(matrixStats)

# set colorblind-friendly color palette
colorblind_palette &lt;- c(&quot;black&quot; &quot;#E69F00&quot; &quot;#56B4E9&quot; &quot;#009E73&quot;
                        &quot;#CC79A7&quot; &quot;#F0E442&quot; &quot;#0072B2&quot; &quot;#D55E00&quot;)


Climate change
Three datasets related to climate change are used to teach data visualization and data wrangling. These data produce clear plots that demonstrate an increase in temperature greenhouse gas levels and carbon emissions from 800000 years ago to modern times. Students can create their own impactful visualizations with real atmospheric and ice core measurements.

Modern temperature anomaly and carbon dioxide data: temp_carbon
The temp_carbon dataset includes annual global temperature anomaly measurements in degrees Celsius relative to the 20th century mean temperature from 1880-2018. The temperature anomalies over land and over ocean are reported also. In addition it includes annual carbon emissions (in millions of metric tons) from 1751-2014. Temperature anomalies are from NOAA and carbon emissions are from Boden et al. 2017 via CDIAC.
data(temp_carbon)

# line plot of annual global land and ocean temperature anomalies since 1880
temp_carbon %&gt;%
    select(Year = year Global = temp_anomaly Land = land_anomaly Ocean = ocean_anomaly) %&gt;%
    gather(Region Temp_anomaly Global:Ocean) %&gt;%
    ggplot(aes(Year Temp_anomaly col = Region)) +
    geom_line(size = 1) +
    geom_hline(aes(yintercept = 0) col = colorblind_palette[8] lty = 2) +
    geom_label(aes(x = 2005 y = -.08) col = colorblind_palette[8] 
               label = &quot;20th century mean&quot; size = 4) +
    ylab(&quot;Temperature anomaly (degrees C)&quot;) +
    xlim(c(1880 2018)) +
    scale_color_manual(values = colorblind_palette) +
    ggtitle(&quot;Temperature anomaly relative to 20th century mean 1880-2018&quot;)



Greenhouse gas concentrations over 2000 years: greenhouse_gases
The greenhouse_gases data frame contains carbon dioxide (\(\mbox{CO}_2\) ppm) methane (\(\mbox{CO}_2\) ppb) and nitrous oxide (\(\mbox{N}_2\mbox{O}\) ppb) concentrations every 20 years from 0-2000 CE. The data are a subset of ice core measurements from MacFarling Meure et al. 2006 via NOAA. There is a clear increase in all 3 gases starting around the time of the Industrial Revolution.
data(greenhouse_gases)

# line plots of atmospheric concentrations of the three major greenhouse gases since 0 CE
greenhouse_gases %&gt;%
    ggplot(aes(year concentration)) +
    geom_line() +
    facet_grid(gas ~ . scales = &quot;free&quot;) +
    xlab(&quot;Year&quot;) +
    ylab(&quot;Concentration (CH4/N2O ppb CO2 ppm)&quot;) +
    ggtitle(&quot;Atmospheric greenhouse gas concentration by year 0-2000 CE&quot;)

Compare this pattern with manmade carbon emissions since 1751 from temp_carbon which have risen in a similar way:
# line plot of anthropogenic carbon emissions over 250+ years
temp_carbon %&gt;%
    ggplot(aes(year carbon_emissions)) +
    geom_line() +
    xlab(&quot;Year&quot;) +
    ylab(&quot;Carbon emissions (metric tons)&quot;) +
    ggtitle(&quot;Annual global carbon emissions 1751-2014&quot;)



Carbon dioxide levels over the last 800000 years historic_co2
A common argument against the existence of anthropogenic climate change is that the Earth naturally undergoes cycles of warming and cooling governed by natural changes beyond human control. \(\mbox{CO}_2\) levels from ice cores and modern atmospheric measurements at the Mauna Loa observatory demonstrate that the speed and magnitude of natural variations in greenhouse gases pale in comparison to the rapid changes in modern industrial times. While the planet has been hotter and had higher \(\mbox{CO}_2\) levels in the distant past (data not shown) the current unprecedented rate of change leaves little time for planetary systems to adapt.
data(historic_co2)

# line plot of atmospheric CO2 concentration over 800K years colored by data source
historic_co2 %&gt;%
    ggplot(aes(year co2 col = source)) +
    geom_line() +
    ylab(&quot;CO2 (ppm)&quot;) +
    scale_color_manual(values = colorblind_palette[7:8]) +
    ggtitle(&quot;Atmospheric CO2 concentration -800000 BCE to today&quot;)




Properties of stars for making an H-R diagram: stars
In astronomy stars are classified by several key features including temperature spectral class (color) and luminosity (brightness). A common plot for demonstrating the different groups of stars and their propreties is the Hertzsprung-Russell diagram or H-R diagram. The stars data frame compiles information for making an H-R diagram with about approximately 100 named stars including their temperature spectral class and magnitude (which is inversely proportional to luminosity).
The H-R diagram has the hottest brightest stars in the upper left and coldest dimmest stars in the lower right. Main sequence stars are along the main diagonal while giants are in the upper right and dwarfs are in the lower left. Several aspects of data visualization can be practiced with these data.
data(stars)

# H-R diagram color-coded by spectral class
stars %&gt;%
    mutate(type = factor(type levels = c(&quot;O&quot; &quot;B&quot; &quot;DB&quot; &quot;A&quot; &quot;DA&quot; &quot;DF&quot; &quot;F&quot; &quot;G&quot; &quot;K&quot; &quot;M&quot;))
           star = ifelse(star %in% c(&quot;Sun&quot; &quot;Polaris&quot; &quot;Betelgeuse&quot; &quot;Deneb&quot;
                                     &quot;Regulus&quot; &quot;*SiriusB&quot; &quot;Alnitak&quot; &quot;*ProximaCentauri&quot;)
                         as.character(star) NA)) %&gt;%
    ggplot(aes(log10(temp) magnitude col = type)) +
    geom_point() +
    geom_label_repel(aes(label = star)) +
    scale_x_reverse() +
    scale_y_reverse() +
    xlab(&quot;Temperature (log10 degrees K)&quot;) +
    ylab(&quot;Magnitude&quot;) +
    labs(color = &quot;Spectral class&quot;) +
    ggtitle(&quot;H-R diagram of selected stars&quot;)
## Warning: Removed 88 rows containing missing values (geom_label_repel).



United States period life tables: death_prob
Obtained from the US Social Security Administration the 2015 period life table lists the probability of death within one year at every age and for both sexes. These values are commonly used to calculate life insurance premiums. They can be used for exercises on probability and random variables. For example the premiums can be calculated with a similar approach to that used for interest rates in this case study on The Big Short in Rafael Irizarry’s Introduction to Data Science textbook.


Brexit polling data: brexit_polls
brexit_polls contains vote percentages and spreads from the six months prior to the Brexit EU membership referendum in 2016 compiled from Wikipedia. These can be used to practice a variety of inference and modeling concepts including confidence intervals p-values hierarchical models and forecasting.
data(brexit_polls)

# plot of Brexit referendum polling spread between &quot;Remain&quot; and &quot;Leave&quot; over time
brexit_polls %&gt;%
    ggplot(aes(enddate spread color = poll_type)) +
    geom_hline(aes(yintercept = -.038 color = &quot;Actual spread&quot;)) +
    geom_smooth(method = &quot;loess&quot; span = 0.4) +
    geom_point() +
    scale_color_manual(values = colorblind_palette[1:3]) +
    xlab(&quot;Poll end date (2016)&quot;) +
    ylab(&quot;Spread (Proportion Remain - Proportion Leave)&quot;) +
    labs(color = &quot;Poll type&quot;) +
    ggtitle(&quot;Spread of Brexit referendum online and telephone polls&quot;)



Breast cancer diagnosis prediction: brca
This is the Breast Cancer Wisconsin (Diagnostic) Dataset a classic machine learning dataset that allows classification of breast lesion biopsies as malignant or benign based on cell nucleus characteristics extracted from digitized images of fine needle aspirate cytology slides. The data are appropriate for principal component analysis and a variety of machine learning algorithms. Models can be trained to a predictive accuracy of over 95%.
# scale x values
x_centered &lt;- sweep(brca$x 2 colMeans(brca$x))
x_scaled &lt;- sweep(x_centered 2 colSds(brca$x) FUN = &quot;/&quot;)

# principal component analysis
pca &lt;- prcomp(x_scaled) 

# scatterplot of PC2 versus PC1 with an ellipse to show the cluster regions
data.frame(pca$x[1:2] type = ifelse(brca$y == &quot;B&quot; &quot;Benign&quot; &quot;Malignant&quot;)) %&gt;%
    ggplot(aes(PC1 PC2 color = type)) +
    geom_point() +
    stat_ellipse() +
    ggtitle(&quot;PCA separates breast biospies into benign and malignant clusters&quot;)



Conclusion
We hope that these additional datasets make the dslabs package even more useful for teaching data science through real-world case studies and motivating examples.
Are you an R programming novice but want to learn how to do all of this and more? Check out the Data Science Professional Certificate Program from HarvardX on edX taught by Rafael Irizarry!
"
2019,5,29,Research quality data and research quality databases,https://simplystatistics.org/2019/05/29/research-quality-data-and-research-quality-databases/,"When you are doing data science you are doing research. You want to use data to answer a question identify a new pattern improve a current product or come up with a new product. The common factor underlying each of these tasks is that you want to use the data to answer a question that you haven’t answered before. The most effective process we have come up for getting those answers is the scientific research process. That is why the key word in data science is not data it is science.

No matter where you are doing data science - in academia in a non-profit or in a company - you are doing research. The data is the substrate you use to get the answers you care about. The first step most people take when using data is to collect the data and store it. This is a data engineering problem and is a necessary first step before you can do data science. But the state and quality of the data you have can make a huge amount of difference in how fast and accurately you can get answers. If the data is structured for analysis - if it is research quality  - then it makes getting answers dramatically faster.

A common analogy says that data is the new oil. Using this analogy pulling the data from all of the different available sources is like mining and extracting the oil. Putting it in a data lake or warehouse is like storing the crude oil for use in different products. In this analogy research is like getting the cars to go using the oil. Crude oil extracted from the ground can be used for a lot of different products but to make it really useful for cars you need to refine the oil into gas. Creating research quality data is the way that you refine and structure data to make it conducive to doing science. It means that the data is no longer as general purpose but it means you can use it much much more efficiently for the purpose you care about - getting answers to your questions.

Research quality data is data that:


Is summarized the right amount
Is formatted to work with the tools you are going to use
Is easy to manipulate and use
Is valid and accurately reflects the underlying data collection
Has potential biases clearly documented.
Combines all the relevant data types you need to answer questions


Let’s use an example to make this concrete. Suppose that you want to analyze data from an electronic health record. You want to do this to identify new potential efficiencies find new therapies and understand variation in prescribing within your medical system. The data that you have collected is in the form of billing records. They might be stored in a large database for a health system where each record looks something like this:



An example electronic health record. Source: http://healthdesignchallenge.com/

These data are collected incidentally during the health process and are designed for billing not for research. Often they contain information about what treatments patients received and were billed for but they might not include information on the health of the patient and whether they had any health complications or relapses they weren’t billed for.

These data are great but they aren’t research grade. They aren’t summarized in any meaningful way can’t be manipulated with visualization or machine learning tools are unwieldy and contain a lot of information we don’t need are subject to all sorts of strange sampling biases and aren’t merged with any of the health outcome data you might care about.

So let’s talk about how we would turn this pile of crude data into research quality data.



Turning raw data into research quality data.

Summarizing the data the right amount

To know how to summarize the data we need to know what are the most common types of questions we want to answer and what resolution we need to answer them. A good idea is to summarize things at the finest unit of analysis you think you will need - it is always easier to aggregate than disaggregate at the analysis level. So we might summarize at the patient and visit level. This would give us a data set where everything is indexed by patient and visit. If we want to answer something at a clinic physician or hospital level we can always aggregate there.

We also need to choose what to quantify. We might record for each visit the date prescriptions with standardized codes tests and other metrics. Depending on the application we may store the free text of the physician notes as a text string - for potential later processing into specific tokens or words. Or if we already have a system for aggregating physicians notes we could apply it at this stage.

Is formatted to work with the tools you are going to use

Research quality data is organized so the most frequent tasks can be completed quickly and without large amounts of data processing and reformatting. Each data analytic tool has different requirements on the type of data you need to input. For example many statistical modeling tools use “tidy data” so you might store the summarized data in a single tidy data set or a set of tidy data tables linked by a common set of indicators. Some software (for example in the analysis of human genomic data) require inputs in different formats - say as a set of objects in the R programming language. Others like software to fit a convolutional neural network to a set of images might require a set of image files organized in a directory in a particular way along with a metadata file providing information about each set of images. Or we might need to one hot encode categories that need to be classified.

In the case of our EHR data we might store everything in a set of tidy tables that can be used to quickly correlate different measurements. If we are going to integrate imaging lab reports and other documents we might store those in different formats to make integration with downstream tools easier.

Is easy to manipulate and use

This seems like it is just a re-hash of formatting the data to work with the tools you care about but there are some subtle nuances. For example if you have a huge amount of data (petabyes of images for example) you might not want to do research on all of those data at once. It will be inefficient and expensive. So you might use sampling to get a smaller data set for your research quality data that is easier to use and manipulate. The data will also be easier to use if they are (a) stored in an easy to access database with security systems well documented (b) have a data dictionary that makes it clear what the data are and where they come from or &copy; have a clear set of tutorials on how to perform common tasks on the data.

In our EHR example you might include a data dictionary that describes the dates of the data pull the types of data pulled the type of processing performed and pointers to the scripts that pulled the data.

Is valid and accurately reflects the underlying data collection

Data can be invalid for a whole host of reasons. The data could be incorrectly formatted input with error could change over time could be mislabeled and more. All of these problems can occur on the original data pull or over time. Data can also be out of date as new data becomes available.

The research quality database should include only data that has been checked validated cleaned and QA’d so that it reflects the real state of the world. This process is not a one time effort but an ongoing set of code scripts and processes that ensure the data you use for research are as accurate as possible.

In the EHR example there would be a series of data pulls code to perform checks and comparisons to additional data sources to validate the values levels variables and other components of the research quality database.

Has potential biases clearly documented

A research quality data set is by definition a derived data set. So there is a danger that problems with the data will be glossed over since it has been processed and easy to use. To avoid this problem there has to be documentation on where the data came from what happened to them during processing and any potential problems with the data.

With our EHR example this could include issues about how patients come into the system what procedures can be billed (or not) what data was ignored in the research quality database what are the time periods the data were collected and more.

Combines all the relevant data types you need to answer questions

One big difference between a research quality data set/database and a raw database or even a general purpose tidy data set is that it merges all of the relevant data you need to answer specific questions even if they come from distinct sources. Research quality data pulls together and makes easy to access all the information you need to answer your questions. This could still be in the form of a relational database - but the databases organization is driven by the research question rather than driven by other purposes.

For example EHR data may already be stored in a relational database. But it is stored in a way that makes it easy to understand billing and patient flow in a clinic. To answer a research question you might need to combine the billing data with patient outcome data and prescription fulfillment data all processed and indexed so they are either already merged or can be easily merged.

Why do this?

So why build a research quality data set? It sure seems like a lot of work (and it is!). The reason is that this work will always be done one way or the other. If you don&rsquo;t invest in making a research quality data set up front you will do it as a thousand papercuts over time. Each time you need to answer a new question or try a different model you&rsquo;ll be slowed down by the friction of identifying creating and checking a new cleaned up data set. On the one hand this amortizes the work over the course of many projects. But by doing it piecemeal you also dramatically increase the chance of an error in processing reduce answer time slow down the research process and make the investment for any individual project much higher.

Problem Forward Data Science

If you want help planning or building a research quality data set or database we can help at Problem Forward Data Science. Get in touch here: https://problemforward.typeform.com/to/L4h89P"
2019,5,20,I co-founded a company! Meet Problem Forward Data Science,https://simplystatistics.org/2019/05/20/i-co-founded-a-company-meet-problem-forward-data-science/,"I have some exciting news about something I&rsquo;ve been working on for the last year or so. I started a company! It&rsquo;s called Problem Forward data science.  I&rsquo;m pumped about this new startup for a lot of reasons.


My co-founder is one of my families closest friends Jamie McGovern who has more than 2 decades of experience in the consulting world and who I&rsquo;ve known for 15 years.
We are creating a cool new model of &ldquo;data scientist as a service&rdquo; (more on that below)
We have a problem forward not solution backward approach to data science that grew out of the Hopkins philosophy of data science.
We are headquartered in East Baltimore and are creating awesome new tech jobs in a place where they haven&rsquo;t been historically.


Problem Forward Not Solution Backward

We have always had a &ldquo;problem forward not solution backward&rdquo; approach to statistics machine learning and data here at Simply Stats. This has grown out of the Johns Hopkins Biostatistics philosophy of starting with the public health or medical problem you care about and working back to the statistical models software and tools you need to solve it.

This idea is so important to us it is in the name of the company. When we work with people our first goal is to find out the problems and questions that they genuinely care about then work backward to figure out how to solve them. We don&rsquo;t come in with a particular predetermined algorithm or strategy. One of the first questions we ask people isn&rsquo;t about data at all it is:


What question do you wish you could answer about your business (ignoring if you have the data or not to answer it yet)?


My favorite example of this is Moneyball. This is one of the classic stories about how the Oakland A&rsquo;s used data to gain a unique advantage. But one of the key messages about this story that often gets missed is that the data weren&rsquo;t unique to the A&rsquo;s! Everyone had the same data the A&rsquo;s just started with a problem that they needed to solve. They needed to find a unique way to win games that wasn&rsquo;t as expensive. Then they moved forward to looking at the data and realized that on base percentage was cheaper than home runs. So the A&rsquo;s used a &ldquo;problem forward not solution backward&rdquo; approach to data analysis.

Using this approach we have worked with companies with a wide variety of needs. Our main capabilities are in data strategy data cleaning and research quality database generation modeling and machine learning and data views through dashboards reports and presentations.



Data Scientist as a Service

There are a huge number of data science platform companies out there. Some of them are producing awesome tools but as any serious data analyst will tell you we are years from automating real data science. We are only very recently seeing formal definitions of what success of a data analysis even means! So it isn&rsquo;t surprising when general purpose platforms like IBM Watson struggle with specific problems - the problem isn&rsquo;t specified clearly enough for a platform to solve it yet..

The reason there are so many platforms is that its easy to sell the &ldquo;cool&rdquo; part of the problem - say building an AI to classify images or drive a car. But often the deeper problem is (a) figuring out what you even want to or can say with a set of data set (b) collecting a set of disorganized data &copy; getting buy in from groups with different motivations and data sets (d) organizing ugly data from different sources or finding new data you might need and (e) putting your answers in context. These problems are more like &ldquo;glue&rdquo; that comes between each of the platforms. We have a phrase we like to use:


To solve your data problem you need a person not a platform


So we have set up a &ldquo;platform&rdquo; that lets you scale up and down the number team members you have to solve data problems just like you would scale up and down the number of servers or tools that you use on AWS.



This means if you are an early stage startup we can help you scale data science before you can afford to hire a whole team. Even if you are a non-profit or a small academic group we can scale up or down to suit your needs. And if you are a big company we can provide utility data science for projects with tight deadlines.

Working with friends and building East Baltimore

The thing that gets me most excited about this new adventure is working with my really close friend Jamie. It&rsquo;s been huge for me to learn about the ins and outs of starting and running a business with someone who has decades of experience in the consulting industry.

It&rsquo;s also exciting to be able to headquarter the company right in East Baltimore and to work to upskill and develop talent here in a neighborhood I care about.

Like what you hear? Get in touch

If you are looking for data science work we&rsquo;d love to hear from you! Whether you are an academic a non-profit a small startup or a big business our utility model means we can work with you.

If you are interested in working with us contact us here:

https://problemforward.typeform.com/to/L4h89P"
2019,4,29,Generative and Analytical Models for Data Analysis,https://simplystatistics.org/2019/04/29/generative-and-analytical-models-for-data-analysis/,"Describing how a data analysis is created is a topic of keen interest to me and there are a few different ways to think about it. Two different ways of thinking about data analysis are what I call the “generative” approach and the “analytical” approach. Another more informal way that I like to think about these approaches is as the “biological” model and the “physician” model. Reading through the literature on the process of data analysis I’ve noticed that many seem to focus on the former rather than the latter and I think that presents an opportunity for new and interesting work.

Generative Model

The generative approach to thinking about data analysis focuses on the process by which an analysis is created. Developing an understanding of the decisions that are made to move from step one to step two to step three etc. can help us recreate or reconstruct a data analysis. While reconstruction may not exactly be the goal of studying data analysis in this manner having a better understanding of the process can open doors with respect to improving the process.

A key feature of the data analytic process is that it typically takes place inside the data analyst’s head making it impossible to directly observe. Measurements can be taken by asking analysts what they were thinking at a given time but that can be subject to a variety of measurement errors as with any data that depend on a subject’s recall. In some situations partial information is available for example if the analyst writes down the thinking process through a series of reports or if a team is involved and there is a record of communication about the process. From this type of information it is possible to gather a reasonable picture of “how things happen” and to describe the process for generating a data analysis.

This model is useful for understanding the “biological process” i.e. the underlying mechanisms for how data analyses are created sometimes referred to as “statistical thinking”. There is no doubt that this process has inherent interest for both teaching purposes and for understanding applied work. But there is a key ingredient that is lacking and I will talk about that more below.

Analytical Model

A second approach to thinking about data analysis ignores the underlying processes that serve to generate the data analysis and instead looks at the observable outputs of the analysis. Such outputs might be an R markdown document a PDF report or even a slide deck (Stephanie Hicks and I refer to this as the analytic container). The advantage of this approach is that the analytic outputs are real and can be directly observed. Of course what an analyst puts into a report or a slide deck typically only represents a fraction of what might have been produced in the course of a full data analysis. However it’s worth noting that the elements placed in the report are the cumulative result of all the decisions made through the course of a data analysis.

I’ve used music theory as an analogy for data analysis many times before mostly because&hellip;it’s all I know but also because it really works! When we listen to or examine a piece of music we have essentially no knowledge of how that music came to be. We can no longer interview Mozart or Beethoven about how they wrote their music. And yet we are still able to do a few important things:


Analyze and Theorize. We can analyze the music that we hear (and their written representation if available) and talk about how different pieces of music differ from each other or share similarities. We might develop a sense of what is commonly done by a given composer or across many composers and evaluate what outputs are more successful or less successful. It’s even possible to draw connections between different kinds of music separated by centuries. None of this requires knowledge of the underlying processes.
Give Feedback. When students are learning to compose music an essential part of that training is the play the music in front of others. The audience can then give feedback about what worked and what didn’t. Occasionally someone might ask “What were you thinking?” but for the most part that isn’t necessary. If something is truly broken it’s sometimes possible to prescribe some corrective action (e.g. “make this a C chord instead of a D chord”).


There are even two whole podcasts dedicated to analyzing music&mdash;Sticky Notes and Switched on Pop&mdash;and they generally do not interview the artists involved (this would be particularly hard for Sticky Notes). By contrast the Song Exploder podcast takes a more “generative approach” by having the artist talk about the creative process.

I referred to this analytical model for data analysis as the “physician” approach because it mirrors in a basic sense the problem that a physician confronts. When a patient arrives there is a set of symptoms and the patient’s own report/history. Based on that information the physician has to prescribe a course of action (usually to collect more data). There is often little detailed understanding of the biological processes underlying a disease but they physician may have a wealth of personal experience as well as a literature of clinical trials comparing various treatments from which to draw. In human medicine knowledge of biological processes is critical for designing new interventions but may not play as large a role in prescribing specific treatments.

When I see a data analysis as a teacher a peer reviewer or just a colleague down the hall it is usually my job to give feedback in a timely manner. In such situations there usually isn’t time for extensive interviews about the development process of the analysis even though that might in fact be useful. Rather I need to make a judgment based on the observed outputs and perhaps some brief follow-up questions. To the extent that I can provide feedback that I think will improve the quality of the analysis it is because I have a sense of what makes for a successful analysis.

The Missing Ingredient

Stephanie Hicks and I have discussed what are the elements of a data analysis as well as what might be the principles that guide the development of an analysis. In a new paper we describe and characterize the success of a data analysis based on a matching of principles between the analyst and the audience. This is something I have touched on previously both in this blog and on my podcast with Hilary Parker but in a generally more hand-wavey fashion. Developing a more formal model as Stephanie and I have done here has been useful and has provided some additional insights.

For both the generative model and the analytical model of data analysis the missing ingredient was a clear definition of what made a data analysis successful. The other side of that coin of course is knowing when a data analysis has failed. The analytical approach is useful because it allows us to separate the analysis from the analyst and to categorize analyses according to their observed features. But the categorization is “unordered” unless we have some notion of success. Without a definition of success we are unable to formally criticize analyses and explain our reasoning in a logical manner.

The generative approach is useful because it reveals potential targets of intervention especially from a teaching perspective in order to improve data analysis (just like understanding a biological process). However without a concrete definition of success we don’t have a target to strive for and we do not know how to intervene in order to make genuine improvement. In other words there is no outcome on which we can “train our model” for data analysis.

I mentioned above that there is a lot of focus on developing the generative model for data analysis but comparatively little work developing the analytical model. Yet both models are fundamental to improving the quality of data analyses and learning from previous work. I think this presents an important opportunity for statisticians data scientists and others to study how we can characterize data analyses based on observed outputs and how we can draw connections between analyses."
2019,4,17,"Tukey, Design Thinking, and Better Questions",https://simplystatistics.org/2019/04/17/tukey-design-thinking-and-better-questions/,"Roughly once a year I read John Tukey’s paper “The Future of Data Analysis” originally published in 1962 in the Annals of Mathematical Statistics. I’ve been doing this for the past 17 years each time hoping to really understand what it was he was talking about. Thankfully each time I read it I seem to get something new out of it. For example in 2017 I wrote a whole talk around some of the basic ideas.
Well it’s that time of year again and I’ve been doing some reading.
Probably the most famous line from this paper is

Far better an approximate answer to the right question which is often vague than an exact answer to the wrong question which can always be made precise.

The underlying idea in this sentence arises in at least two ways in Tukey’s paper. First is his warning that statisticians should not be called upon to produce the “right” answers. He argues that the idea that statistics is a “monolithic authoritarian structure designed to produce the ‘official’ results” presents a “real danger to data analysis”. Second Tukey criticizes the idea that much of statistical practice centers around optimizing statistical methods around precise (and inadequate) criteria. One can feel free to identify a method that minimizes mean squared error but that should not be viewed as the goal of data analysis.
But that got me thinking—what is the ultimate goal of data analysis? In 64 pages of writing I’ve found it difficult to identify a sentence or two where Tukey describes the ultimate goal why it is we’re bothering to analyze all this data. It occurred to me in this year’s reading of the paper that maybe the reason Tukey’s writing about data analysis is often so confusing to me is because his goal is actually quite different from that of the rest of us.

More Questions Better Questions
Most of the time in data analysis we are trying to answer a question with data. I don’t think it’s controversial to say that but maybe that’s the wrong approach? Or maybe that’s what we’re not trying to do at first. Maybe what we spend most of our time doing is figuring out a better question.
Hilary Parker and I have discussed at length the idea of design thinking on our podcast. One of the fundamental ideas from design thinking involves identifying the problem. It’s the first “diamond” in the “double diamond” approach to design.
Tukey describes the first three steps in a data analysis as:

Recognition of problem
One technique used
Competing techniques used

In other words try one approach then try a bunch of other approaches! You might be thinking why not just try the best approach (or perhaps the right approach) and save yourself all that work? Well that’s the kind of path you go down when you’re trying to answer the question. Stop doing that! There are two reasons why you should stop thinking about answering the question:

You’re probably asking the wrong question anyway so don’t take yourself too seriously;
The “best” approach is only defined as “best” according to some arbitrary criterion that probably isn’t suitable for your problem/question.

After thinking about all this I was inspired to draw the following diagram.


Strength of Evidence vs. Quality of Question

The goal in this picture is to get to the upper right corner where you have a high quality question and very strong evidence. In my experience most people assume that they are starting in the bottom right corner where the quality of the question is at its highest. In that case the only thing left to do is to choose the optimal procedure so that you can squeeze as much information out of your data. The reality is that we almost always start in the bottom left corner with a vague and poorly defined question and a similarly vague sense of what procedure to use. In that case what’s a data scientist to do?
In my view the most useful thing a data scientist can do is to devote serious effort towards improving the quality and sharpness of the question being asked. On the diagram the goal is to move us as much as possible to the right hand side. Along the way we will look at data we will consider things outside the data like context resources and subject matter expertise and we will try a bunch of different procedures (some optimal some less so).
Ultimately we will develop some of idea of what the data tell us but more importantly we will have a better sense of what kinds of questions we can ask of the data and what kinds of questions we actually want to have answered. In other words we can learn more about ourselves by looking at the data.


Exploring the Data
It would seem that the message here is that the goal of data analysis is to explore the data. In other words data analysis is exploratory data analysis. Maybe this shouldn’t be so surprising given that Tukey wrote the book on exploratory data analysis. In this paper at least he essentially dismisses other goals as overly optimistic or not really meaningful.
For the most part I agree with that sentiment in the sense that looking for “the answer” in a single set of data is going to result in disappointment. At best you will accumulate evidence that will point you in a new and promising direction. Then you can iterate perhaps by collecting new data or by asking different questions. At worst you will conclude that you’ve “figured it out” and then be shocked when someone else looking at another dataset concludes something completely different. In light of this discussions about p-values and statistical significance are very much beside the point.
The following is from the very opening of Tukey’s book *Exploratory Data Analysis:

It is important to understand what you CAN DO before you learn to measure how WELL you seem to have DONE it

(Note that the all caps are originally his!) Given this it’s not too surprising that Tukey seems to equate exploratory data analysis with essentially all of data analysis.


Better Questions
There’s one story that for me totally captures the spirit of exploratory data analysis. Legend has it that Tukey once asked a student what were the benefits of the median polish technique a technique he invented to analyze two-way tabular data. The student dutifully answered that the benefit of the technique is that it provided summaries of the rows and columns via the row- and column-medians. In other words like any good statistical technique it summarized the data by reducing it in some way. Tukey fired back saying that this was incorrect—the benefit was that the technique created more data. That “more data” was the residuals that are leftover in the table itself after running the median polish. It is the residuals that really let you learn about the data discover whether there is anything unusual whether your question is well-formulated and how you might move on to the next step. So in the end you got row medians column medians and residuals i.e. more data.
If a good exploratory technique gives you more data then maybe good exploratory data analysis gives you more questions or better questions. More refined more focused and with a sharper point. The benefit of developing a sharper question is that it has a greater potential to provide discriminating information. With a vague question the best you can hope for is a vague answer that may not lead to any useful decisions. Exploratory data analysis (or maybe just data analysis) gives you the tools that let the data guide you towards a better question.
"
2019,4,1,Interview with Abhi Datta,https://simplystatistics.org/2019/04/01/interview-with-abhi-datta/,"Editor’s note: This is the next in our series of interviews with early career statisticians and data scientists. Today we are talking to Abhi Datta about his work in large scale spatial analysis and his interest in soccer! Follow him on Twitter at @datta_science. If you have recommendations of an (early career) person in academics or industry you would like to see promoted reach out to Jeff (@jtleek) on Twitter!

SS: Do you consider yourself a statistician biostatistician data scientist or something else?

AD: That is a difficult question for me as I enjoy working on theory methods and data analysis and have co-authored diverse papers ranging from theoretical expositions to being primarily centered around a complex data analysis. My research interests also span a wide range of areas. A lot of my work on spatial statistics is driven by applications in environmental health and air pollution. Another significant area of my research is developing Bayesian models for epidemiological applications using survey data.

I would say what I enjoy most is developing statistical methodology motivated by a complex application where current methods fall short applying the method for analysis of the motivating data and trying to see if it is possible to establish some guarantees about the method through a combination of theoretical studies and empirical experiments that will help to generalize applicability of the method for other datasets. Of course not all projects involve all the steps but that is my ideal workflow. Not sure what that classifies me as.

SS: How did you get into statistics? What was your path to ending up at Hopkins?

AD: I was born and grew up in Kolkata India. I had the option of going for engineering medical or statistics undergrad. I chose statistics persuaded by my appreciation for mathematics and the reputation of the statistics program at Indian Statistical Institute (ISI) Kolkata. I completed my undergrad (BStat) and Masters (MStat) in Statistics from ISI and I’m thankful I made that choice as those 5 years at ISI played a pivotal role in my life. Besides getting rigorous training in the foundations of statistics most importantly I met my wife Dr. Debashree Ray at ISI.

After my Masters I had a brief stint in the finance industry working for 2 years at Morgan Stanley (in Mumbai and then in New York City) before I joined the PhD program at the Division of Biostatistics at University of Minnesota (UMN) in 2012 where Debashree was pursuing her PhD in Biostatistics. I had initially planned to work in Statistical Genetics as I had done a research project in that area in my Master’s. However I explored other research areas in my first year and ended up working on spatial statistics under the supervision of my advisor Dr. Sudipto Banerjee and on high-dimensional data with my co-advisorDr. Hui Zou from the Department of Statistics in Minnesota. I graduated from Minnesota in 2016 and joined Hopkins Biostat as an Assistant Professor in the Fall of 2016.

SS: You work on large scale spatio-temporal modeling - how do you speed up computations for the bootstrap when the data are very large?

AD: A main computational roadblock in spatio-temporal statistics is working with very big covariance matrices that strain memory and computing resources typically available in personal computers. Previously I have developed nearest neighbor Gaussian Processes (NNGP) &ndash; a Bayesian hierarchical model for inference in massive geospatial datasets. One issue with hierarchical Bayesian models is their reliance on long sequential MCMC runs. Bootstrap unlike MCMC can be implemented in an embarrassingly parallel fashion. However for geospatial data all observations are correlated across space prohibiting direct resampling for bootstrap.

In a recent work with my student Arkajyoti Saha we proposed a semi-parametric bootstrap for inference on large spatial covariance matrices. We use sparse Cholesky factors of spatial covariance matrices to approximately decorrelate the data before resampling for bootstrap. Arkajyoti has implemented this in an R-package BRISC: Bootstrap for rapid inference on spatial covariances. BRISC is extremely fast and at the time of publication to my knowledge it was the only R-package that offered inference on all the spatial covariance parameters without using MCMC. The package can also be used simply for super-fast estimation and prediction in geo-statistics.

SS: You have a cool paper on mapping local and global trait variation in plant distributions how did you get involved in that collaboration? Does your modeling have implications for people studying the impacts of climate change?

AD: In my final year of PhD at UMN I was awarded the Inter-Disciplinary Doctoral Fellowship &ndash;  a fantastic initiative by the graduate school at UMN providing research and travel funding and office space to work with an inter-disciplinary team of researchers on a collaborative project. In  my IDF mentored by Dr. Arindam Banerjee and Dr. Peter Reich I worked with a group of climate modelers ecologists and computer scientists from several institutions on a project whose eventual goal is to improve carbon projections from climate models.

The paper you mention was aimed at improving the global characterization of plant traits (measurements). This is important as plant trait values are critical inputs to climate model. Even the largest plant trait database TRY offers poor geographical coverage with little or no data across many large geographical regions. We used the fast NNGP approach I had been developing in my PhD to spatially gap-fill the plant trait data to create a global map of important plant traits with proper uncertainty quantification. The collaboration was a great learning experience for me on how to conduct a complex data analysis and how to communicate with scientists.

Currently we are looking at ways to incorporate the uncertainty quantified trait values as inputs to Earth System Models (ESMs) – the land component of climate models. We hope that replacing single trait values with entire trait distributions as inputs to these models will help to better propagate the uncertainty and improve the final model projections.

SS: What project has you most excited at the moment?

AD: There are two. I have been working with Dr. Scott Zeger on a project lead by Dr. Agbessi Amouzou in the Department of International Health at Hopkins aiming to estimate the cause-specific fractions (CSMF) of child mortality in Mozambique using family questionnaire data (verbal autopsy). Verbal autopsies are often used as a surrogate to full autopsy in many countries and there exists software that use these questionnaire data to predict a cause for every death. However these software are usually trained on some standard training data and yield inaccurate predictions in local context. This problem is a special case of transfer learning where a model trained using data representing a standard population offers poor predictive accuracy when specific populations are of interest. We have developed a general approach for transfer learning of classifiers that uses the predictions from these verbal autopsy software and limited full autopsy data from the local population to provide improved estimates of cause-specific mortality fractions. The approach is very general and offers a parsimonious model-based solution to transfer learning and can be used in any other classification-based application.

The second project involves creating high-resolution space-time maps of particulate matter (PM2.5) in Baltimore. Currently a network of low-cost air pollution monitors is being deployed in Baltimore that promises to offer air pollution measurements at a much higher geospatial resolution than what is provided by EPA’s sparse regulatory monitoring network. I was awarded a Bloomberg American Health Initiative Spark award for working with Dr. Kirsten Koehler in the Department of Environmental Health and Engineering to combine the low-cost network data the sparse EPA data and other land-use covariates to create uncertainty quantified maps of PM2.5 at an unprecedented spatial resolution. We have just started analyzing the first two months of data and I’m really looking forward to help create the end-product and understand how PM2.5 levels vary across the different neighborhoods in Baltimore.

SS: You have an interest in soccer and spatio temporal models have played an increasing role in soccer analytics. Have you thought about using your statistics skills to study soccer or do you try to avoid mixing professional work and being a fan?

AD: Yes I’m an avid soccer fan. I have travelled to Brazil in 2014 and Russia in 2018 to watch live games in the world cups. It also unfortunately means that I set my alarm to earlier times on weekends than on weekdays as the European league games start pretty early in US time.

However until recent times I’ve been largely ignorant of applications of spatio-temporal statistics in soccer analytics. I just finished teaching a Spatial Statistics course and one of the students presented a fascinating work he has done on predicting player’s scoring abilities using spatial statistics. I certainly plan to read more literature on this and maybe one day can contribute. Till then I remain a fan."
2021,6,25,Smarten Support Portal Updates – June – 2021!,https://www.smarten.com/blog/smarten-support-portal-updates-june-2021/,We invite you to explore our latest knowledgebase articles and to join the Smarten user community on&#160;Smarten Support Portal. If you have not registered yet&#160;Click Here&#160;to obtain your login credentials. Knowledgebase Articles Smarten Application Security: Banner Grabbing vulnerabilities and solutions Smarten Application Security: HTTP Method [&#8230;]
2021,6,24,Using Predictive Analytics to Understand Your Business Future!,https://www.smarten.com/blog/using-predictive-analytics-to-understand-your-business-future/,Can Predictive Analytics REALLY Help My Business During These Uncertain Times? How accurate is predictive analytics? Is it worth using for my business? How can forecasting and prediction help me in such an uncertain environment? These are all valid questions and they are they are [&#8230;]
2021,6,17,Why Do I Need to Analyze eCommerce Results?,https://www.smarten.com/blog/why-do-i-need-to-analyze-ecommerce-results/,Do You Want Your eCommerce Growth to Explode? Why does an eCommerce or online shopping business need analytics? After all eCommerce is exploding. There is no limit to what a business can do! Some of what you just read is true. Some of it isn’t! [&#8230;]
2021,6,16,How Can I Make Search and Analytics Easier for My Staff?,https://www.smarten.com/blog/how-can-i-make-search-and-analytics-easier-for-my-staff/,Wouldn’t it be great if you could give your business users an easy way to surf all that data you have within your walls? What if they could just ask a question? Just ask a simple question and get an answer that would help them [&#8230;]
2021,6,9,Tally ERP Analytics: Providing a Comprehensive Picture of Business!,https://www.smarten.com/blog/tally-erp-analytics-providing-a-comprehensive-picture-of-business/,The Tally ERP Solution is a popular accounting and financial application that is used by many business professionals. It acts as a repository for crucial business information and allows for data entry and reporting. But today every team member is expected to contribute to business [&#8230;]
2021,6,8,Clickless Analytics is the Future of Business User Analytics!,https://www.smarten.com/blog/clickless-analytics-is-the-future-of-business-user-analytics/,What Does a Business Need to Do To Ensure That Users Adopt Analytics? If your business is trying to incorporate data analytics into the fabric of day-to-day work you will need to get your users to adopt analytical tools. The way forward is not all [&#8230;]
2021,6,3,Give Citizen Data Scientists the Tools They Need!,https://www.smarten.com/blog/give-citizen-data-scientists-the-tools-they-need/,Don’t Be Intimidated! Citizen Data Scientists Don’t Need Rocket Science! When you take on the mantle of Citizen Data Scientist there is a lot to process! Much of the reason business users feel overwhelmed at the idea of the Citizen Data Scientist role is the [&#8230;]
2021,6,2,Digital Transformation (Dx) Requires Augmented Analytics Technology to Support Business Users!,https://www.smarten.com/blog/digital-transformation-dx-requires-augmented-analytics-technology-to-support-business-users/,If your business has embraced the concept of Digital Transformation (Dx) and Data Literacy it must plan for these initiatives in order to ensure that they are successful. The best way to achieve Dx and data literacy is to establish a technology environment that will [&#8230;]
2021,5,31,Machine Maintenance Using Smarten Assisted Predictive Modelling!,https://www.smarten.com/blog/machine-maintenance-using-smarten-assisted-predictive-modelling/,1. &#160;Machine Maintenance is always cheaper then downtime! Sooner or later all machines run to fail and monitoring the condition of the machine is crucial for any enterprise as any unplanned downtime can have greater economic impact resulting in reduced productivity and ultimately losing the [&#8230;]
2021,5,27,eCommerce Analytics is Mandatory to Business Success!,https://www.smarten.com/blog/ecommerce-analytics-is-mandatory-to-business-success/,Are You Succeeding at eCommerce? If You Don’t Know You NEED Integrated Analytics! When eCommerce and online shopping businesses reflect on their results it can sometimes be difficult to decide on a strategy going forward. You may be able to produce reports and have a [&#8230;]
2020,11,19,A Typology of Data Relationships,https://statswithcats.net/2020/11/19/a-typology-of-data-relationships/,Nine patterns of three types of relationships that aren’t spurious. When analysts see a large correlation coefficient they begin speculating about possible reasons. They’ll naturally gravitate toward their initial hypothesis (or&#160;preconceived notion) which set them to investigate the data relationship &#8230; Continue reading &#8594;
2020,10,22,The Evolution of Data Science … As I Remember It,https://statswithcats.net/2020/10/22/the-evolution-of-data-science-as-i-remember-it/,Those who cannot remember the past are condemned to repeat it. George Santayana History isn’t always clear-cut. It’s written by anyone with the will to write it down and the forum to distribute it. It’s valuable to understand different perspectives &#8230; Continue reading &#8594;
2020,10,16,PII. The Great Taboo of Data Analysis,https://statswithcats.net/2020/10/16/pii-the-great-taboo-of-data-analysis/,Many of us have had our personal information stolen from the Internet some more than once. Even governments can’t prevent the thefts. Professionals who work with data come under a lot of scrutiny because of PII. Continue reading &#8594;
2020,9,27,Anecdotes and Big Data,https://statswithcats.net/2020/09/27/anecdotes-and-big-data/,"An anecdote is a kitten gently licking your face with a warm wet 
raspy tongue. Big data is a three-inch high-pressure firehose held an arm’s length away. They have to be treated quite differently.
 Continue reading &#8594;"
2020,8,25,WHAT ARE THE ODDS?,https://statswithcats.net/2020/08/25/what-are-the-odds/,Probability is the core of statistics. You hear the phrase “what’s the probability of …” all the time in statistics. You also hear that phrase in everyday life too. What’s the probability of rain tomorrow? What’s the probability of winning &#8230; Continue reading &#8594;
2020,8,11,WHY YOU NEED TO TAKE STATS 101,https://statswithcats.net/2020/08/11/why-you-need-to-take-stats-101/,Whether you’re in high school college or a continuing education program you may have the opportunity or be required to take an introductory course in statistics call it Stats 101. It’s certainly a requirement for degrees in the sciences and &#8230; Continue reading &#8594;
2020,8,1,PROBABILITY IS SIMPLE … KINDA,https://statswithcats.net/2020/08/01/probability-is-simple-kinda/,Language isn’t very precise in dealing with uncertainty. Probably is more certain than possibly but who knows where dollars-to-doughnuts falls on the spectrum. Statistics needs to deal with uncertainty more quantitatively. That’s where probability comes in. For the most part &#8230; Continue reading &#8594;
2020,7,17,What To Look For In Data,https://statswithcats.net/2020/07/17/what-to-look-for-in-data/,Sometimes you have to do things when you have no idea where to start. It can be a stressful experience. If you&#8217;ve ever had to analyze a data set you know the anxiety. Deciding how and where to start exploring &#8230; Continue reading &#8594;
2020,7,11,35 Ways Data Go Bad,https://statswithcats.net/2020/07/11/35-ways-data-go-bad/,When you take your first statistics class your professor will be a kind person who cares about your mental well-being. OK maybe not but what the professor won’t do is give you real-world data sets. The data may represent things &#8230; Continue reading &#8594;
2020,7,6,The Most Important Statistical Assumptions,https://statswithcats.net/2020/07/05/the-most-important-statistical-assumptions/,
2021,6,22,Why Generalized Linear Models Have No Error Term,https://www.theanalysisfactor.com/generalized-linear-models-no-error-term/,"Even if you’ve never heard the term Generalized Linear Model you may have run one. It’s a term for a family of models that includes logistic and Poisson regression among others. It’s a small leap to generalized linear models if you already understand linear models. Many many concepts are the same in both types of [&#8230;]
The post Why Generalized Linear Models Have No Error Term appeared first on The Analysis Factor."
2021,6,10,Interpreting the Shapes of Hazard Functions in Survival Analysis,https://www.theanalysisfactor.com/survival-analysis-interpreting-shapes-of-hazard-functions/,"by Steve Simon PhD Hazard functions are a key tool in survival analysis. But they&#8217;re not always easy to interpret. In this article we&#8217;re going to explore the definition purpose and meaning of hazard functions. Then we&#8217;ll explore a few different shapes to see what they tell us about the data. Motivating example This is [&#8230;]
The post Interpreting the Shapes of Hazard Functions in Survival Analysis appeared first on The Analysis Factor."
2021,6,1,Member Training: An Introduction into the Grammar of Graphics,https://www.theanalysisfactor.com/grammar-of-graphics-introduction/,"As it has been said a picture is worth a thousand words and so it is with graphics too. A well constructed graph can summarize information collected from tens to hundreds or even thousands of data points. But not every graph has the same power to convey complex information clearly. Underlying every statistical graph is [&#8230;]
The post Member Training: An Introduction into the Grammar of Graphics appeared first on The Analysis Factor."
2021,5,19,What is a Chi-Square Test?,https://www.theanalysisfactor.com/what-is-a-chi-square-test/,"Just about everyone who does any data analysis has used a chi-square test. Probably because there are quite a few of them and they’re all useful. But it gets confusing because very often you’ll just hear them called “Chi-Square test” without their full formal name. And without that context it’s hard to tell exactly what [&#8230;]
The post What is a Chi-Square Test? appeared first on The Analysis Factor."
2021,5,11,Missing Data Mechanisms: A Primer,https://www.theanalysisfactor.com/causes-of-missing-data/,"Missing data are a widespread problem as most researchers can attest. Whether data are from surveys experiments or secondary sources missing data abounds. But what&#8217;s the impact on the results of statistical analysis? That depends on two things: the mechanism that led the data to be missing and the way in which the data analyst [&#8230;]
The post Missing Data Mechanisms: A Primer appeared first on The Analysis Factor."
2021,5,3,Member Training: Writing Study Design and Statistical Analysis Plans,https://www.theanalysisfactor.com/study-design-statistical-analysis-plans/,"One component often overlooked in the ‘Define &#38; Design’ phase of a study is writing the analysis plan. The statistical analysis plan integrates a lot of information about the study including the research question study design variables and data used and the type of statistical analysis that will be conducted. In this training we consider [&#8230;]
The post Member Training: Writing Study Design and Statistical Analysis Plans appeared first on The Analysis Factor."
2021,1,17,Python script for requesting user input repeatedly,http://themainstreamseer.blogspot.com/2021/01/python-script-for-requesting-user-input.html,"&nbsp;Someone recently asked how to write a Python script that took user input into account and took some action based on the validity of the user input.&nbsp; Here are some code snippets that I developed for a few different scenarios:1) A game is being played and it has just ended.&nbsp; How would you ask the user if they wanted to play again?game_running = True #assuming the game is running play_again = input(""Play again? "")&nbsp; #get user inputwhile True: #restart game if answer is ""y"" or ""Y""&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if play_again == 'y' or play_again == ""Y"":&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; game_running = True&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; break&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; else: #print thank you message and exit the program&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; print(""Thank you for playing!"")&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; game_running = False&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; breakHere if the user answers either ""y"" or ""Y"" they get to play the game again.&nbsp; If they type anything else they receive a thank you message and the game ends.2) Create a while loop which validates that the user input is a float before it takes an action on it.#create a while loop to seek user input for fuel to burn in the next second until valid input is provided#cast valid user input as a float#use try and except statements to account for invalid user input&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; while (some condition):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x = (input(""Enter a number: ""))&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; try:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x = float(x)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; break&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; except ValueError as e:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; print(str(x) + "" is not a valid number.&nbsp; Please try again."")3) Define a function that asks a user for input does something if the answer is ""yes"" does something else if the answer is ""no"" and prompts the user again if the answer is neither ""yes"" or ""no"".def user_input():&nbsp;&nbsp;&nbsp; while True:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x = input(""Enter yes or no: "")&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (x == ""yes"" or x == ""Yes""):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; continue&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; elif (x == ""no"" or x == ""No""):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; break&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; else:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; print(""Invalid input.&nbsp; Please try again."")&nbsp;user_input()4) Similar to #3 except that the function has a parameter.def user_input(x):&nbsp;&nbsp;&nbsp; while True:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x = input(""Enter y or n: "")&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (x == ""y"" or x == ""Y""):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; continue&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; elif (x == ""n"" or x == ""N""):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; break&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; else:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; print(""Invalid input.&nbsp; Please try again."")user_input(""Enter y or n"")"
2020,2,5,Dis-aggregated hardware,http://themainstreamseer.blogspot.com/2020/02/dis-aggregated-hardware.html,This post focuses on dis-aggregated hardware follows my earlier post Open Source Networking: a hierarchical approach.&nbsp;Hardware dis-aggregation is an important aspect of open networking.&nbsp; Essentially dis-aggregated hardware involves software being separated from the underlying hardware.&nbsp; Similar to a PC or a server where you can boot with a Linux installer CD to Linux you are able to boot dis-aggregated hardware with an Operating System (OS) installer and install an operating system on it.&nbsp; Most commercial vendors bundle their hardware with software which means you cannot modify the software or change the OS that it comes loaded with.&nbsp; Open dis-aggregated hardware however does not have this restriction.Take for example Ethernet switches which have two main components:A packet switching/processing ASIC chipset controllerA CPU which hosts the software / firmware and the packet switching ASIC.Most commercial vendors bundle the software with their hardware whereas Ethernet switches by vendors of dis-aggregated hardware contain the same hardware components but allow you to install your choice of network OS (for example Open Network Linux or ONL) on it.&nbsp; ONL and other network OS provide hardware compatibility guides which clearly specify what hardware they support.&nbsp; Source: Reza Toghraee and The Linux FoundationThe most simple form of a disaggregated networking hardware is a  standard bare metal x86 machine with multiple network adapter cards. You can  install a Linux OS on that PC activate routing and  transform it into a firewall or a router. &nbsp;There are two main open source projects that are driving developments in the dis-aggregated hardware space:Open Compute Project; andTelecom Infra Project.Open Compute ProjectIn 2009 as Facebook was growing exponentially the company realized that it had to rethink its infrastructure  to accommodate the huge influx of new people and data and also control  costs and energy consumption.&nbsp; Facebook then started a project to design the world’s most energy  efficient data center one that could handle unprecedented scale at the  lowest possible cost.&nbsp; Two years and a small team of engineers later they built a data center from the ground up at Prineville Oregon including software servers  racks power supplies and cooling.&nbsp; It was 38% more energy efficient to build and 24%  less expensive to run than the company’s previous facilities—and has  led to even greater innovation.&nbsp;In 2011 Facebook shared its designs with the  public and—along with Intel and Rackspace Goldman Sachs and Andy  Bechtolsheim—launched the Open Compute Project (OCP) and incorporated the Open  Compute Project Foundation.&nbsp; There are multiple project workgroups within  OCP each including a project charter and a team working towards  producing and enhancing the open source technologies within that  project.&nbsp; Currently these workgroups are:Data Center Facility: focuses on maximizing mechanical performance and thermal and electrical efficiencyHardware Management: incorporates a set of existing tools and best practices and leverages existing tools for remote machine managementNetworking: creating a set of technologies that are dis-aggregated and fully&nbsp;open allowing for rapid innovation in the network spaceOpen System Firmware: creating and deploying at scale an open source hardware platform  initialization and OS load firmware optimized for web-scale cloud  hardwareRack &amp; Power: focuses on rack standards that are designed for data centers integrating the rack into the data center infrastructureSecurity: creates designs and specifications to enable software security for all  IT gear through collaboration with the wider Open Compute communityServer: provides standardized server system specifications for scale computingStorage: focuses on chassis and sleds components and peripherals networked enabled storage and compatibility solutionsTelco: enlists participants from telecom companies and carriers as well as sub  systems software board and semiconductor suppliers who are seeking to  use data center infrastructure to deliver IT servicesMore details on each of these can be found at the OCP website: https://www.opencompute.org/ Telecom Infra ProjectSimilar to the Open  Compute Project which addresses open source hardware and software used  to build data centers&nbsp;and enterprise IT the Telecom Infra Project (TIP) is a  workgroup meant to create open source standards for telecommunication  and service provider companies. The Telecom Infra Project includes  multiple project groups:Access projects:&nbsp; working to identify and create innovative infrastructure solutions  technologies and methodologies to make it easier to connect people to  the internet. Access is focused on removing some of the blockers that  can make the connection to the end user difficult.Transport projects: to keep the pace of exponential growth in network traffic better  backhaul is essential. Our transport project groups are addressing the  scalability fast convergence ease of configuration and extensibility  challenges in wireless and wired backhaul.Core &amp; Services: the most significant costs associated with a network are the ongoing  costs of operations and maintenance. The core and services project  groups are simplifying the core network architecture and improving  efficiency and flexibility while reducing ongoing costs associated with  keeping a network up and running.&nbsp;More details on each of these can be found at the TIP website: https://telecominfraproject.com/&nbsp;My next blog will focus on IO Abstraction and Datapath.&nbsp; 
2020,1,16,Open Source Networking: a hierarchical approach,http://themainstreamseer.blogspot.com/2020/01/open-source-networking-hierarchical.html,I have been extremely fortunate to be part of several network cloudification projects over the past 18-24 months.&nbsp; It's been very exciting to see the changes that disaggregation and open source projects have been making to move an industry forward.&nbsp; Here I capture some initial thoughts about open source networking.Note: while the on-the-job learning has been immense I have supplemented this with some terrific courses from The Linux Foundation including LinuxFoundationX:      LFS165x             Introduction to Open Source Networking Technologies by Reza Toghraee.    Networking has come a long way from the widespread use of rigid appliances to&nbsp;perform networking functions such as routing firewalling switching and load balancing.&nbsp; A key principle behind this transformation is disaggregation i.e. the de-coupling of the software performing the networking functions with the hardware it is installed on.&nbsp; The main driver of this transformation is the evolution of Cloud computing.Let's explore the impact of Cloud computing on networking further.&nbsp; The Cloud virtualizes Compute Storage and Network.&nbsp; Compute virtualization came first.&nbsp; While things began slowly once virtualization technology became reliable enterprises began migrating physical servers to virtual servers en masse.&nbsp; Today almost all enterprise applications are served on virtual compute environments.&nbsp; Storage virtualization came next with massive disk arrays giving way to distributed filesystems where JBOD (Just a Bunch of Disks) could be converted into a highly available and robust storage system.&nbsp; Both compute and storage virtualization disaggregated software from the underlying hardware.&nbsp; The same changes have now been occurring with networking over the past several years.&nbsp; The open source revolution has been a driving force behind this change and several open source projects exist for both hardware and software elements of networking.To understand all elements of the open source networking landscape it is useful to take a hierarchical approach starting with the open hardware at the bottom and working our way up to the applications layer at the top.&nbsp; Here is the complete hierarchy:Disaggregated HardwareIO Abstraction and DatapathNetwork Operating SystemsNetwork ControlNetwork VirtualizationCloud and Virtual ManagementOrchestration Management PolicyNetwork Data AnalyticsApplication Layer.A diagram with this hierarchy and the various open source projects associated with it is provided below (source: Reza Toghraee and The Linux Foundation):In following blogs I intend to cover the different elements of the hierarchy in detail along with the various open source projects that are driving developments in this space. 
2020,1,13,Curious about 5G?,http://themainstreamseer.blogspot.com/2020/01/curious-about-5g.html,Are you curious about 5G but not sure what it's all about?&nbsp; Here's a quick summary to bring you up to speed.A quick refresher about 1G through 4G before we get started:1G refers to the first generation of wireless cellular technologies that brought us our very first cell phones.2G was a significant improvement from 1G in the way the radio frequency spectrum was used enabling many more users per frequency band.&nbsp; Importantly for consumers 2G enabled digitally encrypted conversations and SMS text messages!3G was a giant leap forward from 2G.&nbsp; 3G enabled consumers to get online using their cell phones.4G pushed the limits further with improved speeds and increased applications for consumers including IP telephony high def mobile TV and video conferencing.&nbsp;So what does 5G promise?&nbsp; 5G promises to dramatically improve speed latency and scale. It is expected to be 100X faster than 4G (you could download an HD movie in 1 second!). Its data volume capacity is expected to be 1000X that of 4G when fully deployed.&nbsp; As a result 5G is expected to be the foundation for turbocharging applications like Virtual Reality Autonomous Driving and the Internet of Things.So how does 5G do this? &nbsp; Here are 5 key technologies enabling 5G:Millimeter waves: mobile phones and other electronic devices use frequencies under 6GHz on the radio frequency spectrum.&nbsp; As the number of devices online increase these frequencies are getting crowded leading to issues like dropped calls.&nbsp; To solve this researchers are experimenting with using extremely high frequency or millimeter waves (30 to 300 GHz).Small cells: Small cells are low-powered mini base stations that  have a range of 10 meters to a few kilometers. Fewer new macrocell  sites are being built with larger numbers of small cells recognized as  an important method of increasing cellular network capacity quality and  resilience.Massive MIMO: MIMO stands for multiple-input and multiple-output.&nbsp; It is a method for multiplying the capacity of a radio link using  multiple transmission and receiving antennas.&nbsp; 4G base stations have about 12 ports for all traffic.&nbsp; 5G / Massive MIMO base stations are expected to have 100 ports which could increase capacity by 22X or more.Beamforming: Beamforming or spatial filtering is a  signal processing technique used in sensor arrays for directional signal  transmission or reception.&nbsp; Instead of broadcasting signals in every direction beamforming would allow a base station to send a specific signal to a user preventing interference.Full Duplex: Today's base station antennas can only do one job at a time - either transmit or receive - a principle known as reciprocity.&nbsp; Researchers have use silicon transistors to create high speed switches that enable networks to overcome reciprocity thereby increasing speeds of data communication.&nbsp;Here's a great video by IEEE Spectrum on this topic: Are there any other technologies not covered in this post?&nbsp; If so please let me know in the comments column.
2019,7,16,"AI, ML, NN and DL: a visual explanation",http://themainstreamseer.blogspot.com/2019/07/ai-ml-nn-and-dl-visual-explanation.html,There appears to be a lot of confusion between the terms Artificial Intelligence (AI) Machine Learning (ML) Neural Networks (NN) and Deep Learning (DL).&nbsp; Based on research from various popular blogs and articles here is my attempt at a simple visual explanation:
2019,6,23,The Research Process,http://themainstreamseer.blogspot.com/2019/06/the-research-process.html,To answer interesting questions you need data.You begin with an observation that you want to understand including anecdotal observations.&nbsp; For example a certain website layout attracts more visitors to our web page than a different website layout.&nbsp; From your observations you generate explanations or theories of those observations from which you can make predictions or hypothesis.&nbsp; To test your hypothesis or predictions you need data.So you collect relevant data (and to do that you need to identify things that can be measured) and then you analyze those data.&nbsp; The analysis of your data may support your theory or give you cause to modify the theory.As such the processes of data collection and analysis and generating theories are intrinsically linked: theories lead to data collection / analysis and data collection / analysis informs theories.&nbsp; The research process is summarized below:(adapted from Discovering Statistics using R by Andy Field et al)
2019,3,28,Operationalize Trusted AI with IBM Watson OpenScale,http://themainstreamseer.blogspot.com/2019/03/operationalize-trusted-ai-with-ibm.html,
2019,2,4,Satellite imagery and remote sensing puzzles,http://themainstreamseer.blogspot.com/2019/02/satellite-imagery-and-remote-sensing.html,If you are looking for a fun way to experience satellite imagery and learn more about remote sensing check out Earth Image Puzzles here.Here is a solved jigsaw puzzle of SouthEastern PA:Enjoy!
2019,1,9,The world of languages,http://themainstreamseer.blogspot.com/2019/01/the-world-of-languages.html, Courtesy of: Visual Capitalist  
2018,7,22,Producing a map with 5 lines of code,http://themainstreamseer.blogspot.com/2018/07/producing-map-in-5-lines-of-code.html,"Over the past year I have been exploring the geospatial capabilities of various R packages.&nbsp; Today I want to share the most basic of geospatial capabilities which is producing a map.&nbsp; Using R you can do this in just 5 lines of code.Let's produce a map of Boston Massachusetts.&nbsp; Boston has a longitude of -71.0588801 and a latitude of 42.3600825.&nbsp; Since we list x and y coordinates in order (i.e. we list y after x) we list longitude (the horizontal coordinate) before latitude (the vertical coordinate).&nbsp; Let's create our map!Code line 1:install.packages(""ggmap"")This command installs the ggmap package in your R environment.Code line 2:library(ggmap)This command loads the ggmap package in your R environment.Code line 3:boston &lt;- c(lon = -71.0588801 lat = 42.3600825)This line creates a variable called ""boston"" and assigns the lon and lat coordinates in it.Code line 4:boston_map &lt;- get_map(boston zoom = 13 scale = 1)This line creates a variable called ""boston_map"" and assigns the get_map command to it.Code line 5:ggmap(boston_map)This line generates the map.And that's it!&nbsp; If you reproduce these five lines in your R environment you will generate a map that looks like this:"
2018,4,29,IBM SPSS and Entity Analytics at work,http://themainstreamseer.blogspot.com/2018/04/ibm-spss-and-entity-analytics-at-work.html,
2018,4,21,Testing Senzing's Entity Resolution Workbench,http://themainstreamseer.blogspot.com/2018/04/testing-senzings-entity-resolution.html,"I have the great honor of knowing ex-IBM Fellow Jeff Jonas the co-Founder CEO and Chief Scientist of Senzing.&nbsp; Apart from being exceptionally talented Jeff is also an amazing human being who is always willing to help others.&nbsp; I have personally been the beneficiary of his generosity and continue to benefit from his counsel every day.&nbsp; Jeff is one of the main reasons why I have chosen to follow a technical career path at IBM.Jeff left IBM in 2016 to start a new venture called Senzing.&nbsp; Senzing has built the first real-time AI software product for Entity Resolution (ER) a space that Jeff is the world's #1 expert in.&nbsp; Senzing's new offering has huge implications in the post-GDPR world and has the potential to increase trust in Blockchain networks.&nbsp; Jeff recently gave a keynote at the IBM Think conference where he described what Senzing does and its potential applications (including as part of IBM Blockchain).&nbsp; I strongly recommend watching it. When I spoke with Jeff yesterday he asked that I give Senzing's ER workbench a try and provide feedback.&nbsp; So that is what I did earlier today.&nbsp; Here are my first impressions.Questions for JeffCurrently Senzing only runs on Windows.&nbsp; When will it be offered on other operating systems (especially MacOS)?Why do I need to download the workbench?&nbsp; Can I not just have a Cloud based version?Getting startedI found the workbench very easy to use.&nbsp; The instructions were clear and the steps to get from start to finish were intuitive.I uploaded a csv file of all my Google contacts.&nbsp; I could not believe I had 2451 contacts in my Google contact list!&nbsp; Clearly I have a lot of spring cleaning to do.The csv file upload process was straightforward and quick.&nbsp; On that point though the workbench currently only works with csv files.&nbsp; Any plans to directly connect to other data sources?The ER process is very quick.&nbsp; After uploading your data ER is a one-click process.&nbsp; Very cool.The user interface could use an upgrade.Results of the ER processThe workbench identified 32 duplicates 4 possible duplicates and 6 possibly related entities.The results had a lot more detail than Google contact's duplicate function providesInterestingly of the 6 possibly related entries entities 5 and 6 both related to my wife.&nbsp; I was a little surprised that the workbench did not merge them both and give me 5 possibly related entities instead of 6Apart from this it was really interesting to see how the workbench linked different entities&nbsp;Single Search FunctionThe Single Search Function (SSF) is very cool.&nbsp; I only tried it with the name field since it was the most intuitive one for me to try it with.One potential bug(?) I noticed is that you have to type full name of a contact in order for the SSF to work correctly.&nbsp; Partial name (just first or last name) searches resulted in (0 results found) errors.Also I wish there was an option to merge the various contacts.&nbsp; While this may not be the focus of the workbench sometimes it is useful if you want to clean up an address book.&nbsp; For example in Google contacts after it displays the duplicates it gives you the option of merging all contacts.&nbsp; That gives the process a logical end point IMHO.Compared to Google contacts' duplicates functionIt is probably unfair to compare Senzing's workbench to Google contacts' duplicates function but I did it and I might as well write about it.Google contacts identified 8 duplicates (Senzing identified 32 duplicates 4 possible duplicates and 6 possibly related entities).The results were not nearly as sophisticated as those from Senzing in terms of the information provided.Also Google got several duplicates wrong.&nbsp; Some were clearly not related.&nbsp; For example for one contact I had an old phone number and a new phone number saved.&nbsp; Even though the person who now owned the ""old"" phone number was clearly different from my friend (based on a Google update they had posted about where they were and a new photograph) Google suggested they might be the same person.&nbsp; Senzing did not.Give it a try yourselfOverall I really enjoyed taking Senzing's ER workbench for a test ride.&nbsp; You can too.&nbsp; Go watch Jeff's IBM Think keynote to get a password to download it and take it for a spin!"
2018,3,1,"Watson Analytics, SPSS Modeler and Esri ArcGIS",http://themainstreamseer.blogspot.com/2018/03/watson-analytics-spss-modeler-and-esri.html,
2017,11,14,Visualization of the 1854 London Cholera Outbreak,http://themainstreamseer.blogspot.com/2017/11/visualization-of-1854-london-cholera.html,This post attempts to visualize the 1854 London Cholera Outbreak based on data collected by Dr. John Snow and provided in the HistData R package. Dr. Snow was able to identify that cholera was a water borne disease by visualizing his data in 1854 and was able to bring the Cholera outbreak to an end. This dataset and analysis speaks to power of geospatial data and its importance in decision making.
2017,10,27,What caused the Challenger disaster?,http://themainstreamseer.blogspot.com/2017/10/what-caused-challenger-disaster.html,The motivation for this blog is to examine the reasons behind the explosion of the USA Space Shuttle Challenger on 28 January 1986. The night before the launch a decision had to be made regarding launch safety and engineers recommended that the launch be postponed in the event the temperature at launch was below freezing as this adversely impacted the integrity of O-rings a key component holding in field joints. The engineers advice was ignored and disaster ensued. Let's dive in!
2017,10,15,Regression in R,http://themainstreamseer.blogspot.com/2017/10/regression-in-r.html,My&nbsp;latest publicly available R notebook&nbsp;created in&nbsp;IBM's Data Science Experience&nbsp;is&nbsp;here! &nbsp;This notebook provides a tutorial on:This notebook covers:Fitting and interpreting linear models;Evaluating model assumptions; andSelecting among competing models.I hope you enjoy this&nbsp;notebook. &nbsp;Please feel free to share and let me know your thoughts.My latest notebook: Regression in R https://t.co/HDYFzTAFPr #rstats #DataScience #ibmaot #Statistics #Stats #dsx #Bluemix h/t @kabacoff pic.twitter.com/LxKc9HkBC0— Venky Rao (@VRaoRao) October 15 2017 
2017,10,7,Coefficient of Alienation,http://themainstreamseer.blogspot.com/2017/10/coefficient-of-alienation.html,"If you thought the coefficient of alienation referred to the hostility I receive from my family as I update my blog on a Saturday afternoon I would not fault you too much. &nbsp;However this is a blog about predictive analytics which is based on Statistics. &nbsp;So let's keep that in mind as we understand what the ""Coefficient of Alienation"" means.Apart from being one of the coolest sounding Statistical terms the Coefficient of Alienation measures the proportion of variation in the outcome not “explained” by the variables on the right-hand side of a simple linear regression (ordinary least squares) equation.The Coefficient of Alienation is also known as the Coefficient of Non-Determination since the formula for calculating it is:where:is the Coefficient of Determination.And now before my personal (and non-Statistical) Coefficient of Alienation reaches the point of no return I will bring this post to an end."
2017,9,30,Homoscedasticity and heteroscedasticity,http://themainstreamseer.blogspot.com/2017/09/homoscedasticity-and-heteroscedasticity.html,"Homoscedasticity and heteroscedasticity - two of the scariest sounding terms in all of Statistics! &nbsp;So what do they mean?When one calculates the variance or standard deviation of a dataset of random variables one assumes that the variance is constant across the entire population. &nbsp;This assumption is homoscedasticity. &nbsp;The opposite of this assumption is heteroscedasticity.In other words a collection of random variables is heteroscedastic if there are sub-populations within the dataset that have different variances from others (source: https://en.wikipedia.org/wiki/Heteroscedasticity). &nbsp;Another way of describing homoscedasticity is constant variance and another way of describing heteroscedasticity is variable variance.Jeremy J Taylor&nbsp;in his blog&nbsp;provides a great example of a distribution that is heteroscedastic. &nbsp;In his example the independent variable is ""age"" and the predictor variable is ""income"". &nbsp;The example discusses how incomes start to vary more as age increases (as some people earn more than others as they grow older). &nbsp;You can read his blog on this topic here."
2017,9,27,Standard Deviation versus Absolute Mean Deviation,http://themainstreamseer.blogspot.com/2017/09/standard-deviation-versus-absolute-mean.html,One of the first things that any student of statistics learns is 2 popular measures of descriptive statistics: mean and standard deviation.Has the approach to calculating Standard Deviation ever got you wondering about the need to square the distances from the mean in order to remove negatives instead of just using the average of the absolute values to eliminate negatives? &nbsp;Well you are certainly not alone.As it turns out squaring the distances from the mean and then calculating their square root to arrive at the Standard Deviation of a distribution is more as a result of convention than anything else. &nbsp;In fact there is a measure called the Absolute Mean Deviation that does not take the squared distances from the mean to eliminate negative values. &nbsp;Instead it just takes the absolute values of the differences from the mean and calculates the average of the sum of those values to determine deviation from the mean.The convention of course is to use Standard Deviation in most cases instead of Absolute Mean Deviation and therefore it is much more popular as a descriptive statistic than Absolute Mean Deviation. &nbsp;Here is an interesting article that discusses the difference between the two approaches and identifies situations where using the Absolute Mean Deviation may be advantageous.
2017,9,13,Basic Statistics in R,http://themainstreamseer.blogspot.com/2017/09/basic-statistics-in-r.html,My&nbsp;latest publicly available R notebook&nbsp;created in&nbsp;IBM's Data Science Experience&nbsp;is&nbsp;here! &nbsp;This notebook provides a tutorial on:This notebook covers:Descriptive statisticsFrequency and contingency tablesCorrelations and covariancest-tests; andNonparametric statistics.I hope you enjoy this&nbsp;notebook. &nbsp;Please feel free to share and let me know your thoughts.My latest R notebook: Basic #Statistics in R https://t.co/b3NmhNXI5X #DataScience #dsx #IBM #Bluemix #ibmaot #rstats h/t @kabacoff pic.twitter.com/AsIhr51Q5l— Venky Rao (@VRaoRao) September 13 2017
2017,9,13,Adding a .RData file to DSX in 5 easy steps,http://themainstreamseer.blogspot.com/2017/09/adding-rdata-file-in-5-easy-steps.html,I created a tutorial to show how users can add a .RData file to an R Jupyter Notebook in IBM's Data Science Experience (DSX) in 5 easy steps.My latest #R #notebook: Add a .RData file to a DSX R Notebook in 5 steps https://t.co/uznXwZWKSv #dsx #IBM #DataScience #rstats #ibmaot pic.twitter.com/plKuTwYDwt&mdash; Venky Rao (@VRaoRao) September 13 2017
2017,9,4,Basic graphs in R,http://themainstreamseer.blogspot.com/2017/09/basic-graphs-in-r.html,My&nbsp;latest publicly available R notebook&nbsp;created in&nbsp;IBM's Data Science Experience&nbsp;is&nbsp;here! &nbsp;This notebook provides a tutorial on:Bar box and dot plotsPie and fan chartsHistograms and kernel density plots.I hope you enjoy this&nbsp;notebook. &nbsp;Please feel free to share and let me know your thoughts. My latest #R #notebook: Basic Graphs in R https://t.co/o7j97GwEUL #DataScience #dsx #IBM #ibmaot h/t @kabacoff pic.twitter.com/MBfZQgg4Y0&mdash; Venky Rao (@VRaoRao) September 4 2017
2017,9,2,Advanced Data Preparation in R,http://themainstreamseer.blogspot.com/2017/09/advanced-data-preparation-in-r.html,My&nbsp;latest publicly available R notebook&nbsp;created in&nbsp;IBM's Data Science Experience&nbsp;is&nbsp;here! &nbsp;This notebook addresses some advanced features available in R focusing on Data Preparation.I hope you enjoy this&nbsp;notebook. &nbsp;Please feel free to share and let me know your thoughts.My latest #R #notebook: Advanced Data Preparation in R https://t.co/7Dvc9nCPK0 #DataScience #dsx #ibmaot #IBM h/t @kabacoff pic.twitter.com/cNpP45vpoR— Venky Rao (@VRaoRao) September 2 2017
2017,8,28,Engine bleed air: a primer,http://themainstreamseer.blogspot.com/2017/08/engine-bleed-air-primer.html,"Use of Bleed Air in Aircraft Pneumatic Systems: A Primer(taken from Chapter 6 on Pneumatic Systems from the 3rd Edition of the book “Aircraft Systems” by Ian Moir and Allan Seabridge)The use of aircraft engines as a source of high pressure high temperature air can be understood by examining the characteristics of the turbofan engine.&nbsp; Modern engines “bypass” a significant portion of the mass flow past the engine and increasingly a small portion of the mass flow passes through the engine core or gas generation section.&nbsp; The ratio of bypass air to engine core air is called the bypass ratio and this can easily exceed 10:1 for the very latest civil engines; much higher than the 4 or 5:1 ratio for the previous generation.The characteristics of a modern turbofan engine are shown in figure 6.1.&nbsp; This shows the pressure (in psi) and the temperature (in degree centigrade) at various points throughout the engine for three conditions: ground idle take off power and in the cruise condition.It can be seen that in the least stressful condition – ground idle – the engine is in a state of equilibrium but that even at this low level the compressor air pressure is 50 psi and the temperature is 180 oC.&nbsp; At take-off conditions the compressed air soars to 410 psi / 540 oC.&nbsp; In the cruise condition the compressor air is at 150 psi / 400 oC.&nbsp; The engine is therefore a source of high pressure and high temperature air that can be “bled” for the engine to perform various functions around the aircraft.&nbsp; The fact that there are such considerable variations in air pressure and temperature for various engine conditions places an imposing control task upon the pneumatic system.&nbsp; Also the variations in engine characteristics between similarly rated engines of different manufacturers poses additional design constraints.&nbsp; Some aircraft such as the Boeing 777 offer three engine choices Pratt &amp; Whitney General Electric and Rolls-Royce and each of these engines has to be separately matched to the aircraft systems the loads of which may differ as a result of operator specified configurations.As well as the main aircraft engines the Auxiliary Power Unit (APU) is also a source of high pressure bleed air.&nbsp; The APU is in itself a small turbojet engine designed more from the viewpoint of an energy and power generator than a thrust provider which is the case for main engines.&nbsp; The APU is primarily designed to provide electrical and pneumatic power by a shaft driven generator and compressor.&nbsp; The APU is therefore able to provide an independent source of electrical power and compressed air while the aircraft is on the ground although it can be used as a backup provider of power while airborne.&nbsp; Some aircraft designs are actively considering the use of in-flight operable APUs to assist in in-flight engine re-lighting and to relieve the engines of offtake load in certain areas of the flight envelope.It is also usual for the aircraft to be designed to accept high pressure air from a ground power cart for aircraft engine starting.The three sources of pneumatic power provide the muscle or means by which the pneumatic is able to satisfy the aircraft demands.&nbsp; In a simplified form the pneumatic system may be represented by the interrelationships shown in figure 6.2 below:This simplified drawing – the ground air power source is omitted – shows how the aircraft High Pressure (HP) air sources provide bleed air which forms the primary source for the three major aircraft air related systems:·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Ice protection: the provision of hot air to provide anti icing of engine nacelles and the wing tailplane or fin leading edges; or to dislodge ice that has formed on the surfaces·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ECS and cooling: the provision of the main air source for environmental temperature control and cooling·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Pressurization: the provision of a means by which the aircraft may be pressurized giving the crew and passengers a more comfortable operating environment.A simplified representation of this relationship is shown in figure 6.3.&nbsp; This example shows a twin-engine configuration typical of many business jets and regional jet transport aircraft.Bleed air from the engines is passed through a Pressure-Reducing Shut-Off Valve (PRSOV) which serves the function of controlling and when required shutting off the engine bleed air supply.&nbsp; Air downstream of the PRSOV may be used in a number of ways:·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; By means of a cross flow Shut-Off Valve (SOV) the system may supply air to the opposite side of the aircraft during engine start or if the opposite engine is inoperative for any reason·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; A SOV from the APU may be used to isolate the APU air supply·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; SOVs provide isolation as appropriate to the left and right air conditioning packs and pressurization systems·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Additional SOVs provide the means by which the supply to left and right wing anti-icing systems may be shut off in the event that these functions are not requiredThis is a simplified model of the use of engine bleed air in pneumatic systems (ATA Chapter 36).&nbsp; A more comprehensive list of those aircraft systems with which bleed air is associated are listed as follows&nbsp; with the accompanying civil ATA chapter classification:·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Air conditioning (ATA Chapter 21)·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Cargo compartment heating (ATA Chapter 21)·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Wing and engine anti-icing (ATA Chapter 30)·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Engine start (ATA Chapter 80)·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Thrust reverser (ATA Chapter 78)·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Hydraulic reservoir pressurization (ATA Chapter 29)·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Rain repellent nozzles – aircraft windscreen (ATA Chapter 30)·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Water tank pressurization and toilet waste (ATA Chapter 38)·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Air driven hydraulic pump (ADP) (ATA Chapter 29)     96     Normal  0          false  false  false    EN-US  X-NONE  X-NONE                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   /* Style Definitions */ table.MsoNormalTable  {mso-style-name:""Table Normal"";  mso-tstyle-rowband-size:0;  mso-tstyle-colband-size:0;  mso-style-noshow:yes;  mso-style-priority:99;  mso-style-parent:"""";  mso-padding-alt:0in 5.4pt 0in 5.4pt;  mso-para-margin:0in;  mso-para-margin-bottom:.0001pt;  mso-pagination:widow-orphan;  font-size:12.0pt;  font-family:Calibri;  mso-ascii-font-family:Calibri;  mso-ascii-theme-font:minor-latin;  mso-hansi-font-family:Calibri;  mso-hansi-theme-font:minor-latin;}                                                                                                       "
2017,8,24,Data Preparation in R,http://themainstreamseer.blogspot.com/2017/08/data-preparation-in-r.html,My latest publicly available R notebook created in IBM's Data Science Experience is here! &nbsp;This notebook focuses on the basics of one of the most important aspects of Data Science: Data Preparation!I hope you enjoy this notebook. &nbsp;Please feel free to share and let me know your thoughts.My latest #R #notebook: Data Preparation in R https://t.co/5yXpG5DHFY #DataScience #dsx #ibmaot #IBM h/t @kabacoff pic.twitter.com/42j6hMRFaF— Venky Rao (@VRaoRao) August 24 2017 
2021,6,25,Gradient Flow Snapshot #60: Self-supervised Learning; SaaS CTO Security Checklist; 2021 NLP Survey,http://practicalquant.blogspot.com/2021/06/gradient-flow-snapshot-60-self.html,Subscribe to our Newsletter YouTube channel and to the Data Exchange podcast by going&nbsp;HERE.
2021,6,24,Training and Sharing Large Language Models,http://practicalquant.blogspot.com/2021/06/training-and-sharing-large-language.html,The Data Exchange Podcast: Connor Leahy on building models and datasets for the natural language research community.Subscribe: Apple • Android • Spotify • Stitcher • Google • RSS. Take the 2021 NLP Industry Survey and get a free pass to the 2021 NLP Summit. Full show notes can be found&nbsp;on the Data Exchange web site.A video version of this conversation is available&nbsp;on YouTube.
2021,6,18,Gradient Flow Snapshot #59: From Cloud → Sky computing; Automation in DataOps; Top Technology Trends,http://practicalquant.blogspot.com/2021/06/gradient-flow-snapshot-59-from-cloud.html,Subscribe to our Newsletter YouTube channel and to the Data Exchange podcast by going&nbsp;HERE.
2021,6,17,Questioning the Efficacy of Neural Recommendation Systems,http://practicalquant.blogspot.com/2021/06/questioning-efficacy-of-neural.html,The Data Exchange Podcast: Paolo Cremonesi and Maurizio Ferrari Dacrema on the reproducibility complexity and inefficiency of neural methods for recommenders.Subscribe: Apple • Android • Spotify • Stitcher • Google • RSS.    Full show notes can be found on the Data Exchange web site. A video version of this conversation is available on YouTube.
2021,6,11,Gradient Flow Snapshot #58: Delta Live Tables; Knowledge Graphs; Data Portability,http://practicalquant.blogspot.com/2021/06/gradient-flow-snapshot-58-delta-live.html,Subscribe to our Newsletter YouTube channel and to the Data Exchange podcast by going&nbsp;HERE.
2021,6,10,Automation in Data Management and Data Labeling,http://practicalquant.blogspot.com/2021/06/automation-in-data-management-and-data.html,The Data Exchange Podcast: Hyun Kim on building automation and DataOps tools to help companies unlock computer vision data.Subscribe: Apple • Android • Spotify • Stitcher • Google • RSS.    Full show notes can be found&nbsp;on the Data Exchange web site.A video version of this conversation is available&nbsp;on YouTube.
2021,6,4,Gradient Flow Snapshot #57: Monitoring Machine Learning Models; Greykite for Time-series Forecasting,http://practicalquant.blogspot.com/2021/06/gradient-flow-snapshot-57-monitoring.html, Subscribe to our Newsletter YouTube channel and to the Data Exchange podcast by going&nbsp;HERE.
2021,6,3,Reinforcement Learning For the Win,http://practicalquant.blogspot.com/2021/06/reinforcement-learning-for-win.html,The Data Exchange Podcast: Nicolas Hohn on challenges and best practices for using RL and machine learning in the enterprise.Subscribe: Apple • Android • Spotify • Stitcher • Google • RSS. Nic Hohn is part of an outstanding speaker lineup at the 2021 Ray Summit a FREE virtual conference that brings together developers machine learning practitioners data scientists DevOps and cloud-native architects interested in building scalable data &amp; AI applications.Full show notes can be found&nbsp;on the Data Exchange web site.A video version of this conversation is available&nbsp;on YouTube.
2021,5,28,Gradient Flow Snapshot #56: Airflow + Ray; Data Warehouse → Lakehouse; CSV file → Knowledge Graph,http://practicalquant.blogspot.com/2021/05/gradient-flow-snapshot-56-airflow-ray.html,Subscribe to our Newsletter YouTube channel and to the Data Exchange podcast by going&nbsp;HERE.
2021,5,27,How Companies Are Investing in AI Risk and Liability Minimization,http://practicalquant.blogspot.com/2021/05/how-companies-are-investing-in-ai-risk.html,The Data Exchange Podcast: Andrew Burt on the state of AI risk mitigation and responsible AI.Subscribe: Apple • Android • Spotify • Stitcher • Google • RSS.  Andrew Burt spoke at our very popular virtual event: Responsible AI in Practice. Click HERE to watch it on-demand.  Full show notes can be found on the Data Exchange web site. A video version of this conversation is available on YouTube. 
2021,5,21,"Gradient Flow Snapshot #55: Reinforcement Learning in the Enterprise, Knowledge Graphs in Finance",http://practicalquant.blogspot.com/2021/05/gradient-flow-snapshot-55-reinforcement.html,Subscribe to our Newsletter YouTube channel and to the Data Exchange podcast by going HERE.
2021,5,20,The Future of Machine Learning Lies in Better Abstractions,http://practicalquant.blogspot.com/2021/05/the-future-of-machine-learning-lies-in.html,The Data Exchange Podcast: Travis Addair on how higher levels of abstractions enable non-experts to build efficient machine learning models.Subscribe: Apple • Android • Spotify • Stitcher • Google • RSS.  Travis Addair is part of an outstanding speaker lineup at the 2021 Ray Summit a FREE virtual conference that brings together developers machine learning practitioners data scientists DevOps and cloud-native architects interested in building scalable data &amp; AI applications.Full show notes can be found on the Data Exchange web site. A video version of this conversation is available on YouTube. 
2021,5,14,Gradient Flow Snapshot #54: NLP Index and Getting Read for New AI Regulations,http://practicalquant.blogspot.com/2021/05/gradient-flow-snapshot-54-nlp-index-and.html, Subscribe to our Newsletter YouTube channel and to the Data Exchange podcast by going HERE.
2021,5,13,Why You Should Optimize Your Deep Learning Inference Platform,http://practicalquant.blogspot.com/2021/05/why-you-should-optimize-your-deep.html,The Data Exchange Podcast: Yonatan Geifman and Ran El-Yaniv on the benefits that accrue from using an inference acceleration platform.Subscribe: Apple • Android • Spotify • Stitcher • Google • RSS.   Full show notes can be found on the Data Exchange web site. A video version of this conversation is available on YouTube. 
2021,5,7,Gradient Flow Snapshot #53: Data Validation for Machine Learning; Modernizing Data Governance,http://practicalquant.blogspot.com/2021/05/gradient-flow-snapshot-53-data.html,Subscribe to our Newsletter YouTube channel and to the Data Exchange podcast by going HERE.
2021,5,6,AI Beyond Automation,http://practicalquant.blogspot.com/2021/05/ai-beyond-automation.html,The Data Exchange Podcast: Jerry Overton on building an Artificial Intelligence Center of Excellence to incubate serious AI talent.Subscribe: Apple • Android • Spotify • Stitcher • Google • RSS  Download the 2021 Business At The Speed Of AI Report and learn how leading companies are using and implementing data and machine learning technologies. Full show notes can be found on the Data Exchange web site. A video version of this conversation is available on YouTube.  
2021,4,30,Gradient Flow Snapshot #52: Data Integration; Language Benchmarks; Online Resource Allocation with Ray,http://practicalquant.blogspot.com/2021/04/gradient-flow-snapshot-52-data.html, Subscribe to our Newsletter YouTube channel and to the Data Exchange podcast by going HERE.
2021,4,29,Injecting Software Engineering Practices and Rigor into Data Governance,http://practicalquant.blogspot.com/2021/04/injecting-software-engineering.html,The Data Exchange Podcast: Steve Touw on why data governance needs to go from the boardroom into code.Subscribe: Apple • Android • Spotify • Stitcher • Google • RSS.   Full show notes can be found on the Data Exchange web site. A video version of this conversation is available on YouTube.
