{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing necessary library for RSS Feed Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import feedparser\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Apply the fucntions to extract RSS Feed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url_reader = open(\"FeedUrl.txt\",\"r\")\n",
    "url_links = url_reader.read().strip().split()\n",
    "\n",
    "entire_data = open(\"entire_data.txt\",\"a\")\n",
    "\n",
    "for i,url in enumerate(url_links):\n",
    "    \n",
    "    url_text_reader = open(\"{}\".format(i+1),\"r+\")\n",
    "    \n",
    "    url_title = url_text_reader.read().strip().split()\n",
    "    \n",
    "    feed = feedparser.parse(url)\n",
    "    \n",
    "    for entry in feed.entries:\n",
    "        \n",
    "        Title = re.sub(r\",\",\"\",entry.title)\n",
    "        \n",
    "        if Title in url_title:\n",
    "            continue                                # check for duplicates\n",
    "        \n",
    "        Year = str(entry.published_parsed.tm_year)\n",
    "        Month = str(entry.published_parsed.tm_mon)\n",
    "        Day = str(entry.published_parsed.tm_mday)\n",
    "        summary = re.sub(r\",\",\"\",entry.summary)     # Removing commas from the text\n",
    "        summary = re.sub(r\"<[^>]*>\",\"\",summary)     # Removing tags \n",
    "        Summary = re.sub(r\"\\n\",\" \",summary)         # Removing New line characters\n",
    "        \n",
    "        url_text_reader.write(Title+\"\\n\")\n",
    "        entire_data.write(Year+\",\"+Month+\",\"+Day+\",\"+Title+\",\"+Summary+\"\\n\")\n",
    "        \n",
    "        title_count += 1\n",
    "        \n",
    "    url_text_reader.close()\n",
    "\n",
    "url_reader.close()\n",
    "entire_data.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main_df = pd.read_csv(\"entire_data.txt\",sep=\",\",header=None)\n",
    "main_df.columns=[\"Year\",\"Month\",\"Day\",\"Title\",\"Summary\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Removing the columns with empty data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Year        0\n",
       "Month       0\n",
       "Day         0\n",
       "Title       2\n",
       "Summary    39\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Title</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1276</th>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>NIPS 2016 Reflections</td>\n",
       "      <td>It was a great conference.  The organizers had...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>2017</td>\n",
       "      <td>8</td>\n",
       "      <td>25</td>\n",
       "      <td>Biggish time series data</td>\n",
       "      <td>Informal presentation for a UNSW research grou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>2018</td>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "      <td>Amazon RDS Oracle Instance Running Out of Disc...</td>\n",
       "      <td>pre{border: 2px solid #666; padding: 10px; bac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>2018</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>IJF Tao Hong Award 2018</td>\n",
       "      <td>Every two years the International Journal of F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705</th>\n",
       "      <td>2015</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>Feeling the FPP love</td>\n",
       "      <td>It is now exactly 12 months since the print ve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Year  Month  Day                                              Title  \\\n",
       "1276  2016     12   12                              NIPS 2016 Reflections   \n",
       "534   2017      8   25                           Biggish time series data   \n",
       "353   2018     12   16  Amazon RDS Oracle Instance Running Out of Disc...   \n",
       "513   2018      3   13                            IJF Tao Hong Award 2018   \n",
       "705   2015      4   10                               Feeling the FPP love   \n",
       "\n",
       "                                                Summary  \n",
       "1276  It was a great conference.  The organizers had...  \n",
       "534   Informal presentation for a UNSW research grou...  \n",
       "353   pre{border: 2px solid #666; padding: 10px; bac...  \n",
       "513   Every two years the International Journal of F...  \n",
       "705   It is now exactly 12 months since the print ve...  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Main_DF = main_df.dropna()\n",
    "Main_DF.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Joining Title and summary for KeyPharse extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-106-dd53172b9cd0>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Main_DF[\"Text\"] = Main_DF[\"Title\"]+\" \"+ Main_DF[\"Summary\"]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Title</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>2020</td>\n",
       "      <td>10</td>\n",
       "      <td>26</td>\n",
       "      <td>Call for papers: Innovations in hierarchical f...</td>\n",
       "      <td>There is a new call for papers for a special i...</td>\n",
       "      <td>Call for papers: Innovations in hierarchical f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>2020</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>Machine Learning Week 2021 Call for Speakers</td>\n",
       "      <td>Copyright © 2021 https://jtonedm.com James Tay...</td>\n",
       "      <td>Machine Learning Week 2021 Call for Speakers C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>2018</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "      <td>Upcoming talks: May-July 2018</td>\n",
       "      <td>First semester teaching is nearly finished and...</td>\n",
       "      <td>Upcoming talks: May-July 2018 First semester t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>2020</td>\n",
       "      <td>10</td>\n",
       "      <td>21</td>\n",
       "      <td>Model selection in reconciling hierarchical ti...</td>\n",
       "      <td>Model selection has been proven an effective s...</td>\n",
       "      <td>Model selection in reconciling hierarchical ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>2017</td>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>Converting to blogdown</td>\n",
       "      <td>This website has gone through several major up...</td>\n",
       "      <td>Converting to blogdown This website has gone t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Year  Month  Day                                              Title  \\\n",
       "411   2020     10   26  Call for papers: Innovations in hierarchical f...   \n",
       "1200  2020     10    6       Machine Learning Week 2021 Call for Speakers   \n",
       "507   2018      4   23                      Upcoming talks: May-July 2018   \n",
       "412   2020     10   21  Model selection in reconciling hierarchical ti...   \n",
       "554   2017      4   30                             Converting to blogdown   \n",
       "\n",
       "                                                Summary  \\\n",
       "411   There is a new call for papers for a special i...   \n",
       "1200  Copyright © 2021 https://jtonedm.com James Tay...   \n",
       "507   First semester teaching is nearly finished and...   \n",
       "412   Model selection has been proven an effective s...   \n",
       "554   This website has gone through several major up...   \n",
       "\n",
       "                                                   Text  \n",
       "411   Call for papers: Innovations in hierarchical f...  \n",
       "1200  Machine Learning Week 2021 Call for Speakers C...  \n",
       "507   Upcoming talks: May-July 2018 First semester t...  \n",
       "412   Model selection in reconciling hierarchical ti...  \n",
       "554   Converting to blogdown This website has gone t...  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Main_DF[\"Text\"] = Main_DF[\"Title\"]+\" \"+ Main_DF[\"Summary\"]\n",
    "Main_DF.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import packages for Key-Phrase extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import wordnet\n",
    "from yake import yake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Making Lemmatization as custom fuction as it is often used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_not_numeric(x):\n",
    "    try:\n",
    "        float(x)\n",
    "        return False\n",
    "    except:\n",
    "        return True\n",
    "    pass\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\")).union(\"nbsp\",\"ldquo\",\"nuitblog\",\"linkedin\",\"post\",\"event\",\"amp\",\"lot\",\"working\",\"quot\",\"paper\",\"article\",\"job\",\"demand\")\n",
    "lemmar = wordnet.WordNetLemmatizer()\n",
    "\n",
    "def lemmatizer(string_line):\n",
    "\n",
    "    word = word_tokenize(string_line.lower())\n",
    "    words = [x for x in word if (x not in stop_words and len(x) > 1 and is_not_numeric(x))]\n",
    "\n",
    "    lemma_word = \"\"\n",
    "\n",
    "    for word in words:\n",
    "        lemma = lemmar.lemmatize(word)\n",
    "        lemma_word += lemma + \" \"\n",
    "    \n",
    "    return lemma_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Applying Lemmatixation for Title and Summary together i.e. Text column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-109-bbdb38c90760>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Main_DF[\"Lemmatized\"] = Main_DF[\"Text\"].apply(lambda x : lemmatizer(x))\n"
     ]
    }
   ],
   "source": [
    "Main_DF[\"Lemmatized\"] = Main_DF[\"Text\"].apply(lambda x : lemmatizer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Title</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>Lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>2010</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>My standard LaTeX preamble</td>\n",
       "      <td>When I was a PhD student I found I needed a lo...</td>\n",
       "      <td>My standard LaTeX preamble When I was a PhD st...</td>\n",
       "      <td>standard latex preamble phd student found need...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1298</th>\n",
       "      <td>2021</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>The $1000 GPT-3</td>\n",
       "      <td>**&amp;nbsp;Nuit Blanche is now on Twitter: @NuitB...</td>\n",
       "      <td>The $1000 GPT-3 **&amp;nbsp;Nuit Blanche is now on...</td>\n",
       "      <td>gpt-3 nbsp nuit blanche twitter nuitblog nbsp ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>917</th>\n",
       "      <td>2011</td>\n",
       "      <td>5</td>\n",
       "      <td>29</td>\n",
       "      <td>Comparing HoltWinters() and ets()</td>\n",
       "      <td>I received this email today:  I have a questio...</td>\n",
       "      <td>Comparing HoltWinters() and ets() I received t...</td>\n",
       "      <td>comparing holtwinters ets received email today...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>2021</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>How to write a research grant proposal?</td>\n",
       "      <td>Today I will discuss how to write a good resea...</td>\n",
       "      <td>How to write a research grant proposal? Today ...</td>\n",
       "      <td>write research grant proposal today discus wri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>2015</td>\n",
       "      <td>5</td>\n",
       "      <td>31</td>\n",
       "      <td>A new R package for detecting unusual time series</td>\n",
       "      <td>The anomalous package provides some tools to d...</td>\n",
       "      <td>A new R package for detecting unusual time ser...</td>\n",
       "      <td>new package detecting unusual time series anom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>930</th>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>Six places left for the forecasting workshop</td>\n",
       "      <td>There are six places left for the forecasting ...</td>\n",
       "      <td>Six places left for the forecasting workshop T...</td>\n",
       "      <td>six place left forecasting workshop six place ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>FFORMA: Feature-based Forecast Model Averaging</td>\n",
       "      <td>We propose an automated method for obtaining w...</td>\n",
       "      <td>FFORMA: Feature-based Forecast Model Averaging...</td>\n",
       "      <td>fforma feature-based forecast model averaging ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1270</th>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>Software Engineering vs Machine Learning Concepts</td>\n",
       "      <td>Not all core concepts from software engineerin...</td>\n",
       "      <td>Software Engineering vs Machine Learning Conce...</td>\n",
       "      <td>software engineering v machine learning concep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>909</th>\n",
       "      <td>2011</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>Beware of junk journals and publishers</td>\n",
       "      <td>Today I received the following email:  Dear Pr...</td>\n",
       "      <td>Beware of junk journals and publishers Today I...</td>\n",
       "      <td>beware junk journal publisher today received f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1313</th>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>Au Revoir Backprop ! Bonjour Optical Transfer ...</td>\n",
       "      <td>**&amp;nbsp;Nuit Blanche is now on Twitter: @NuitB...</td>\n",
       "      <td>Au Revoir Backprop ! Bonjour Optical Transfer ...</td>\n",
       "      <td>au revoir backprop bonjour optical transfer le...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Year  Month  Day                                              Title  \\\n",
       "991   2010      3   21                         My standard LaTeX preamble   \n",
       "1298  2021      4    6                                    The $1000 GPT-3   \n",
       "917   2011      5   29                  Comparing HoltWinters() and ets()   \n",
       "259   2021      3    9            How to write a research grant proposal?   \n",
       "696   2015      5   31  A new R package for detecting unusual time series   \n",
       "930   2011      1   11       Six places left for the forecasting workshop   \n",
       "444   2020      1    3     FFORMA: Feature-based Forecast Model Averaging   \n",
       "1270  2017      2   16  Software Engineering vs Machine Learning Concepts   \n",
       "909   2011      8   12             Beware of junk journals and publishers   \n",
       "1313  2020      3   14  Au Revoir Backprop ! Bonjour Optical Transfer ...   \n",
       "\n",
       "                                                Summary  \\\n",
       "991   When I was a PhD student I found I needed a lo...   \n",
       "1298  **&nbsp;Nuit Blanche is now on Twitter: @NuitB...   \n",
       "917   I received this email today:  I have a questio...   \n",
       "259   Today I will discuss how to write a good resea...   \n",
       "696   The anomalous package provides some tools to d...   \n",
       "930   There are six places left for the forecasting ...   \n",
       "444   We propose an automated method for obtaining w...   \n",
       "1270  Not all core concepts from software engineerin...   \n",
       "909   Today I received the following email:  Dear Pr...   \n",
       "1313  **&nbsp;Nuit Blanche is now on Twitter: @NuitB...   \n",
       "\n",
       "                                                   Text  \\\n",
       "991   My standard LaTeX preamble When I was a PhD st...   \n",
       "1298  The $1000 GPT-3 **&nbsp;Nuit Blanche is now on...   \n",
       "917   Comparing HoltWinters() and ets() I received t...   \n",
       "259   How to write a research grant proposal? Today ...   \n",
       "696   A new R package for detecting unusual time ser...   \n",
       "930   Six places left for the forecasting workshop T...   \n",
       "444   FFORMA: Feature-based Forecast Model Averaging...   \n",
       "1270  Software Engineering vs Machine Learning Conce...   \n",
       "909   Beware of junk journals and publishers Today I...   \n",
       "1313  Au Revoir Backprop ! Bonjour Optical Transfer ...   \n",
       "\n",
       "                                             Lemmatized  \n",
       "991   standard latex preamble phd student found need...  \n",
       "1298  gpt-3 nbsp nuit blanche twitter nuitblog nbsp ...  \n",
       "917   comparing holtwinters ets received email today...  \n",
       "259   write research grant proposal today discus wri...  \n",
       "696   new package detecting unusual time series anom...  \n",
       "930   six place left forecasting workshop six place ...  \n",
       "444   fforma feature-based forecast model averaging ...  \n",
       "1270  software engineering v machine learning concep...  \n",
       "909   beware junk journal publisher today received f...  \n",
       "1313  au revoir backprop bonjour optical transfer le...  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Main_DF.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "Main_DF.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Title</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>Lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021</td>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "      <td>Text Preprocessing in NLP with Python codes</td>\n",
       "      <td>ArticleVideo Book This article was published a...</td>\n",
       "      <td>Text Preprocessing in NLP with Python codes Ar...</td>\n",
       "      <td>text preprocessing nlp python code articlevide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021</td>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "      <td>Part 14: Step by Step Guide to Master NLP – Ba...</td>\n",
       "      <td>ArticleVideo Book This article was published a...</td>\n",
       "      <td>Part 14: Step by Step Guide to Master NLP – Ba...</td>\n",
       "      <td>part step step guide master nlp basic topic mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021</td>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "      <td>Part 13: Step by Step Guide to Master NLP – Re...</td>\n",
       "      <td>ArticleVideo Book This article was published a...</td>\n",
       "      <td>Part 13: Step by Step Guide to Master NLP – Re...</td>\n",
       "      <td>part step step guide master nlp regular expres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021</td>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "      <td>Decide Best Learning Rate with LearningRateSch...</td>\n",
       "      <td>ArticleVideo Book This article was published a...</td>\n",
       "      <td>Decide Best Learning Rate with LearningRateSch...</td>\n",
       "      <td>decide best learning rate learningrateschedule...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021</td>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "      <td>Generate Reports Using Pandas Profiling Deploy...</td>\n",
       "      <td>ArticleVideo Book This article was published a...</td>\n",
       "      <td>Generate Reports Using Pandas Profiling Deploy...</td>\n",
       "      <td>generate report using panda profiling deploy u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1460</th>\n",
       "      <td>2021</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>FREE Report: 2021 Business at the Speed of AI ...</td>\n",
       "      <td>DOWNLOAD</td>\n",
       "      <td>FREE Report: 2021 Business at the Speed of AI ...</td>\n",
       "      <td>free report business speed ai report download</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1461</th>\n",
       "      <td>2021</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>Gradient Flow Snapshot #50: Data Engineering j...</td>\n",
       "      <td>Subscribe to our Newsletter YouTube channel a...</td>\n",
       "      <td>Gradient Flow Snapshot #50: Data Engineering j...</td>\n",
       "      <td>gradient flow snapshot data engineering job u....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1462</th>\n",
       "      <td>2021</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>How Technology Companies Are Using Ray</td>\n",
       "      <td>The Data Exchange Podcast: Zhe Zhang describes...</td>\n",
       "      <td>How Technology Companies Are Using Ray The Dat...</td>\n",
       "      <td>technology company using ray data exchange pod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1463</th>\n",
       "      <td>2021</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>FREE Report: 2020 NLP Industry Survey Report</td>\n",
       "      <td>DOWNLOAD</td>\n",
       "      <td>FREE Report: 2020 NLP Industry Survey Report D...</td>\n",
       "      <td>free report nlp industry survey report download</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1464</th>\n",
       "      <td>2021</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>Gradient Flow Snapshot #49: Data Cascades; Exp...</td>\n",
       "      <td>Subscribe to our Newsletter YouTube channel a...</td>\n",
       "      <td>Gradient Flow Snapshot #49: Data Cascades; Exp...</td>\n",
       "      <td>gradient flow snapshot data cascade exploiting...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1465 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Year  Month  Day                                              Title  \\\n",
       "0     2021      6   25        Text Preprocessing in NLP with Python codes   \n",
       "1     2021      6   25  Part 14: Step by Step Guide to Master NLP – Ba...   \n",
       "2     2021      6   25  Part 13: Step by Step Guide to Master NLP – Re...   \n",
       "3     2021      6   25  Decide Best Learning Rate with LearningRateSch...   \n",
       "4     2021      6   25  Generate Reports Using Pandas Profiling Deploy...   \n",
       "...    ...    ...  ...                                                ...   \n",
       "1460  2021      4   20  FREE Report: 2021 Business at the Speed of AI ...   \n",
       "1461  2021      4   16  Gradient Flow Snapshot #50: Data Engineering j...   \n",
       "1462  2021      4   15             How Technology Companies Are Using Ray   \n",
       "1463  2021      4   13       FREE Report: 2020 NLP Industry Survey Report   \n",
       "1464  2021      4    9  Gradient Flow Snapshot #49: Data Cascades; Exp...   \n",
       "\n",
       "                                                Summary  \\\n",
       "0     ArticleVideo Book This article was published a...   \n",
       "1     ArticleVideo Book This article was published a...   \n",
       "2     ArticleVideo Book This article was published a...   \n",
       "3     ArticleVideo Book This article was published a...   \n",
       "4     ArticleVideo Book This article was published a...   \n",
       "...                                                 ...   \n",
       "1460                                           DOWNLOAD   \n",
       "1461   Subscribe to our Newsletter YouTube channel a...   \n",
       "1462  The Data Exchange Podcast: Zhe Zhang describes...   \n",
       "1463                                           DOWNLOAD   \n",
       "1464   Subscribe to our Newsletter YouTube channel a...   \n",
       "\n",
       "                                                   Text  \\\n",
       "0     Text Preprocessing in NLP with Python codes Ar...   \n",
       "1     Part 14: Step by Step Guide to Master NLP – Ba...   \n",
       "2     Part 13: Step by Step Guide to Master NLP – Re...   \n",
       "3     Decide Best Learning Rate with LearningRateSch...   \n",
       "4     Generate Reports Using Pandas Profiling Deploy...   \n",
       "...                                                 ...   \n",
       "1460  FREE Report: 2021 Business at the Speed of AI ...   \n",
       "1461  Gradient Flow Snapshot #50: Data Engineering j...   \n",
       "1462  How Technology Companies Are Using Ray The Dat...   \n",
       "1463  FREE Report: 2020 NLP Industry Survey Report D...   \n",
       "1464  Gradient Flow Snapshot #49: Data Cascades; Exp...   \n",
       "\n",
       "                                             Lemmatized  \n",
       "0     text preprocessing nlp python code articlevide...  \n",
       "1     part step step guide master nlp basic topic mo...  \n",
       "2     part step step guide master nlp regular expres...  \n",
       "3     decide best learning rate learningrateschedule...  \n",
       "4     generate report using panda profiling deploy u...  \n",
       "...                                                 ...  \n",
       "1460     free report business speed ai report download   \n",
       "1461  gradient flow snapshot data engineering job u....  \n",
       "1462  technology company using ray data exchange pod...  \n",
       "1463   free report nlp industry survey report download   \n",
       "1464  gradient flow snapshot data cascade exploiting...  \n",
       "\n",
       "[1465 rows x 7 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Main_DF.drop(columns=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Title</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>Lemmatized</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006</th>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>119</td>\n",
       "      <td>119</td>\n",
       "      <td>119</td>\n",
       "      <td>119</td>\n",
       "      <td>119</td>\n",
       "      <td>119</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>79</td>\n",
       "      <td>79</td>\n",
       "      <td>79</td>\n",
       "      <td>79</td>\n",
       "      <td>79</td>\n",
       "      <td>79</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>126</td>\n",
       "      <td>126</td>\n",
       "      <td>126</td>\n",
       "      <td>126</td>\n",
       "      <td>126</td>\n",
       "      <td>126</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>140</td>\n",
       "      <td>140</td>\n",
       "      <td>140</td>\n",
       "      <td>140</td>\n",
       "      <td>140</td>\n",
       "      <td>140</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>103</td>\n",
       "      <td>103</td>\n",
       "      <td>103</td>\n",
       "      <td>103</td>\n",
       "      <td>103</td>\n",
       "      <td>103</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020</th>\n",
       "      <td>153</td>\n",
       "      <td>153</td>\n",
       "      <td>153</td>\n",
       "      <td>153</td>\n",
       "      <td>153</td>\n",
       "      <td>153</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021</th>\n",
       "      <td>319</td>\n",
       "      <td>319</td>\n",
       "      <td>319</td>\n",
       "      <td>319</td>\n",
       "      <td>319</td>\n",
       "      <td>319</td>\n",
       "      <td>319</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      index  Month  Day  Title  Summary  Text  Lemmatized\n",
       "Year                                                     \n",
       "1         6      6    6      6        6     6           6\n",
       "1991      1      1    1      1        1     1           1\n",
       "1992      2      2    2      2        2     2           2\n",
       "1993      1      1    1      1        1     1           1\n",
       "1994      1      1    1      1        1     1           1\n",
       "1995      2      2    2      2        2     2           2\n",
       "1996      4      4    4      4        4     4           4\n",
       "1997      3      3    3      3        3     3           3\n",
       "1998      1      1    1      1        1     1           1\n",
       "1999      1      1    1      1        1     1           1\n",
       "2000      4      4    4      4        4     4           4\n",
       "2001      5      5    5      5        5     5           5\n",
       "2002      4      4    4      4        4     4           4\n",
       "2003      4      4    4      4        4     4           4\n",
       "2004      4      4    4      4        4     4           4\n",
       "2005      8      8    8      8        8     8           8\n",
       "2006     14     14   14     14       14    14          14\n",
       "2007     16     16   16     16       16    16          16\n",
       "2008     30     30   30     30       30    30          30\n",
       "2009     49     49   49     49       49    49          49\n",
       "2010     65     65   65     65       65    65          65\n",
       "2011     44     44   44     44       44    44          44\n",
       "2012     38     38   38     38       38    38          38\n",
       "2013     48     48   48     48       48    48          48\n",
       "2014    119    119  119    119      119   119         119\n",
       "2015     79     79   79     79       79    79          79\n",
       "2016    126    126  126    126      126   126         126\n",
       "2017    140    140  140    140      140   140         140\n",
       "2018     71     71   71     71       71    71          71\n",
       "2019    103    103  103    103      103   103         103\n",
       "2020    153    153  153    153      153   153         153\n",
       "2021    319    319  319    319      319   319         319"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Main_DF.groupby(\"Year\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Key-Phrase Extraction for Text column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('nbsp nbsp', 1.765922954459206e-05)\n",
      "('time series', 3.557459012742076e-05)\n",
      "('data', 5.007585605936319e-05)\n",
      "('data science', 6.20800018302835e-05)\n",
      "('nbsp', 7.030222167434287e-05)\n",
      "('machine learning', 7.42260400834347e-05)\n",
      "('time', 0.00015257647245884033)\n",
      "('data analysis', 0.0001772436598306144)\n",
      "('learning', 0.0001955913271537126)\n",
      "('forecasting', 0.0002083998877496126)\n",
      "('model', 0.00021070380558534875)\n",
      "('series', 0.00024313994114023513)\n",
      "('series data', 0.00024478303786162935)\n",
      "('data scientist', 0.0002798799764738667)\n",
      "('series forecasting', 0.0003178309672140404)\n",
      "('quot quot', 0.00037525489736537964)\n",
      "('learning nbsp', 0.00038509694673104856)\n",
      "('forecast', 0.00042972288920793407)\n",
      "('paper', 0.00044164457048107415)\n",
      "('research', 0.00045526290993114926)\n",
      "('science', 0.0004574871768061947)\n",
      "('machine', 0.00048667082876349335)\n",
      "('deep learning', 0.000520589089552624)\n",
      "('quot', 0.0005398122023998585)\n",
      "('post', 0.0005611069101984513)\n",
      "('method', 0.0005737453883019756)\n",
      "('data time', 0.0005772749748366833)\n",
      "('year', 0.0005806964007536408)\n",
      "('data analytics', 0.0005967437827679416)\n",
      "('group nbsp', 0.0006048066988385701)\n",
      "('data set', 0.0006093072937211493)\n",
      "('rsquo', 0.0007108572145991791)\n",
      "('nbsp lighton', 0.0007227196102175281)\n",
      "('forecasting method', 0.000737028420515302)\n",
      "('learning data', 0.000768539631048973)\n",
      "('analysis', 0.0007806380188812037)\n",
      "('learning model', 0.0007903518676672511)\n",
      "('analytics', 0.0007930400661177507)\n",
      "('problem', 0.0008010923890120634)\n",
      "('work', 0.0009066215206781265)\n",
      "('neural network', 0.0009874161542603038)\n",
      "('appeared', 0.0010179774857644451)\n",
      "('time data', 0.0010495908633394242)\n",
      "('series analysis', 0.001065166279686401)\n",
      "('data model', 0.0010852758002010817)\n",
      "('data data', 0.0010998386259561087)\n",
      "('big data', 0.0011136804937287312)\n",
      "('nuitblog nbsp', 0.0011328022329057123)\n",
      "('package', 0.0011418724814743657)\n",
      "('functional data', 0.0011470402525027555)\n",
      "('series forecast', 0.0011538768924325739)\n",
      "('forecasting time', 0.0011556228779600057)\n",
      "('data visualization', 0.0011639146043872766)\n",
      "('ldquo', 0.001169712418936836)\n",
      "('forecasting model', 0.0011783050227535837)\n",
      "('series model', 0.0011801184048563186)\n",
      "('journal forecasting', 0.0012178368342660932)\n",
      "('forecast package', 0.0012316909163153605)\n",
      "('rdquo', 0.0012322009856953545)\n",
      "('data mining', 0.0013263605971910331)\n",
      "('make', 0.001326681214258024)\n",
      "('nbsp data', 0.0013780455017153778)\n",
      "('nbsp linkedin', 0.001380237307571715)\n",
      "('linkedin nbsp', 0.001380237307571715)\n",
      "('model data', 0.0014108585402614062)\n",
      "('lighton nbsp', 0.0014454392204350562)\n",
      "('forecasting competition', 0.0015523141010512938)\n",
      "('approach', 0.001560943743477258)\n",
      "('nbsp facebook', 0.001574966746981752)\n",
      "('function', 0.0015813117926704349)\n",
      "('network', 0.0015932404178976263)\n",
      "('talk', 0.001600454979790624)\n",
      "('predictive analytics', 0.0017009661485642975)\n",
      "('post data', 0.0017281784234356409)\n",
      "('business', 0.0017501012363340767)\n",
      "('data exchange', 0.0017560562640342972)\n",
      "('model time', 0.001854348922865341)\n",
      "('workshop', 0.0018595456175576216)\n",
      "('blog post', 0.0018616563370371637)\n",
      "('nbsp join', 0.0018723753341237468)\n",
      "('system', 0.0018949288039724996)\n",
      "('university', 0.0019002059304319518)\n",
      "('statistical', 0.001911505997205148)\n",
      "('number', 0.001964125570198139)\n",
      "('set', 0.001990987985026457)\n",
      "('nbsp nuit', 0.0019950661754030963)\n",
      "('people', 0.0020030338419013733)\n",
      "('good', 0.0020290650030028823)\n",
      "('nbsp advanced', 0.0020483368654587945)\n",
      "('learning algorithm', 0.002064159292020812)\n",
      "('question', 0.0020800238526850075)\n",
      "('nbsp newsletter', 0.002108730148915274)\n",
      "('reinforcement learning', 0.002114025496422346)\n",
      "('result', 0.002117045381702593)\n",
      "('analytics data', 0.002121755672063792)\n",
      "('functional time', 0.0021278158508925738)\n",
      "('business analytics', 0.002129347259009998)\n",
      "('scientist', 0.0021518032865397345)\n",
      "('book', 0.002151868442600103)\n",
      "('blog', 0.0021946173802541782)\n",
      "('conference', 0.0021973360381645885)\n",
      "('year year', 0.0022007628844125894)\n",
      "('statistic', 0.0022017289547721416)\n",
      "('algorithm', 0.002226466252847439)\n",
      "('tool', 0.002302004866087741)\n",
      "('analysis data', 0.0023238613177791667)\n",
      "('neural', 0.0023717276396735382)\n",
      "('journal', 0.00237980084813128)\n",
      "('large', 0.002387488728239287)\n",
      "('group', 0.002422353235516647)\n",
      "('deep', 0.0024492287229445214)\n",
      "('world', 0.0024640557821061747)\n",
      "('reddit nbsp', 0.0024866344593263774)\n",
      "('data management', 0.0025039336696042994)\n",
      "('university nbsp', 0.002541570380645262)\n",
      "('quality data', 0.0025663332231380437)\n",
      "('nbsp meetup.com', 0.0025737723994145147)\n",
      "('company', 0.0025838456207737544)\n",
      "('analytics world', 0.002594701632586044)\n",
      "('blogabout nbsp', 0.0026075032722008232)\n",
      "('nbsp linkedinliked', 0.0026078832913516256)\n",
      "('forecasting data', 0.0026384734113822656)\n",
      "('research paper', 0.002718072108039429)\n",
      "('space model', 0.002734494563596171)\n",
      "('symposium forecasting', 0.0027544381002421623)\n",
      "('application', 0.0028046409521394677)\n",
      "('code', 0.0028155933937312762)\n",
      "('process', 0.002824663515321974)\n",
      "('change', 0.0028346432545955703)\n",
      "('great', 0.002865669208247188)\n",
      "('user', 0.002866959465616524)\n",
      "('student', 0.0028727216381191968)\n",
      "('software', 0.0029328123542009007)\n",
      "('predictive model', 0.0029424757644165075)\n",
      "('method forecasting', 0.002948113682061208)\n",
      "('continue reading', 0.0029500199797886655)\n",
      "('data work', 0.0029577571195844705)\n",
      "('arima model', 0.0029609347142752185)\n",
      "('version', 0.0029918346787201518)\n",
      "('year ago', 0.0030081273053262224)\n",
      "('thing', 0.0030234330283305227)\n",
      "('part', 0.0030575274155853387)\n",
      "('learn', 0.0030579854399197604)\n",
      "('data analyst', 0.00305947923536611)\n",
      "('data rsquo', 0.0030710584750259368)\n",
      "('forecasting research', 0.00307296248030667)\n",
      "('based', 0.003082102907716529)\n",
      "('job data', 0.0030919329786402543)\n",
      "('training data', 0.003157845340202977)\n",
      "('energy forecasting', 0.0032121914473938955)\n",
      "('real data', 0.003238629632534536)\n",
      "('amount data', 0.0032429168965141635)\n",
      "('data source', 0.003260084944297663)\n",
      "('science data', 0.0032654080962729124)\n",
      "('prediction', 0.0033384145001116327)\n",
      "('data collection', 0.003350440007354292)\n",
      "('google', 0.003392314114380919)\n",
      "('project', 0.0033935365866656314)\n",
      "('important', 0.0034565757008296494)\n",
      "('article', 0.0034750745282547944)\n",
      "('job', 0.0034824864983323553)\n",
      "('part data', 0.0035481432070424332)\n",
      "('training', 0.0035540140204634195)\n",
      "('ldquo forecasting', 0.003558760121243946)\n",
      "('post nbsp', 0.0035706080276539094)\n",
      "('international', 0.003586972598590887)\n",
      "('event', 0.00363844963703759)\n",
      "('week', 0.0036433507509418053)\n",
      "('linkedin', 0.0036525482177395346)\n",
      "('demand', 0.003674036120556961)\n",
      "('nuit blanche', 0.003694227433211703)\n",
      "('online', 0.0037703981255939704)\n",
      "('reading', 0.003805008416162144)\n",
      "('set data', 0.0038081705857571833)\n",
      "('international journal', 0.003817198789037636)\n",
      "('information', 0.0038794655648635218)\n",
      "('nbsp amp', 0.0039057962133766592)\n",
      "('predictive', 0.003906618034575988)\n",
      "('learning method', 0.003921352692655754)\n",
      "('state', 0.003941532800238857)\n",
      "('point', 0.003968868837895523)\n",
      "('practice', 0.003990970315378422)\n",
      "('science machine', 0.004006713666334861)\n",
      "('rate', 0.004011283573222076)\n",
      "('python data', 0.004023888298045006)\n",
      "('case', 0.004063549679679183)\n",
      "('big time', 0.00408638979088806)\n",
      "('source', 0.0041558999101107144)\n",
      "('year time', 0.0041831440213275515)\n",
      "('issue', 0.004197378832171988)\n",
      "('lighton', 0.004209140598030235)\n",
      "('future', 0.004223968444674513)\n",
      "('rdquo ldquo', 0.004241583584346148)\n",
      "('model forecasting', 0.004241898081912901)\n",
      "('data forecast', 0.004255016922554394)\n",
      "('lot', 0.004282051407487958)\n",
      "('find', 0.004296678707348442)\n",
      "('forecast accuracy', 0.004305664437155471)\n",
      "('working', 0.004313983787557748)\n",
      "('scientist data', 0.004318148208453942)\n"
     ]
    }
   ],
   "source": [
    "lemma_word = \"\"\n",
    "key_phrases = []\n",
    "\n",
    "for sentence in Main_DF[\"Lemmatized\"]:\n",
    "    lemma_word += sentence\n",
    "\n",
    "lemma_extractor = yake.KeywordExtractor(lan=\"en\", n=2, dedupLim=0.9, dedupFunc=\"seqm\", windowsSize=1, top=200,\n",
    "                                        features=None)\n",
    "keywords = lemma_extractor.extract_keywords(lemma_word)\n",
    "\n",
    "for kw in keywords:\n",
    "    print(kw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There is a lot of unneccessary key-Phrases, this is due use of verbal text present in sumaary\n",
    "#### To avoid this apply Key-Phrase on Title only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-115-08ee5c7cb5e0>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Main_DF[\"Title_Lemmatized\"] = Main_DF[\"Title\"].apply(lambda x : lemmatizer(x))\n"
     ]
    }
   ],
   "source": [
    "Main_DF[\"Title_Lemmatized\"] = Main_DF[\"Title\"].apply(lambda x : lemmatizer(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Key-Phrasing on Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('time series', 1.8339022913673134e-05)\n",
      "('data science', 0.00010874918122059422)\n",
      "('series forecasting', 0.00013092487105765638)\n",
      "('machine learning', 0.00013668020837973073)\n",
      "('series data', 0.0001732422081483741)\n",
      "('data', 0.00024101550479053093)\n",
      "('data scientist', 0.0003460424561280207)\n",
      "('forecasting', 0.00035356665221525845)\n",
      "('series analysis', 0.000362186316833339)\n",
      "('data analysis', 0.00037563527957470424)\n",
      "('series', 0.0003791584283542332)\n",
      "('time', 0.00039186049140560583)\n",
      "('forecasting competition', 0.0004113963743720885)\n",
      "('functional data', 0.0004974717503199238)\n",
      "('forecasting time', 0.0006087185324095115)\n",
      "('deep learning', 0.0006258922043444288)\n",
      "('functional time', 0.0006327222321257903)\n",
      "('data analytics', 0.0006687841260483232)\n",
      "('learning', 0.0007998005160761186)\n",
      "('series model', 0.0008280506491949598)\n",
      "('big time', 0.0008433954072446943)\n",
      "('model', 0.0008959487953709055)\n",
      "('data model', 0.0009366113550205776)\n",
      "('big data', 0.000943081020644543)\n",
      "('data forecasting', 0.0009716126851065315)\n",
      "('hierarchical time', 0.0009776002757640838)\n",
      "('data visualization', 0.0010071518531745806)\n",
      "('learning model', 0.0010531570611194885)\n",
      "('forecast reconciliation', 0.0011151126529692177)\n",
      "('forecast', 0.0011560485197075815)\n",
      "('series forecast', 0.0011597393360889404)\n",
      "('feature-based time', 0.001162027946086675)\n",
      "('energy forecasting', 0.001188877076318311)\n",
      "('forecasting model', 0.0012388488075696026)\n",
      "('forecasting workshop', 0.0012767335212764228)\n",
      "('job data', 0.0015613973945519245)\n",
      "('science', 0.0015744063651321057)\n",
      "('hierarchical forecasting', 0.0015865994164378358)\n",
      "('business analytics', 0.0016158877177442227)\n",
      "('analytics', 0.001629093758265917)\n",
      "('automatic time', 0.0016534464695868003)\n",
      "('arima model', 0.0016664750093330226)\n",
      "('data management', 0.0016932995266907668)\n",
      "('probabilistic forecasting', 0.0017766685299970533)\n",
      "('analytics data', 0.0017834243361288618)\n",
      "('machine', 0.001786366189491398)\n",
      "('high-dimensional time', 0.0018243168226573872)\n",
      "('electricity demand', 0.00190228489587161)\n",
      "('data mining', 0.0019930449515853563)\n",
      "('forecasting big', 0.001996180539063725)\n",
      "('forecast hierarchical', 0.002010498729513007)\n",
      "('forecast package', 0.002036850391141227)\n",
      "('tourism forecasting', 0.0020599995240730565)\n",
      "('reinforcement learning', 0.002104237908610956)\n",
      "('sql data', 0.0021523182274760023)\n",
      "('mortality forecasting', 0.0021944093385058137)\n",
      "('analysis time', 0.002205449016956571)\n",
      "('analysis', 0.002315467884562367)\n",
      "('tidy time', 0.0023349280003259836)\n",
      "('model data', 0.002341528387551444)\n",
      "('exponential smoothing', 0.002376328164285778)\n",
      "('additive model', 0.0024698418035952827)\n",
      "('scientist data', 0.002537644678272152)\n",
      "('data analyst', 0.0025412248423060625)\n",
      "('workshop forecasting', 0.0025534670425528456)\n",
      "('water-quality data', 0.0025587455128168405)\n",
      "('python data', 0.0025747750720165226)\n",
      "('forecasting forecast', 0.00260284867046311)\n",
      "('science book', 0.002616924601382695)\n",
      "('meter data', 0.002639410994928071)\n",
      "('series hierarchical', 0.002648618519505202)\n",
      "('series package', 0.002683168464414524)\n",
      "('making data', 0.0027416485275245433)\n",
      "('collection time', 0.002788231157622754)\n",
      "('statistical analysis', 0.002796138303996378)\n",
      "('probabilistic forecast', 0.002814895990599875)\n",
      "('analytics job', 0.0028483101744597064)\n",
      "('forecasting electricity', 0.0029740259197199204)\n",
      "('time forecasting', 0.0030435926620475577)\n",
      "('neural network', 0.0030958955412308873)\n",
      "('linear model', 0.003106424219036908)\n",
      "('predictive analytics', 0.0031812421532238806)\n",
      "('grouped time', 0.003186400121652037)\n",
      "('series visualization', 0.0032014070472940323)\n",
      "('research', 0.0032285131064123643)\n",
      "('master data', 0.003291635788753078)\n",
      "('science machine', 0.003326634613741704)\n",
      "('mortality rate', 0.0033821350284944617)\n",
      "('science job', 0.0033943218003822102)\n",
      "('forecasting functional', 0.003421964554324423)\n",
      "('data career', 0.0034548936804859455)\n",
      "('research habit', 0.003455852764440977)\n",
      "('series regression', 0.0034743646291819897)\n",
      "('predictive model', 0.0034760973520947393)\n",
      "('temporal data', 0.003520723642131433)\n",
      "('scientist extinct', 0.0035361063356747586)\n",
      "('forecasting research', 0.0035372269558414187)\n",
      "('data statistical', 0.0036385397185006592)\n",
      "('model time', 0.0036668069997226227)\n",
      "('data data', 0.003673074021027462)\n",
      "('outlier detection', 0.0037397491930325436)\n",
      "('space model', 0.003760552121891277)\n",
      "('dimensional time', 0.0037762408355320127)\n",
      "('forecasting principle', 0.0037955553849200567)\n",
      "('forecast time', 0.0038517717256987814)\n",
      "('autoregressive model', 0.0038644829711288304)\n",
      "('forecasting data', 0.003886450740426126)\n",
      "('state space', 0.003923991257122459)\n",
      "('forecasting hierarchical', 0.003966498541094589)\n",
      "('habit forecasting', 0.004006428374624839)\n",
      "('series modelling', 0.00403624456704852)\n",
      "('load forecasting', 0.004045405039960935)\n",
      "('forecasting conference', 0.004056687574054491)\n",
      "('statistical learning', 0.004092610340299537)\n",
      "('forecasting forecasting', 0.004112273756633731)\n",
      "('demand data', 0.004119890442297231)\n",
      "('learning data', 0.0041296104231179735)\n",
      "('data learning', 0.0041296104231179735)\n",
      "('snapshot data', 0.004161625295042629)\n",
      "('analytics time', 0.0041886298976594035)\n",
      "('test time', 0.004208295058521352)\n",
      "('tidy forecasting', 0.00421566379546593)\n",
      "('functional', 0.004248674563678468)\n",
      "('apply data', 0.004297716188987166)\n",
      "('learning forecasting', 0.004369623781204211)\n",
      "('forecasting mortality', 0.004388818677011627)\n",
      "('workshop', 0.004399706276694621)\n",
      "('electricity forecasting', 0.004461038879579881)\n",
      "('series functional', 0.004571656102229688)\n",
      "('forecasting algorithm', 0.004584072275796635)\n",
      "('statistical', 0.004593207998913734)\n",
      "('forecasting method', 0.004670566978486702)\n",
      "('python', 0.004692846411433979)\n",
      "('optimal forecast', 0.004700597357409782)\n",
      "('forecasting based', 0.0047033069254808595)\n",
      "('visualization forecasting', 0.004796964426316682)\n",
      "('performing data', 0.004828000345754346)\n",
      "('data engineer', 0.004828000345754346)\n",
      "('engineer data', 0.004828000345754346)\n",
      "('data situ', 0.004828000345754346)\n",
      "('metre data', 0.004828000345754346)\n",
      "('forecasting smart', 0.004850491285737805)\n",
      "('land data', 0.00486046850117259)\n",
      "('learning algorithm', 0.004872483248841832)\n",
      "('data ethic', 0.004896594722082462)\n",
      "('job', 0.004925088492574766)\n",
      "('model forecasting', 0.0049553952302784105)\n",
      "('series cross-validation', 0.004970356860751001)\n",
      "('hierarchical forecast', 0.005026246823782518)\n",
      "('anomaly detection', 0.0050749486569046445)\n",
      "('management data', 0.0050798985800723)\n",
      "('forecasting importance', 0.005091625363793028)\n",
      "('package forecast', 0.005092125977853067)\n",
      "('learning online', 0.0050946836972421755)\n",
      "('machine data', 0.00510503235824595)\n",
      "('data privacy', 0.005124784439370035)\n",
      "('follow-up forecasting', 0.005144624366512019)\n",
      "('forecasting forum', 0.005144624366512019)\n",
      "('data warehouse', 0.005155264705543201)\n",
      "('data optimal', 0.005252252746420865)\n",
      "('rate functional', 0.0052608758299329825)\n",
      "('guide data', 0.005263103418193123)\n",
      "('model mortality', 0.005293681714428245)\n",
      "('forecasting book', 0.005305576882550331)\n",
      "('smart meter', 0.005323521296232202)\n",
      "('statistical test', 0.005368287560790437)\n",
      "('package', 0.00541659797761727)\n",
      "('data preparation', 0.005452942380925621)\n",
      "('gradient flow', 0.005462719740984627)\n",
      "('good research', 0.0055418050078249615)\n",
      "('forecast mortality', 0.005562645662432786)\n",
      "('data approach', 0.005577370067742094)\n",
      "('automatic forecasting', 0.005595263049035629)\n",
      "('future forecasting', 0.005613069917387657)\n",
      "('forecasting future', 0.005613069917387657)\n",
      "('analysis data', 0.005634529193620564)\n",
      "('prediction interval', 0.00566391405603219)\n",
      "('time threshold', 0.005670435683798721)\n",
      "('scientist', 0.005676974568351127)\n",
      "('continuous time', 0.005704515158012456)\n",
      "('extinct year', 0.005710505527398894)\n",
      "('tutorial', 0.005717107736068088)\n",
      "('learning machine', 0.0057405687519486905)\n",
      "('model selection', 0.005757997949857649)\n",
      "('competition', 0.005771164016690935)\n",
      "('business', 0.0057791221843865445)\n",
      "('series funding', 0.005782255065013108)\n",
      "('product management', 0.005799756289223149)\n",
      "('forecasting large', 0.005804283751845331)\n",
      "('beginner tutorial', 0.005850598219508469)\n",
      "('visualizing forecasting', 0.005884445719668143)\n",
      "('scraping python', 0.0059342172129941305)\n",
      "('analysis python', 0.005937390792616474)\n",
      "('cloud data', 0.00594637182019684)\n",
      "('data skill', 0.00594637182019684)\n",
      "('analysis forecasting', 0.005962527950330567)\n",
      "('tidy data', 0.005973456600107243)\n",
      "('history time', 0.006047710763666411)\n",
      "('modelling forecasting', 0.00605288361862285)\n",
      "('learning application', 0.006062267037271618)\n",
      "('group time', 0.006076347102890683)\n",
      "('series tsibbles', 0.00611344214673208)\n",
      "('year data', 0.006122866317520396)\n",
      "('analysis tidy', 0.006135902508242189)\n",
      "('smoothing model', 0.00616550694966252)\n",
      "('big', 0.006189078513071021)\n",
      "('flow snapshot', 0.006270612695389135)\n",
      "('forecast model', 0.006274399495399047)\n",
      "('forecasting high', 0.006276450537880033)\n",
      "('forecasting feature-based', 0.00628370887114525)\n",
      "('feature-based forecasting', 0.00628370887114525)\n",
      "('model exponential', 0.006286017985060083)\n",
      "('series deep', 0.006288004747276698)\n",
      "('paper ijf', 0.006320818104116356)\n",
      "('developing good', 0.006333749492242718)\n",
      "('short time', 0.006425565153458112)\n",
      "('paper', 0.006427873187044198)\n",
      "('hierarchical', 0.006472948612483333)\n",
      "('mortality', 0.006474601177715112)\n",
      "('symposium forecasting', 0.00651377283625376)\n",
      "('tao hong', 0.006520577736081312)\n",
      "('probabilistic time', 0.006566666867141123)\n",
      "('price forecasting', 0.00659623296847839)\n",
      "('data research', 0.006684881619287093)\n",
      "('tensorflow model', 0.006691894471631593)\n",
      "('sql', 0.00671901359770073)\n",
      "('platform data', 0.00674463025310116)\n",
      "('strategy time', 0.0067699708662907025)\n",
      "('algorithm time', 0.006776696743569086)\n",
      "('series smoothing', 0.006819386907552833)\n",
      "('soup tutorial', 0.006838662156219604)\n",
      "('forecasting social', 0.0068557257827670795)\n",
      "('series detection', 0.006862556178602344)\n",
      "('series invited', 0.006865285283045473)\n",
      "('model application', 0.006882069757157109)\n",
      "('probabilistic energy', 0.006891562307648509)\n",
      "('series instance', 0.0069110865154121855)\n",
      "('series outlier', 0.006918222219705712)\n",
      "('jun data', 0.006969202551636552)\n",
      "('science analytics', 0.0069734016464910445)\n",
      "('tip learning', 0.006975650484974851)\n",
      "('oracle sql', 0.006980097156545527)\n",
      "('high-dimensional data', 0.006993257880895119)\n",
      "('data simple', 0.007004748042607052)\n",
      "('data generate', 0.007041447284262866)\n",
      "('research forecasting', 0.007074453911682837)\n",
      "('series book', 0.007079319042080095)\n",
      "('visualization time', 0.007090637768925003)\n",
      "('challenge forecasting', 0.007097714990071055)\n",
      "('analytics monash', 0.00710943269865862)\n",
      "('series seasonal', 0.007166437774575119)\n",
      "('analytics forecast', 0.007169959545042005)\n",
      "('analysis model', 0.007187714425006428)\n",
      "('importance data', 0.007213629017991108)\n",
      "('series prediction', 0.007271876099742066)\n",
      "('data integration', 0.007301121644376838)\n",
      "('learn python', 0.007309074402693626)\n",
      "('generate automated', 0.0073848556704066695)\n",
      "('competition data', 0.0073857948099275185)\n",
      "('ray data', 0.007388677304093072)\n",
      "('forecasting temporal', 0.007451952836854444)\n",
      "('paper award', 0.00753002235742084)\n",
      "('data big', 0.007544648165156344)\n",
      "('learning challenge', 0.0075466393113925775)\n",
      "('series collection', 0.007552353862411626)\n",
      "('beautiful soup', 0.007554881764889892)\n",
      "('soup data', 0.007571220486930666)\n",
      "('data package', 0.007593790742351535)\n",
      "('series application', 0.00760713091918535)\n",
      "('monash time', 0.0076138514098724195)\n",
      "('analyst data', 0.0076236745269181875)\n",
      "('feast time', 0.007653926718672279)\n",
      "('bagging time', 0.007653926718672279)\n",
      "('time trend', 0.0076609590780037035)\n",
      "('easier forecast', 0.007719468079218644)\n",
      "('data python', 0.007724325216049568)\n",
      "('election data', 0.007728905653183976)\n",
      "('non-negative data', 0.007732897058314802)\n",
      "('series density', 0.007778271602530155)\n",
      "('modern data', 0.007778624440453508)\n",
      "('sql tutorial', 0.007782743724708992)\n",
      "('science mining', 0.007794522352099121)\n",
      "('data job', 0.007806986972759622)\n",
      "('competition forecasting', 0.007816531113069682)\n",
      "('high performing', 0.007824667227671446)\n",
      "('high dimensional', 0.007824667227671446)\n",
      "('rds oracle', 0.007833800361637637)\n",
      "('book time', 0.007840382569652643)\n",
      "('peak electricity', 0.007881439401607985)\n",
      "('learning language', 0.007885481043087136)\n",
      "('learning work', 0.007932853587687713)\n",
      "('seasonal time', 0.007936940830489193)\n",
      "('time management', 0.007945664507767234)\n",
      "('demand', 0.007946652261483247)\n",
      "('feature-based forecast', 0.007959077710177714)\n",
      "('series locality', 0.00798727438194132)\n",
      "('series co-authorships', 0.00798727438194132)\n",
      "('series diverse', 0.00798727438194132)\n",
      "('series trace', 0.00798727438194132)\n",
      "('series repository', 0.00798727438194132)\n",
      "('quantile forecast', 0.008002436784208401)\n",
      "('package forecasting', 0.008036754049682576)\n",
      "('large collection', 0.008096819629065976)\n",
      "('hong award', 0.008099246655345789)\n",
      "('deep', 0.008102497433038838)\n",
      "('learn sql', 0.008159623686811737)\n",
      "('history forecasting', 0.008179814508371054)\n",
      "('series bayesian', 0.008217120531733051)\n",
      "('job interview', 0.008224062332488511)\n",
      "('exploratory data', 0.008227265742519049)\n",
      "('data sharing', 0.008227265742519049)\n",
      "('visualization', 0.008249495440074535)\n",
      "('future time', 0.008293466428520795)\n",
      "('algorithm', 0.008350627788223333)\n",
      "('hierarchical grouped', 0.008392800066334197)\n",
      "('data probabilistic', 0.00839332923355959)\n",
      "('application time', 0.008425411680429944)\n",
      "('series tidy', 0.008431891400215878)\n",
      "('series tool', 0.008495673629091618)\n",
      "('step guide', 0.008498531951574344)\n",
      "('reconciliation hierarchical', 0.008523048778691764)\n",
      "('series autoregressive', 0.00853687161495915)\n",
      "('model long', 0.008570573831743266)\n",
      "('detection functional', 0.00859371665219522)\n",
      "('session data', 0.008595432377974332)\n",
      "('sensor data', 0.008595432377974332)\n",
      "('probabilistic', 0.008599349667328917)\n",
      "('hierarchical probabilistic', 0.008606433166487395)\n",
      "('data matter', 0.00865367290006745)\n",
      "('insurance data', 0.00865367290006745)\n",
      "('practical data', 0.00865367290006745)\n",
      "('facebook data', 0.00865367290006745)\n",
      "('data tsibble', 0.00865367290006745)\n",
      "('functional forecast', 0.008669951349790673)\n",
      "('pak data', 0.008698505347289514)\n",
      "('data trustworthy', 0.008698505347289514)\n",
      "('demand forecasting', 0.008720752424618002)\n",
      "('approach time', 0.008721906632224122)\n",
      "('forest tutorial', 0.00873630653221518)\n",
      "('decision-making time', 0.00884680528500561)\n",
      "('ton time', 0.00884680528500561)\n",
      "('archive time', 0.00884680528500561)\n",
      "('ultra-long time', 0.00884680528500561)\n",
      "('sub-daily time', 0.00884680528500561)\n",
      "('biggish time', 0.00884680528500561)\n",
      "('analysis easier', 0.008854869088907591)\n",
      "('data deep', 0.008901410716427383)\n",
      "('model high-dimensional', 0.008924307031507544)\n",
      "('forecasting peak', 0.008936025166082004)\n",
      "('language model', 0.008956201167550294)\n",
      "('forecast research', 0.00896252976032644)\n",
      "('seasonal functional', 0.008978332842258612)\n",
      "('energy data', 0.008986071730571685)\n",
      "('method', 0.009038404501777546)\n",
      "('seminar time', 0.00904508044168139)\n",
      "('visualization data', 0.009064366678571226)\n",
      "('exploring time', 0.009101609101661896)\n",
      "('bayesian time', 0.009101609101661896)\n",
      "('analysis feature-based', 0.00911984045359492)\n",
      "('time tourism', 0.00912844536505883)\n",
      "('tourism time', 0.00912844536505883)\n",
      "('forecasting long-term', 0.00913083583174713)\n",
      "('forecasting impact', 0.009159023567589592)\n",
      "('cairn forecasting', 0.009159023567589592)\n",
      "('algorithm forecasting', 0.00916814455159327)\n",
      "('science project', 0.009208848144672733)\n",
      "('forecasting practitioner', 0.009264323996705397)\n",
      "('statistical model', 0.00928722306903003)\n",
      "('beginner guide', 0.009300622731787438)\n",
      "('forecasting ijf', 0.009350415441015359)\n",
      "('science workshop', 0.009439622726504794)\n",
      "('autoregressive time', 0.009456100909366444)\n",
      "('oracle function', 0.00945994891384642)\n",
      "('guide master', 0.009471873596274003)\n",
      "('statistical science', 0.009490267146280327)\n",
      "('year time', 0.00957270252683531)\n",
      "('series characteristic', 0.009572963758754823)\n",
      "('series word', 0.009572963758754823)\n",
      "('guide', 0.009616143775484734)\n",
      "('beijing workshop', 0.009634616466942053)\n",
      "('ijf', 0.009650455460544358)\n",
      "('learning sql', 0.009685263442355839)\n",
      "('statistical software', 0.009695636616696892)\n",
      "('learning applied', 0.009735266123006125)\n",
      "('forecast statistical', 0.009758454930851986)\n",
      "('generalized additive', 0.009784659395260834)\n",
      "('data accelerate', 0.009785974663517502)\n",
      "('lesson data', 0.009785974663517502)\n",
      "('data terminology', 0.009785974663517502)\n",
      "('wrong data', 0.009785974663517502)\n",
      "('salary data', 0.009785974663517502)\n",
      "('data medascin', 0.009785974663517502)\n",
      "('learning meetup', 0.009790703913760785)\n",
      "('time reason', 0.00980192900651056)\n",
      "('electricity', 0.009870302693685964)\n",
      "('online', 0.009891281546295714)\n",
      "('business model', 0.009893311752211295)\n",
      "('learning method', 0.009928946767675917)\n",
      "('story jun', 0.009978072683471718)\n",
      "('series podcast', 0.009999757576550675)\n",
      "('series graphic', 0.009999757576550675)\n",
      "('make data', 0.010039012486735279)\n",
      "('data make', 0.010039012486735279)\n",
      "('learning rate', 0.010044449555772874)\n",
      "('principle practice', 0.010105728351795937)\n",
      "('science business', 0.010109739313444636)\n",
      "('thief package', 0.010113686290827314)\n",
      "('sql sql', 0.010156714301782251)\n",
      "('analysis tutorial', 0.010162958916014835)\n",
      "('additive modelling', 0.010164058226844568)\n",
      "('science event', 0.010169772788378391)\n",
      "('learning visualization', 0.01019781009884895)\n",
      "('smoothing forecasting', 0.010220226837495567)\n",
      "('automate python', 0.010220284355837422)\n",
      "('federal election', 0.01022642139054551)\n",
      "('world', 0.010254522097349224)\n",
      "('ijf tao', 0.010266174537786322)\n",
      "('competition machine', 0.010279992849426241)\n",
      "('space', 0.010341679174701277)\n",
      "('learn data', 0.010350331413360227)\n",
      "('data learn', 0.010350331413360227)\n",
      "('macroeconomic forecasting', 0.010358127673443221)\n",
      "('forecasting bivariate', 0.010358127673443221)\n",
      "('forecasting nyc', 0.010358127673443221)\n",
      "('career data', 0.010364681041457836)\n",
      "('trend forecasting', 0.010367670192201398)\n",
      "('forecasting exponential', 0.010419566803386788)\n",
      "('data story', 0.010453993187260273)\n",
      "('job forecast', 0.01047093135833406)\n",
      "('long time', 0.010479032486381601)\n",
      "('series framework', 0.010482491608925743)\n",
      "('cancer mortality', 0.01048714297485443)\n",
      "('death rate', 0.01052034868877193)\n",
      "('fitting model', 0.010526371244794323)\n",
      "('smoothing', 0.01053909562283571)\n",
      "('selection', 0.010554048115304948)\n",
      "('journal forecasting', 0.010560413055366953)\n",
      "('analytics workshop', 0.010560625956750894)\n",
      "('research seminar', 0.01059604844976546)\n",
      "('detection', 0.010605357317426414)\n",
      "('future data', 0.01060553705885065)\n",
      "('algorithm selection', 0.010712520303349257)\n",
      "('model probabilistic', 0.010715093964531167)\n",
      "('functional autoregressive', 0.010717210991762493)\n",
      "('forecasting seasonal', 0.01074219917815149)\n",
      "('seasonal forecasting', 0.01074219917815149)\n",
      "('data neural', 0.010749295601884788)\n",
      "('management forecasting', 0.0107540390775783)\n",
      "('science position', 0.01075924858403218)\n",
      "('data test', 0.01076335184727311)\n",
      "('web scraping', 0.010784881754739123)\n",
      "('learning sas', 0.010864220831257234)\n",
      "('book', 0.010937986688479676)\n",
      "('combination forecast', 0.011002296648733676)\n",
      "('measure forecast', 0.011002296648733676)\n",
      "('datacamp learning', 0.011010635327495587)\n",
      "('learning hard', 0.011010635327495587)\n",
      "('learning poll', 0.011010635327495587)\n",
      "('cross-validation time', 0.011012755895748863)\n",
      "('series hts', 0.01103267483291471)\n",
      "('model facebook', 0.011048242989334914)\n",
      "('reconciling forecast', 0.011067023238797618)\n",
      "('seasonal', 0.011071632763428483)\n",
      "('data google', 0.011117954418598268)\n",
      "('forecasting award', 0.011119131171293236)\n",
      "('forecasting optimal', 0.011119131171293236)\n",
      "('model bakery', 0.0111321830132141)\n",
      "('analysis statistical', 0.011184553215985512)\n",
      "('planning analytics', 0.011189812923863347)\n",
      "('forecasting automatic', 0.011190526098071259)\n",
      "('rate', 0.01125231023657671)\n",
      "('forecast probabilistic', 0.0112595839623995)\n",
      "('learning world', 0.01127995619734314)\n",
      "('probabilistic outlier', 0.011280968009416253)\n",
      "('science journey', 0.011290218456286954)\n",
      "('top data', 0.011290514075160591)\n",
      "('data top', 0.011290514075160591)\n",
      "('state data', 0.011296289478490177)\n",
      "('data state', 0.011296289478490177)\n",
      "('visualize data', 0.011314868012942753)\n",
      "('data joy', 0.011314868012942753)\n",
      "('14-20 data', 0.011314868012942753)\n",
      "('external data', 0.011314868012942753)\n",
      "('data storytelling', 0.011314868012942753)\n",
      "('cost data', 0.011314868012942753)\n",
      "('income data', 0.011314868012942753)\n",
      "('trial data', 0.011314868012942753)\n",
      "('harm data', 0.011314868012942753)\n",
      "('found data', 0.011314868012942753)\n",
      "('high-quality data', 0.011314868012942753)\n",
      "('compare data', 0.011314868012942753)\n",
      "('data frame', 0.011314868012942753)\n",
      "('erwin data', 0.011314868012942753)\n",
      "('data catalog', 0.011314868012942753)\n",
      "('spring data', 0.011314868012942753)\n",
      "('chief data', 0.011314868012942753)\n",
      "('data officer', 0.011314868012942753)\n",
      "('data entering', 0.011314868012942753)\n",
      "('data mandarin', 0.011314868012942753)\n",
      "('data elca', 0.011314868012942753)\n"
     ]
    }
   ],
   "source": [
    "lemma_word = \"\"\n",
    "\n",
    "for sentence in Main_DF[\"Title_Lemmatized\"]:\n",
    "    lemma_word += sentence\n",
    "\n",
    "lemma_extractor = yake.KeywordExtractor(lan=\"en\", n=2, dedupLim=0.9, dedupFunc=\"seqm\", windowsSize=1, top=500,\n",
    "                                        features=None)\n",
    "keywords = lemma_extractor.extract_keywords(lemma_word)\n",
    "\n",
    "for kw in keywords:\n",
    "    print(kw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing the values of key-phrase , which can be later used for client suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['time series',\n",
       " 'data science',\n",
       " 'series forecasting',\n",
       " 'machine learning',\n",
       " 'series data',\n",
       " 'data',\n",
       " 'data scientist',\n",
       " 'forecasting',\n",
       " 'series analysis',\n",
       " 'data analysis',\n",
       " 'series',\n",
       " 'time',\n",
       " 'forecasting competition',\n",
       " 'functional data',\n",
       " 'forecasting time',\n",
       " 'deep learning',\n",
       " 'functional time',\n",
       " 'data analytics',\n",
       " 'learning',\n",
       " 'series model',\n",
       " 'big time',\n",
       " 'model',\n",
       " 'data model',\n",
       " 'big data',\n",
       " 'data forecasting',\n",
       " 'hierarchical time',\n",
       " 'data visualization',\n",
       " 'learning model',\n",
       " 'forecast reconciliation',\n",
       " 'forecast',\n",
       " 'series forecast',\n",
       " 'feature-based time',\n",
       " 'energy forecasting',\n",
       " 'forecasting model',\n",
       " 'forecasting workshop',\n",
       " 'job data',\n",
       " 'science',\n",
       " 'hierarchical forecasting',\n",
       " 'business analytics',\n",
       " 'analytics',\n",
       " 'automatic time',\n",
       " 'arima model',\n",
       " 'data management',\n",
       " 'probabilistic forecasting',\n",
       " 'analytics data',\n",
       " 'machine',\n",
       " 'high-dimensional time',\n",
       " 'electricity demand',\n",
       " 'data mining',\n",
       " 'forecasting big',\n",
       " 'forecast hierarchical',\n",
       " 'forecast package',\n",
       " 'tourism forecasting',\n",
       " 'reinforcement learning',\n",
       " 'sql data',\n",
       " 'mortality forecasting',\n",
       " 'analysis time',\n",
       " 'analysis',\n",
       " 'tidy time',\n",
       " 'model data',\n",
       " 'exponential smoothing',\n",
       " 'additive model',\n",
       " 'scientist data',\n",
       " 'data analyst',\n",
       " 'workshop forecasting',\n",
       " 'water-quality data',\n",
       " 'python data',\n",
       " 'forecasting forecast',\n",
       " 'science book',\n",
       " 'meter data',\n",
       " 'series hierarchical',\n",
       " 'series package',\n",
       " 'making data',\n",
       " 'collection time',\n",
       " 'statistical analysis',\n",
       " 'probabilistic forecast',\n",
       " 'analytics job',\n",
       " 'forecasting electricity',\n",
       " 'time forecasting',\n",
       " 'neural network',\n",
       " 'linear model',\n",
       " 'predictive analytics',\n",
       " 'grouped time',\n",
       " 'series visualization',\n",
       " 'research',\n",
       " 'master data',\n",
       " 'science machine',\n",
       " 'mortality rate',\n",
       " 'science job',\n",
       " 'forecasting functional',\n",
       " 'data career',\n",
       " 'research habit',\n",
       " 'series regression',\n",
       " 'predictive model',\n",
       " 'temporal data',\n",
       " 'scientist extinct',\n",
       " 'forecasting research',\n",
       " 'data statistical',\n",
       " 'model time',\n",
       " 'data data',\n",
       " 'outlier detection',\n",
       " 'space model',\n",
       " 'dimensional time',\n",
       " 'forecasting principle',\n",
       " 'forecast time',\n",
       " 'autoregressive model',\n",
       " 'forecasting data',\n",
       " 'state space',\n",
       " 'forecasting hierarchical',\n",
       " 'habit forecasting',\n",
       " 'series modelling',\n",
       " 'load forecasting',\n",
       " 'forecasting conference',\n",
       " 'statistical learning',\n",
       " 'forecasting forecasting',\n",
       " 'demand data',\n",
       " 'learning data',\n",
       " 'data learning',\n",
       " 'snapshot data',\n",
       " 'analytics time',\n",
       " 'test time',\n",
       " 'tidy forecasting',\n",
       " 'functional',\n",
       " 'apply data',\n",
       " 'learning forecasting',\n",
       " 'forecasting mortality',\n",
       " 'workshop',\n",
       " 'electricity forecasting',\n",
       " 'series functional',\n",
       " 'forecasting algorithm',\n",
       " 'statistical',\n",
       " 'forecasting method',\n",
       " 'python',\n",
       " 'optimal forecast',\n",
       " 'forecasting based',\n",
       " 'visualization forecasting',\n",
       " 'performing data',\n",
       " 'data engineer',\n",
       " 'engineer data',\n",
       " 'data situ',\n",
       " 'metre data',\n",
       " 'forecasting smart',\n",
       " 'land data',\n",
       " 'learning algorithm',\n",
       " 'data ethic',\n",
       " 'job',\n",
       " 'model forecasting',\n",
       " 'series cross-validation',\n",
       " 'hierarchical forecast',\n",
       " 'anomaly detection',\n",
       " 'management data',\n",
       " 'forecasting importance',\n",
       " 'package forecast',\n",
       " 'learning online',\n",
       " 'machine data',\n",
       " 'data privacy',\n",
       " 'follow-up forecasting',\n",
       " 'forecasting forum',\n",
       " 'data warehouse',\n",
       " 'data optimal',\n",
       " 'rate functional',\n",
       " 'guide data',\n",
       " 'model mortality',\n",
       " 'forecasting book',\n",
       " 'smart meter',\n",
       " 'statistical test',\n",
       " 'package',\n",
       " 'data preparation',\n",
       " 'gradient flow',\n",
       " 'good research',\n",
       " 'forecast mortality',\n",
       " 'data approach',\n",
       " 'automatic forecasting',\n",
       " 'future forecasting',\n",
       " 'forecasting future',\n",
       " 'analysis data',\n",
       " 'prediction interval',\n",
       " 'time threshold',\n",
       " 'scientist',\n",
       " 'continuous time',\n",
       " 'extinct year',\n",
       " 'tutorial',\n",
       " 'learning machine',\n",
       " 'model selection',\n",
       " 'competition',\n",
       " 'business',\n",
       " 'series funding',\n",
       " 'product management',\n",
       " 'forecasting large',\n",
       " 'beginner tutorial',\n",
       " 'visualizing forecasting',\n",
       " 'scraping python',\n",
       " 'analysis python',\n",
       " 'cloud data',\n",
       " 'data skill',\n",
       " 'analysis forecasting',\n",
       " 'tidy data',\n",
       " 'history time',\n",
       " 'modelling forecasting',\n",
       " 'learning application',\n",
       " 'group time',\n",
       " 'series tsibbles',\n",
       " 'year data',\n",
       " 'analysis tidy',\n",
       " 'smoothing model',\n",
       " 'big',\n",
       " 'flow snapshot',\n",
       " 'forecast model',\n",
       " 'forecasting high',\n",
       " 'forecasting feature-based',\n",
       " 'feature-based forecasting',\n",
       " 'model exponential',\n",
       " 'series deep',\n",
       " 'paper ijf',\n",
       " 'developing good',\n",
       " 'short time',\n",
       " 'paper',\n",
       " 'hierarchical',\n",
       " 'mortality',\n",
       " 'symposium forecasting',\n",
       " 'tao hong',\n",
       " 'probabilistic time',\n",
       " 'price forecasting',\n",
       " 'data research',\n",
       " 'tensorflow model',\n",
       " 'sql',\n",
       " 'platform data',\n",
       " 'strategy time',\n",
       " 'algorithm time',\n",
       " 'series smoothing',\n",
       " 'soup tutorial',\n",
       " 'forecasting social',\n",
       " 'series detection',\n",
       " 'series invited',\n",
       " 'model application',\n",
       " 'probabilistic energy',\n",
       " 'series instance',\n",
       " 'series outlier',\n",
       " 'jun data',\n",
       " 'science analytics',\n",
       " 'tip learning',\n",
       " 'oracle sql',\n",
       " 'high-dimensional data',\n",
       " 'data simple',\n",
       " 'data generate',\n",
       " 'research forecasting',\n",
       " 'series book',\n",
       " 'visualization time',\n",
       " 'challenge forecasting',\n",
       " 'analytics monash',\n",
       " 'series seasonal',\n",
       " 'analytics forecast',\n",
       " 'analysis model',\n",
       " 'importance data',\n",
       " 'series prediction',\n",
       " 'data integration',\n",
       " 'learn python',\n",
       " 'generate automated',\n",
       " 'competition data',\n",
       " 'ray data',\n",
       " 'forecasting temporal',\n",
       " 'paper award',\n",
       " 'data big',\n",
       " 'learning challenge',\n",
       " 'series collection',\n",
       " 'beautiful soup',\n",
       " 'soup data',\n",
       " 'data package',\n",
       " 'series application',\n",
       " 'monash time',\n",
       " 'analyst data',\n",
       " 'feast time',\n",
       " 'bagging time',\n",
       " 'time trend',\n",
       " 'easier forecast',\n",
       " 'data python',\n",
       " 'election data',\n",
       " 'non-negative data',\n",
       " 'series density',\n",
       " 'modern data',\n",
       " 'sql tutorial',\n",
       " 'science mining',\n",
       " 'data job',\n",
       " 'competition forecasting',\n",
       " 'high performing',\n",
       " 'high dimensional',\n",
       " 'rds oracle',\n",
       " 'book time',\n",
       " 'peak electricity',\n",
       " 'learning language',\n",
       " 'learning work',\n",
       " 'seasonal time',\n",
       " 'time management',\n",
       " 'demand',\n",
       " 'feature-based forecast',\n",
       " 'series locality',\n",
       " 'series co-authorships',\n",
       " 'series diverse',\n",
       " 'series trace',\n",
       " 'series repository',\n",
       " 'quantile forecast',\n",
       " 'package forecasting',\n",
       " 'large collection',\n",
       " 'hong award',\n",
       " 'deep',\n",
       " 'learn sql',\n",
       " 'history forecasting',\n",
       " 'series bayesian',\n",
       " 'job interview',\n",
       " 'exploratory data',\n",
       " 'data sharing',\n",
       " 'visualization',\n",
       " 'future time',\n",
       " 'algorithm',\n",
       " 'hierarchical grouped',\n",
       " 'data probabilistic',\n",
       " 'application time',\n",
       " 'series tidy',\n",
       " 'series tool',\n",
       " 'step guide',\n",
       " 'reconciliation hierarchical',\n",
       " 'series autoregressive',\n",
       " 'model long',\n",
       " 'detection functional',\n",
       " 'session data',\n",
       " 'sensor data',\n",
       " 'probabilistic',\n",
       " 'hierarchical probabilistic',\n",
       " 'data matter',\n",
       " 'insurance data',\n",
       " 'practical data',\n",
       " 'facebook data',\n",
       " 'data tsibble',\n",
       " 'functional forecast',\n",
       " 'pak data',\n",
       " 'data trustworthy',\n",
       " 'demand forecasting',\n",
       " 'approach time',\n",
       " 'forest tutorial',\n",
       " 'decision-making time',\n",
       " 'ton time',\n",
       " 'archive time',\n",
       " 'ultra-long time',\n",
       " 'sub-daily time',\n",
       " 'biggish time',\n",
       " 'analysis easier',\n",
       " 'data deep',\n",
       " 'model high-dimensional',\n",
       " 'forecasting peak',\n",
       " 'language model',\n",
       " 'forecast research',\n",
       " 'seasonal functional',\n",
       " 'energy data',\n",
       " 'method',\n",
       " 'seminar time',\n",
       " 'visualization data',\n",
       " 'exploring time',\n",
       " 'bayesian time',\n",
       " 'analysis feature-based',\n",
       " 'time tourism',\n",
       " 'tourism time',\n",
       " 'forecasting long-term',\n",
       " 'forecasting impact',\n",
       " 'cairn forecasting',\n",
       " 'algorithm forecasting',\n",
       " 'science project',\n",
       " 'forecasting practitioner',\n",
       " 'statistical model',\n",
       " 'beginner guide',\n",
       " 'forecasting ijf',\n",
       " 'science workshop',\n",
       " 'autoregressive time',\n",
       " 'oracle function',\n",
       " 'guide master',\n",
       " 'statistical science',\n",
       " 'year time',\n",
       " 'series characteristic',\n",
       " 'series word',\n",
       " 'guide',\n",
       " 'beijing workshop',\n",
       " 'ijf',\n",
       " 'learning sql',\n",
       " 'statistical software',\n",
       " 'learning applied',\n",
       " 'forecast statistical',\n",
       " 'generalized additive',\n",
       " 'data accelerate',\n",
       " 'lesson data',\n",
       " 'data terminology',\n",
       " 'wrong data',\n",
       " 'salary data',\n",
       " 'data medascin',\n",
       " 'learning meetup',\n",
       " 'time reason',\n",
       " 'electricity',\n",
       " 'online',\n",
       " 'business model',\n",
       " 'learning method',\n",
       " 'story jun',\n",
       " 'series podcast',\n",
       " 'series graphic',\n",
       " 'make data',\n",
       " 'data make',\n",
       " 'learning rate',\n",
       " 'principle practice',\n",
       " 'science business',\n",
       " 'thief package',\n",
       " 'sql sql',\n",
       " 'analysis tutorial',\n",
       " 'additive modelling',\n",
       " 'science event',\n",
       " 'learning visualization',\n",
       " 'smoothing forecasting',\n",
       " 'automate python',\n",
       " 'federal election',\n",
       " 'world',\n",
       " 'ijf tao',\n",
       " 'competition machine',\n",
       " 'space',\n",
       " 'learn data',\n",
       " 'data learn',\n",
       " 'macroeconomic forecasting',\n",
       " 'forecasting bivariate',\n",
       " 'forecasting nyc',\n",
       " 'career data',\n",
       " 'trend forecasting',\n",
       " 'forecasting exponential',\n",
       " 'data story',\n",
       " 'job forecast',\n",
       " 'long time',\n",
       " 'series framework',\n",
       " 'cancer mortality',\n",
       " 'death rate',\n",
       " 'fitting model',\n",
       " 'smoothing',\n",
       " 'selection',\n",
       " 'journal forecasting',\n",
       " 'analytics workshop',\n",
       " 'research seminar',\n",
       " 'detection',\n",
       " 'future data',\n",
       " 'algorithm selection',\n",
       " 'model probabilistic',\n",
       " 'functional autoregressive',\n",
       " 'forecasting seasonal',\n",
       " 'seasonal forecasting',\n",
       " 'data neural',\n",
       " 'management forecasting',\n",
       " 'science position',\n",
       " 'data test',\n",
       " 'web scraping',\n",
       " 'learning sas',\n",
       " 'book',\n",
       " 'combination forecast',\n",
       " 'measure forecast',\n",
       " 'datacamp learning',\n",
       " 'learning hard',\n",
       " 'learning poll',\n",
       " 'cross-validation time',\n",
       " 'series hts',\n",
       " 'model facebook',\n",
       " 'reconciling forecast',\n",
       " 'seasonal',\n",
       " 'data google',\n",
       " 'forecasting award',\n",
       " 'forecasting optimal',\n",
       " 'model bakery',\n",
       " 'analysis statistical',\n",
       " 'planning analytics',\n",
       " 'forecasting automatic',\n",
       " 'rate',\n",
       " 'forecast probabilistic',\n",
       " 'learning world',\n",
       " 'probabilistic outlier',\n",
       " 'science journey',\n",
       " 'top data',\n",
       " 'data top',\n",
       " 'state data',\n",
       " 'data state',\n",
       " 'visualize data',\n",
       " 'data joy',\n",
       " '14-20 data',\n",
       " 'external data',\n",
       " 'data storytelling',\n",
       " 'cost data',\n",
       " 'income data',\n",
       " 'trial data',\n",
       " 'harm data',\n",
       " 'found data',\n",
       " 'high-quality data',\n",
       " 'compare data',\n",
       " 'data frame',\n",
       " 'erwin data',\n",
       " 'data catalog',\n",
       " 'spring data',\n",
       " 'chief data',\n",
       " 'data officer',\n",
       " 'data entering',\n",
       " 'data mandarin',\n",
       " 'data elca']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_phrases = []\n",
    "\n",
    "for kw in keywords:\n",
    "    key_phrases.append(kw[0])\n",
    "\n",
    "key_phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages of Unsupervised Sentimental Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consideering user would search for python doing Sentimental Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text preprocessing nlp python code articlevideo book article published part data science blogathon introduction natural language processing nlp branch data science ... post text preprocessing nlp python code appeared first analytics vidhya  0.175\n",
      "\n",
      "text preprocessing nlp python code articlevideo book article published part data science blogathon introduction natural language processing nlp branch data science ... post text preprocessing nlp python code appeared first analytics vidhya  {'neg': 0.0, 'neu': 0.925, 'pos': 0.075, 'compound': 0.3612}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "plotly cufflink advanced python data visualization library articlevideo book article published part data science blogathon introduction data visualization help bridge gap number ... post plotly cufflink advanced python data visualization library appeared first analytics vidhya  0.35000000000000003\n",
      "\n",
      "plotly cufflink advanced python data visualization library articlevideo book article published part data science blogathon introduction data visualization help bridge gap number ... post plotly cufflink advanced python data visualization library appeared first analytics vidhya  {'neg': 0.0, 'neu': 0.795, 'pos': 0.205, 'compound': 0.7184}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "eda exploratory data analysis python articlevideo book article published part data science blogathon introduction exploratory data analysis first important phase ... post eda exploratory data analysis python appeared first analytics vidhya  0.3\n",
      "\n",
      "eda exploratory data analysis python articlevideo book article published part data science blogathon introduction exploratory data analysis first important phase ... post eda exploratory data analysis python appeared first analytics vidhya  {'neg': 0.0, 'neu': 0.943, 'pos': 0.057, 'compound': 0.2023}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "python code snippet know check python code snippet start using solve everyday problem  -0.2\n",
      "\n",
      "python code snippet know check python code snippet start using solve everyday problem  {'neg': 0.174, 'neu': 0.71, 'pos': 0.116, 'compound': -0.2263}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "troubleshoot memory problem python memory problem hard diagnose fix python post go step-by-step process pinpoint fix memory leak using popular open source python package  0.10277777777777776\n",
      "\n",
      "troubleshoot memory problem python memory problem hard diagnose fix python post go step-by-step process pinpoint fix memory leak using popular open source python package  {'neg': 0.289, 'neu': 0.566, 'pos': 0.145, 'compound': -0.5574}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "kdnuggets™ news n22 jun data scientist extinct year generate automated pdf document python data scientist extinct year generate pdf document python top data science project beginner five type thinking high performing data scientist get interactive plot directly panda  -0.008000000000000007\n",
      "\n",
      "kdnuggets™ news n22 jun data scientist extinct year generate automated pdf document python data scientist extinct year generate pdf document python top data science project beginner five type thinking high performing data scientist get interactive plot directly panda  {'neg': 0.0, 'neu': 0.954, 'pos': 0.046, 'compound': 0.2023}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "top story jun 7-13 task automate python five type thinking high performing data scientist also generate automated pdf document python five type thinking high performing data scientist doubled income data science machine learning top data science project beginner  0.33\n",
      "\n",
      "top story jun 7-13 task automate python five type thinking high performing data scientist also generate automated pdf document python five type thinking high performing data scientist doubled income data science machine learning top data science project beginner  {'neg': 0.0, 'neu': 0.909, 'pos': 0.091, 'compound': 0.3818}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "generate automated pdf document python discover leverage automation create dazzling pdf document effortlessly  0.75\n",
      "\n",
      "generate automated pdf document python discover leverage automation create dazzling pdf document effortlessly  {'neg': 0.0, 'neu': 0.851, 'pos': 0.149, 'compound': 0.2732}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "kdnuggets™ news n21 jun task automate python doubled income data science machine learning task automate python doubled income data science machine learning shortage data science job next year make python code run incredibly fast stop start hiring data scientist  0.1\n",
      "\n",
      "kdnuggets™ news n21 jun task automate python doubled income data science machine learning task automate python doubled income data science machine learning shortage data science job next year make python code run incredibly fast stop start hiring data scientist  {'neg': 0.108, 'neu': 0.892, 'pos': 0.0, 'compound': -0.539}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "data visualization data analysis python using okcupid dataset part article dating data science please welcome guest author amy birdee done multiple data science hobby project recently built truly ... post data visualization data analysis python using okcupid dataset part appeared first data36  0.2625\n",
      "\n",
      "data visualization data analysis python using okcupid dataset part article dating data science please welcome guest author amy birdee done multiple data science hobby project recently built truly ... post data visualization data analysis python using okcupid dataset part appeared first data36  {'neg': 0.0, 'neu': 0.826, 'pos': 0.174, 'compound': 0.802}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "beautiful soup tutorial introduction web scraping python data scientist data analyst sooner later come point collect large amount data hobby ... post beautiful soup tutorial introduction web scraping python appeared first data36  0.43285714285714283\n",
      "\n",
      "beautiful soup tutorial introduction web scraping python data scientist data analyst sooner later come point collect large amount data hobby ... post beautiful soup tutorial introduction web scraping python appeared first data36  {'neg': 0.0, 'neu': 0.794, 'pos': 0.206, 'compound': 0.8316}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "real world application python skill python one frequently-recommended programming language probably heard people say relatively easy learn true python actually useful real-world application python skill got post look common use-cases post real world application python skill appeared first dataquest  0.20476190476190473\n",
      "\n",
      "real world application python skill python one frequently-recommended programming language probably heard people say relatively easy learn true python actually useful real-world application python skill got post look common use-cases post real world application python skill appeared first dataquest  {'neg': 0.0, 'neu': 0.807, 'pos': 0.193, 'compound': 0.8225}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "python practice free way improve python skill getting good python practice help solidify coding skill best resource practicing python post python practice free way improve python skill appeared first dataquest  0.55\n",
      "\n",
      "python practice free way improve python skill getting good python practice help solidify coding skill best resource practicing python post python practice free way improve python skill appeared first dataquest  {'neg': 0.0, 'neu': 0.509, 'pos': 0.491, 'compound': 0.969}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tutorial web scraping python using beautiful soup learn scrape web python internet absolutely massive source data data access using web scraping python fact web scraping often way access data lot information n't available convenient csv export post tutorial web scraping python using beautiful soup appeared first dataquest  0.47000000000000003\n",
      "\n",
      "tutorial web scraping python using beautiful soup learn scrape web python internet absolutely massive source data data access using web scraping python fact web scraping often way access data lot information n't available convenient csv export post tutorial web scraping python using beautiful soup appeared first dataquest  {'neg': 0.0, 'neu': 0.852, 'pos': 0.148, 'compound': 0.8316}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "learn python step-by-step learn python right way avoid `` cliff boring '' give best chance actually learn code following step post learn python step-by-step appeared first dataquest  0.08928571428571429\n",
      "\n",
      "learn python step-by-step learn python right way avoid `` cliff boring '' give best chance actually learn code following step post learn python step-by-step appeared first dataquest  {'neg': 0.134, 'neu': 0.682, 'pos': 0.184, 'compound': 0.4019}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "fun unique python project idea easy learning building project extremely succesful way learn building python project beginner difficult learn build success post fun unique python project idea easy learning appeared first dataquest  0.21416666666666667\n",
      "\n",
      "fun unique python project idea easy learning building project extremely succesful way learn building python project beginner difficult learn build success post fun unique python project idea easy learning appeared first dataquest  {'neg': 0.056, 'neu': 0.583, 'pos': 0.361, 'compound': 0.9274}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "beginner python tutorial analyze personal netflix data much time spent watching office netflix find entry-level tutorial analyzing netflix usage data post beginner python tutorial analyze personal netflix data appeared first dataquest  0.06999999999999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "beginner python tutorial analyze personal netflix data much time spent watching office netflix find entry-level tutorial analyzing netflix usage data post beginner python tutorial analyze personal netflix data appeared first dataquest  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "v python data analysis objective comparison python vs. better data science compare two language side side see python perform analysis step post v python data analysis objective comparison appeared first dataquest  0.1875\n",
      "\n",
      "v python data analysis objective comparison python vs. better data science compare two language side side see python perform analysis step post v python data analysis objective comparison appeared first dataquest  {'neg': 0.0, 'neu': 0.906, 'pos': 0.094, 'compound': 0.4404}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "long take learn python ready learn python data science right program habit structure master quickly might think post long take learn python appeared first dataquest  0.1615079365079365\n",
      "\n",
      "long take learn python ready learn python data science right program habit structure master quickly might think post long take learn python appeared first dataquest  {'neg': 0.0, 'neu': 0.906, 'pos': 0.094, 'compound': 0.3612}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tutorial web scraping python using beautiful soup web scraping allows u extract information web page tutorial 'll learn perform web scraping python beautifulsoup post tutorial web scraping python using beautiful soup appeared first dataquest  0.65\n",
      "\n",
      "tutorial web scraping python using beautiful soup web scraping allows u extract information web page tutorial 'll learn perform web scraping python beautifulsoup post tutorial web scraping python using beautiful soup appeared first dataquest  {'neg': 0.0, 'neu': 0.799, 'pos': 0.201, 'compound': 0.8316}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "post much analyze personal facebook data python q2 facebook claim billion active user mean 're reading article chance 're facebook user much facebook user much really post find using python specifically 're going use python create post post much analyze personal facebook data python appeared first dataquest  0.06712962962962964\n",
      "\n",
      "post much analyze personal facebook data python q2 facebook claim billion active user mean 're reading article chance 're facebook user much facebook user much really post find using python specifically 're going use python create post post much analyze personal facebook data python appeared first dataquest  {'neg': 0.0, 'neu': 0.866, 'pos': 0.134, 'compound': 0.7003}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "python api tutorial getting started apis data science tutorial learn apis analyzing data international space station step-by-step python api tutorial post python api tutorial getting started apis appeared first dataquest  0.125\n",
      "\n",
      "python api tutorial getting started apis data science tutorial learn apis analyzing data international space station step-by-step python api tutorial post python api tutorial getting started apis appeared first dataquest  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "learn python data science step interested learn python data science 's hard think 's clear roadmap learning python programming data science skill post learn python data science step appeared first dataquest  0.07708333333333334\n",
      "\n",
      "learn python data science step interested learn python data science 's hard think 's clear roadmap learning python programming data science skill post learn python data science step appeared first dataquest  {'neg': 0.04, 'neu': 0.807, 'pos': 0.153, 'compound': 0.5994}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ray data science distributed python task scale article dr dean wampler provides overview ray including raising question need article cover practical technique walk code help user get started post ray data science distributed python task scale appeared first data science blog domino  0.25\n",
      "\n",
      "ray data science distributed python task scale article dr dean wampler provides overview ray including raising question need article cover practical technique walk code help user get started post ray data science distributed python task scale appeared first data science blog domino  {'neg': 0.0, 'neu': 0.938, 'pos': 0.062, 'compound': 0.4019}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "differential evolution scratch python last updated june differential evolution heuristic approach global optimisation nonlinear non- post differential evolution scratch python appeared first machine learning mastery  0.08333333333333333\n",
      "\n",
      "differential evolution scratch python last updated june differential evolution heuristic approach global optimisation nonlinear non- post differential evolution scratch python appeared first machine learning mastery  {'neg': 0.0, 'neu': 0.902, 'pos': 0.098, 'compound': 0.3818}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "python script requesting user input repeatedly nbsp someone recently asked write python script took user input account took action based validity user input. nbsp code snippet developed different scenarios:1 game played ended. nbsp would ask user wanted play game_running true assuming game running play_again input `` play `` nbsp get user inputwhile true restart game answer `` '' `` '' nbsp nbsp nbsp nbsp nbsp nbsp nbsp play_again == play_again == `` '' nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp game_running true nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp break nbsp nbsp nbsp nbsp nbsp nbsp nbsp else print thank message exit program nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp print `` thank playing `` nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp game_running false nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp breakhere user answer either `` '' `` '' get play game again. nbsp type anything else receive thank message game ends.2 create loop validates user input float take action it. create loop seek user input fuel burn next second valid input provided cast valid user input float use try except statement account invalid user input nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp condition nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp input `` enter number `` nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp try nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp float nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp break nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp except valueerror nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp print str `` valid number. nbsp please try `` define function asks user input something answer `` yes '' something else answer `` '' prompt user answer neither `` yes '' `` '' .def user_input nbsp nbsp nbsp true nbsp nbsp nbsp nbsp nbsp nbsp nbsp input `` enter yes `` nbsp nbsp nbsp nbsp nbsp nbsp nbsp == `` yes '' == `` yes '' nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp continue nbsp nbsp nbsp nbsp nbsp nbsp nbsp elif == `` '' == `` '' nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp break nbsp nbsp nbsp nbsp nbsp nbsp nbsp else nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp print `` invalid input. nbsp please try `` nbsp user_input similar except function parameter.def user_input nbsp nbsp nbsp true nbsp nbsp nbsp nbsp nbsp nbsp nbsp input `` enter `` nbsp nbsp nbsp nbsp nbsp nbsp nbsp == `` '' == `` '' nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp continue nbsp nbsp nbsp nbsp nbsp nbsp nbsp elif == `` '' == `` '' nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp break nbsp nbsp nbsp nbsp nbsp nbsp nbsp else nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp print `` invalid input. nbsp please try `` user_input `` enter ''  -0.018421052631578963\n",
      "\n",
      "python script requesting user input repeatedly nbsp someone recently asked write python script took user input account took action based validity user input. nbsp code snippet developed different scenarios:1 game played ended. nbsp would ask user wanted play game_running true assuming game running play_again input `` play `` nbsp get user inputwhile true restart game answer `` '' `` '' nbsp nbsp nbsp nbsp nbsp nbsp nbsp play_again == play_again == `` '' nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp game_running true nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp break nbsp nbsp nbsp nbsp nbsp nbsp nbsp else print thank message exit program nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp print `` thank playing `` nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp game_running false nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp breakhere user answer either `` '' `` '' get play game again. nbsp type anything else receive thank message game ends.2 create loop validates user input float take action it. create loop seek user input fuel burn next second valid input provided cast valid user input float use try except statement account invalid user input nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp condition nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp input `` enter number `` nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp try nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp float nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp break nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp except valueerror nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp print str `` valid number. nbsp please try `` define function asks user input something answer `` yes '' something else answer `` '' prompt user answer neither `` yes '' `` '' .def user_input nbsp nbsp nbsp true nbsp nbsp nbsp nbsp nbsp nbsp nbsp input `` enter yes `` nbsp nbsp nbsp nbsp nbsp nbsp nbsp == `` yes '' == `` yes '' nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp continue nbsp nbsp nbsp nbsp nbsp nbsp nbsp elif == `` '' == `` '' nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp break nbsp nbsp nbsp nbsp nbsp nbsp nbsp else nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp print `` invalid input. nbsp please try `` nbsp user_input similar except function parameter.def user_input nbsp nbsp nbsp true nbsp nbsp nbsp nbsp nbsp nbsp nbsp input `` enter `` nbsp nbsp nbsp nbsp nbsp nbsp nbsp == `` '' == `` '' nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp continue nbsp nbsp nbsp nbsp nbsp nbsp nbsp elif == `` '' == `` '' nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp break nbsp nbsp nbsp nbsp nbsp nbsp nbsp else nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp nbsp print `` invalid input. nbsp please try `` user_input `` enter ''  {'neg': 0.0, 'neu': 0.892, 'pos': 0.108, 'compound': 0.9944}\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "choice = \"python\"\n",
    "\n",
    "New_df = Main_DF[Main_DF[\"Title_Lemmatized\"].str.contains(choice)]\n",
    "\n",
    "for text in New_df[\"Lemmatized\"]:\n",
    "    print(text, TextBlob(text).sentiment.polarity)\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    print()\n",
    "    print(text, sid.polarity_scores(text))\n",
    "    print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trying to build supervised Setimental Analyzer using Gaussian Naive Base Classifier\n",
    "Use Resturant Review dataset to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wow loved place', 'crust not good', 'not tasty texture nasty', 'stopped late may bank holiday rick steve recommendation loved', 'selection menu great price', 'getting angry want damn pho', 'honeslty taste fresh', 'potato like rubber could tell made ahead time kept warmer', 'fry great', 'great touch', 'service prompt', 'would not go back', 'cashier care ever say still ended wayyy overpriced', 'tried cape cod ravoli chicken cranberry mmmm', 'disgusted pretty sure human hair', 'shocked sign indicate cash', 'highly recommended', 'waitress little slow service', 'place not worth time let alone vega', 'not like', 'burrittos blah', 'food amazing', 'service also cute', 'could care le interior beautiful', 'performed', 'right red velvet cake ohhh stuff good', 'never brought salad asked', 'hole wall great mexican street taco friendly staff', 'took hour get food table restaurant food luke warm sever running around like totally overwhelmed', 'worst salmon sashimi', 'also combo like burger fry beer decent deal', 'like final blow', 'found place accident could not happier', 'seems like good quick place grab bite familiar pub food favor look elsewhere', 'overall like place lot', 'redeeming quality restaurant inexpensive', 'ample portion good price', 'poor service waiter made feel like stupid every time came table', 'first visit hiro delight', 'service suck', 'shrimp tender moist', 'not deal good enough would drag establishment', 'hard judge whether side good grossed melted styrofoam want eat fear getting sick', 'positive note server attentive provided great service', 'frozen puck disgust worst people behind register', 'thing like prime rib dessert section', 'bad food damn generic', 'burger good beef cooked right', 'want sandwich go firehouse', 'side greek salad greek dressing tasty pita hummus refreshing', 'ordered duck rare pink tender inside nice char outside', 'came running u realized husband left sunglass table', 'chow mein good', 'horrible attitude towards customer talk one customer enjoy food', 'portion huge', 'loved friendly server great food wonderful imaginative menu', 'heart attack grill downtown vega absolutely flat lined excuse restaurant', 'not much seafood like string pasta bottom', 'salad right amount sauce not power scallop perfectly cooked', 'ripped banana not ripped petrified tasteless', 'least think refill water struggle wave minute', 'place receives star appetizer', 'cocktail handmade delicious', 'definitely go back', 'glad found place', 'great food service huge portion give military discount', 'always great time do gringo', 'update went back second time still amazing', 'got food apparently never heard salt batter fish chewy', 'great way finish great', 'deal included tasting drink jeff went beyond expected', 'really really good rice time', 'service meh', 'took min get milkshake nothing chocolate milk', 'guess known place would suck inside excalibur use common sense', 'scallop dish quite appalling value well', 'time bad customer service', 'sweet potato fry good seasoned well', 'today second time lunch buffet pretty good', 'much good food vega feel cheated wasting eating opportunity going rice company', 'coming like experiencing underwhelming relationship party wait person ask break', 'walked place smelled like old grease trap others eating', 'turkey roast beef bland', 'place', 'pan cake everyone raving taste like sugary disaster tailored palate six year old', 'love pho spring roll oh yummy try', 'poor batter meat ratio made chicken tender unsatisfying', 'say food amazing', 'omelet die', 'everything fresh delicious', 'summary largely disappointing dining experience', 'like really sexy party mouth outrageously flirting hottest person party', 'never hard rock casino never ever step forward', 'best breakfast buffet', 'say bye bye tip lady', 'never go', 'back', 'food arrived quickly', 'not good', 'side cafe serf really good food', 'server fantastic found wife love roasted garlic bone marrow added extra meal another marrow go', 'good thing waiter helpful kept bloddy mary coming', 'best buffet town price cannot beat', 'loved mussel cooked wine reduction duck tender potato dish delicious', 'one better buffet', 'went tigerlilly fantastic afternoon', 'food delicious bartender attentive personable got great deal', 'ambience wonderful music playing', 'go back next trip', 'sooooo good', 'real sushi lover let honest yama not good', 'least min passed u ordering food arriving busy', 'really fantastic thai restaurant definitely worth visit', 'nice spicy tender', 'good price', 'check', 'pretty gross', 'better atmosphere', 'kind hard mess steak', 'although much liked look sound place actual experience bit disappointing', 'know place managed served blandest food ever eaten preparing indian cuisine', 'worst service boot least worry', 'service fine waitress friendly', 'guy steak steak loving son steak best worst place said best steak ever eaten', 'thought venture away get good sushi place really hit spot night', 'host staff lack better word bitch', 'bland not liking place number reason want waste time bad reviewing leave', 'phenomenal food service ambiance', 'return', 'definitely worth venturing strip pork belly return next time vega', 'place way overpriced mediocre food', 'penne vodka excellent', 'good selection food including massive meatloaf sandwich crispy chicken wrap delish tuna melt tasty burger', 'management rude', 'delicious nyc bagel good selection cream cheese real lox caper even', 'great subway fact good come every subway not meet expectation', 'seriously solid breakfast', 'one best bar food vega', 'extremely rude really many restaurant would love dine weekend vega', 'drink never empty made really great menu suggestion', '', 'waiter helpful friendly rarely checked u', 'husband ate lunch disappointed food service', 'red curry much bamboo shoot tasty', 'nice blanket moz top feel like done cover subpar food', 'bathroom clean place well decorated', 'menu always changing food quality going service extremely slow', 'service little slow considering served people server food coming slow pace', 'give thumb', 'watched waiter pay lot attention table ignore u', 'fianc came middle day greeted seated right away', 'great restaurant mandalay bay', 'waited forty five minute vain', 'crostini came salad stale', 'highlight great quality nigiri', 'staff friendly joint always clean', 'different cut piece day still wonderful tender well well flavored', 'ordered voodoo pasta first time really excellent pasta since going gluten free several year ago', 'place good', 'unfortunately must hit bakery leftover day everything ordered stale', 'came back today since relocated still not impressed', 'seated immediately', 'menu diverse reasonably priced', 'avoid cost', 'restaurant always full never wait', 'delicious', 'place hand one best place eat phoenix metro area', 'go looking good food', 'never treated bad', 'bacon hella salty', 'also ordered spinach avocado salad ingredient sad dressing literally zero taste', 'really vega fine dining used right menu handed lady price listed', 'waitress friendly', 'lordy khao soi dish not missed curry lover', 'everything menu terrific also thrilled made amazing accommodation vegetarian daughter', 'perhaps caught night judging review not inspired go back', 'service leaf lot desired', 'atmosphere modern hip maintaining touch coziness', 'not weekly haunt definitely place come back every', 'literally sat minute one asking take order', 'burger absolutely flavor meat totally bland burger overcooked charcoal flavor', 'also decided not send back waitress looked like verge heart attack', 'dressed treated rudely', 'probably dirt', 'love place hit spot want something healthy not lacking quantity flavor', 'ordered lemon raspberry ice cocktail also incredible', 'food sucked expected sucked could imagined', 'interesting decor', 'really like crepe station', 'also served hot bread butter home made potato chip bacon bit top original good', 'watch preparing delicious food', 'egg roll fantastic', 'order arrived one gyro missing', 'salad wing ice cream dessert left feeling quite satisfied', 'not really sure joey voted best hot dog valley reader phoenix magazine', 'best place go tasty bowl pho', 'live music friday totally blow', 'never insulted felt disrespected', 'friendly staff', 'worth drive', 'heard good thing place exceeding every hope could dreamed', 'food great serivce', 'warm beer help', 'great brunch spot', 'service friendly inviting', 'good lunch spot', 'lived since first last time stepped foot place', 'worst experience ever', 'must night place', 'side delish mixed mushroom yukon gold puree white corn beateous', 'bug never showed would given sure side wall bug climbing kitchen', 'minute waiting salad realized coming time soon', 'friend loved salmon tartar', 'go back', 'extremely tasty', 'waitress good though', 'soggy not good', 'jamaican mojitos delicious', 'small not worth price', 'food rich order accordingly', 'shower area outside rinse not take full shower unless mind nude everyone see', 'service bit lacking', 'lobster bisque bussell sprout risotto filet needed salt pepper course none table', 'hopefully bodes going business someone cook come', 'either cold not enough flavor bad', 'loved bacon wrapped date', 'unbelievable bargain', 'folk otto always make u feel welcome special', 'main also uninspired', 'place first pho amazing', 'wonderful experience made place must stop whenever town', 'food bad enough enjoy dealing world worst annoying drunk people', 'fun chef', 'ordered double cheeseburger got single patty falling apart picture uploaded yeah still suck', 'great place couple drink watch sporting event wall covered tv', 'possible give zero star', 'description said yum yum sauce another said eel sauce yet another said spicy mayo well none roll sauce', 'say would hardest decision honestly dish taste supposed taste amazing', 'not rolled eye may stayed not sure go back try', 'everyone attentive providing excellent customer service', 'horrible waste time money', 'dish quite flavourful', 'time side restaurant almost empty excuse', 'busy either also building freezing cold', 'like reviewer said pay eat place', 'drink took close minute come one point', 'seriously flavorful delight folk', 'much better ayce sushi place went vega', 'lighting dark enough set mood', 'based sub par service received effort show gratitude business going back', 'owner really great people', 'nothing privileged working eating', 'greek dressing creamy flavorful', 'overall think would take parent place made similar complaint silently felt', 'pizza good peanut sauce tasty', 'table service pretty fast', 'fantastic service', 'well would given godfather zero star possible', 'know make', 'tough short flavor', 'hope place stick around', 'bar vega not ever recall charged tap water', 'restaurant atmosphere exquisite', 'good service clean inexpensive boot', 'seafood fresh generous portion', 'plus buck', 'service not par either', 'thus far visited twice food absolutely delicious time', 'good year ago', 'self proclaimed coffee cafe wildly disappointed', 'veggitarian platter world', 'cant go wrong food', 'beat', 'stopped place madison ironman friendly kind staff', 'chef friendly good job', 'better not dedicated boba tea spot even jenni pho', 'liked patio service outstanding', 'goat taco skimp meat wow flavor', 'think not', 'mac salad pretty bland not getting', 'went bachi burger friend recommendation not disappointed', 'service stink', 'waited waited', 'place not quality sushi not quality restaurant', 'would definitely recommend wing well pizza', 'great pizza salad', 'thing went wrong burned saganaki', 'waited hour breakfast could done time better home', 'place amazing', 'hate disagree fellow yelpers husband disappointed place', 'waited hour never got either pizza many around u came later', 'know slow', 'staff great food delish incredible beer selection', 'live neighborhood disappointed back convenient location', 'know pulled pork could soooo delicious', 'get incredibly fresh fish prepared care', 'go gave star rating please know third time eating bachi burger writing review', 'love fact everything menu worth', 'never dining place', 'food excellent service good', 'good beer drink selection good food selection', 'please stay away shrimp stir fried noodle', 'potato chip order sad could probably count many chip box probably around', 'food really boring', 'good service check', 'greedy corporation never see another dime', 'never ever go back', 'much like go back get passed atrocious service never return', 'summer dine charming outdoor patio delightful', 'not expect good', 'fantastic food', 'ordered toasted english muffin came untoasted', 'food good', 'never going back', 'great food price high quality house made', 'bus boy hand rude', 'point friend basically figured place joke mind making publicly loudly known', 'back good bbq lighter fare reasonable pricing tell public back old way', 'considering two u left full happy go wrong', 'bread made house', 'downside service', 'also fry without doubt worst fry ever', 'service exceptional food good review', 'couple month later returned amazing meal', 'favorite place town shawarrrrrrma', 'black eyed pea sweet potato unreal', 'disappointed', 'could serve vinaigrette may make better overall dish still good', 'go far many place never seen restaurant serf egg breakfast especially', 'mom got home immediately got sick bite salad', 'server not pleasant deal always honor pizza hut coupon', 'truly unbelievably good glad went back', 'fantastic service pleased atmosphere', 'everything gross', 'love place', 'great service food', 'first bathroom location dirty seat cover not replenished plain yucky', 'burger got gold standard burger kind disappointed', 'omg food delicioso', 'nothing authentic place', 'spaghetti nothing special whatsoever', 'dish salmon best great', 'vegetable fresh sauce feel like authentic thai', 'worth driving tucson', 'selection probably worst seen vega none', 'pretty good beer selection', 'place like chipotle better', 'classy warm atmosphere fun fresh appetizer succulent steak baseball steak', 'star brick oven bread app', 'eaten multiple time time food delicious', 'sat another ten minute finally gave left', 'terrible', 'everyone treated equally special', 'take min pancake egg', 'delicious', 'good side staff genuinely pleasant enthusiastic real treat', 'sadly gordon ramsey steak place shall sharply avoid next trip vega', 'always evening wonderful food delicious', 'best fish ever life', 'bathroom next door nice', 'buffet small food offered bland', 'outstanding little restaurant best food ever tasted', 'pretty cool would say', 'definitely turn doubt back unless someone else buying', 'server great job handling large rowdy table', 'find wasting food despicable food', 'wife lobster bisque soup lukewarm', 'would come back sushi craving vega', 'staff great ambiance great', 'deserves star', 'left stomach ache felt sick rest day', 'dropped ball', 'dining space tiny elegantly decorated comfortable', 'customize order way like usual eggplant green bean stir fry love', 'bean rice mediocre best', 'best taco town far', 'took back money got outta', 'interesting part town place amazing', 'rude inconsiderate management', 'staff not friendly wait time served horrible one even say hi first minute', 'back', 'great dinner', 'service outshining definitely recommend halibut', 'food terrible', 'never ever go back told many people happened', 'recommend unless car break front starving', 'come back every time vega', 'place deserves one star food', 'disgrace', 'def coming back bowl next time', 'want healthy authentic ethic food try place', 'continue come lady night andddd date night highly recommend place anyone area', 'several time past experience always great', 'walked away stuffed happy first vega buffet experience', 'service excellent price pretty reasonable considering vega located inside crystal shopping mall aria', 'summarize food incredible nay transcendant nothing brings joy quite like memory pneumatic condiment dispenser', 'probably one people ever go ians not like', 'kid pizza always hit lot great side dish option kiddos', 'service perfect family atmosphere nice see', 'cooked perfection service impeccable', 'one simply disappointment', 'overall disappointed quality food bouchon', 'accountant know getting screwed', 'great place eat reminds little mom pop shop san francisco bay area', 'today first taste buldogis gourmet hot dog tell ever thought possible', 'left frustrated', 'definitely soon', 'food really good got full petty fast', 'service fantastic', 'total waste time', 'know kind best iced tea', 'come hungry leave happy stuffed', 'service give star', 'assure disappointed', 'take little bad service food suck', 'gave trying eat crust teeth still sore', 'completely grossed', 'really enjoyed eating', 'first time going think quickly become regular', 'server nice even though looked little overwhelmed need stayed professional friendly end', 'dinner companion told everything fresh nice texture taste', 'ground right next table large smeared stepped tracked everywhere pile green bird poop', 'furthermore even find hour operation website', 'tried like place time think done', 'mistake', 'complaint', 'seriously good pizza expert connisseur topic', 'waiter jerk', 'strike want rushed', 'nicest restaurant owner ever come across', 'never come', 'loved biscuit', 'service quick friendly', 'ordered appetizer took minute pizza another minute', 'absolutley fantastic', 'huge awkward lb piece cow th gristle fat', 'definitely come back', 'like steiner dark feel like bar', 'wow spicy delicious', 'not familiar check', 'take business dinner dollar elsewhere', 'love go back', 'anyway f restaurant wonderful breakfast lunch', 'nothing special', 'day week different deal delicious', 'not mention combination pear almond bacon big winner', 'not back', 'sauce tasteless', 'food delicious spicy enough sure ask spicier prefer way', 'ribeye steak cooked perfectly great mesquite flavor', 'think going back anytime soon', 'food gooodd', 'far sushi connoisseur definitely tell difference good food bad food certainly bad food', 'insulted', 'last time lunch bad', 'chicken wing contained driest chicken meat ever eaten', 'food good enjoyed every mouthful enjoyable relaxed venue couple small family group etc', 'nargile think great', 'best tater tot southwest', 'loved place', 'definitely not worth paid', 'vanilla ice cream creamy smooth profiterole chou pastry fresh enough', 'im az time new spot', 'manager worst', 'inside really quite nice clean', 'food outstanding price reasonable', 'think running back carly anytime soon food', 'due fact took minute acknowledged another minute get food kept forgetting thing', 'love margarita', 'first vega buffet not disappoint', 'good though', 'one note ventilation could use upgrading', 'great pork sandwich', 'waste time', 'total letdown would much rather go camelback flower shop cartel coffee', 'third cheese friend burger cold', 'enjoy pizza brunch', 'steak well trimmed also perfectly cooked', 'group claimed would handled u beautifully', 'loved', 'asked bill leave without eating bring either', 'place jewel la vega exactly hoping find nearly ten year living', 'seafood limited boiled shrimp crab leg crab leg definitely not taste fresh', 'selection food not best', 'delicious absolutely back', 'small family restaurant fine dining establishment', 'toro tartare cavier extraordinary liked thinly sliced wagyu white truffle', 'dont think back long time', 'attached gas station rarely good sign', 'awesome', 'back many time soon', 'menu much good stuff could not decide', 'worse humiliated worker right front bunch horrible name calling', 'conclusion filling meal', 'daily special always hit group', 'tragedy struck', 'pancake also really good pretty large', 'first crawfish experience delicious', 'monster chicken fried steak egg time favorite', 'waitress sweet funny', 'also taste mom multi grain pumpkin pancake pecan butter amazing fluffy delicious', 'rather eat airline food seriously', 'cant say enough good thing place', 'ambiance incredible', 'waitress manager friendly', 'would not recommend place', 'overall impressed noca', 'gyro basically lettuce', 'terrible service', 'thoroughly disappointed', 'much pasta love homemade hand made pasta thin pizza', 'give try happy', 'far best cheesecurds ever', 'reasonably priced also', 'everything perfect night', 'food good typical bar food', 'drive get', 'first glance lovely bakery cafe nice ambiance clean friendly staff', 'anyway not think go back', 'point finger item menu order disappointed', 'oh thing beauty restaurant', 'gone go', 'greasy unhealthy meal', 'first time might last', 'burger amazing', 'similarly delivery man not say word apology food minute late', 'way expensive', 'sure order dessert even need pack go tiramisu cannoli die', 'first time wait next', 'bartender also nice', 'everything good tasty', 'place two thumb way', 'best place vega breakfast check sat sun', 'love authentic mexican food want whole bunch interesting yet delicious meat choose need try place', 'terrible management', 'excellent new restaurant experienced frenchman', 'zero star would give zero star', 'great steak great side great wine amazing dessert', 'worst martini ever', 'steak shrimp opinion best entree gc', 'opportunity today sample amazing pizza', 'waited thirty minute seated although vacant table folk waiting', 'yellowtail carpaccio melt mouth fresh', 'try going back even empty', 'going eat potato found stranger hair', 'spicy enough perfect actually', 'last night second time dining happy decided go back', 'not even hello right', 'dessert bit strange', 'boyfriend came first time recent trip vega could not pleased quality food service', 'really recommend place go wrong donut place', 'nice ambiance', 'would recommend saving room', 'guess maybe went night disgraceful', 'however recent experience particular location not good', 'know not like restaurant something', 'avoid establishment', 'think restaurant suffers not trying hard enough', 'tapa dish delicious', 'heart place', 'salad bland vinegrette baby green heart palm', 'two felt disgusting', 'good time', 'believe place great stop huge belly hankering sushi', 'generous portion great taste', 'never go back place never ever recommended place anyone', 'server went back forth several time not even much helped', 'food delicious', 'hour seriously', 'consider theft', 'eew location need complete overhaul', 'recently witnessed poor quality management towards guest well', 'waited waited waited', 'also came back check u regularly excellent service', 'server super nice checked u many time', 'pizza tasted old super chewy not good way', 'swung give try deeply disappointed', 'service good company better', 'staff also friendly efficient', 'service fan quick served nice folk', 'boy sucker dry', 'rated', 'look authentic thai food go else', 'steak recommended', 'pulled car waited another minute acknowledged', 'great food great service clean friendly setting', 'assure back', 'hate thing much cheap quality black olive', 'breakfast perpared great beautiful presentation giant slice toast lightly dusted powdered sugar', 'kid play area nasty', 'great place fo take eat', 'waitress friendly happy accomodate vegan veggie option', 'omg felt like never eaten thai food dish', 'extremely crumby pretty tasteless', 'pale color instead nice char flavor', 'crouton also taste homemade extra plus', 'got home see driest damn wing ever', 'regular stop trip phoenix', 'really enjoyed crema caf expanded even told friend best breakfast', 'not good money', 'miss wish one philadelphia', 'got sitting fairly fast ended waiting minute place order another minute food arrived', 'also best cheese crisp town', 'good value great food great service', 'ask satisfying meal', 'food good', 'awesome', 'wanted leave', 'made drive way north scottsdale not one bit disappointed', 'not eating', 'owner really really need quit soooooo cheap let wrap freaking sandwich two paper not one', 'checked place couple year ago not impressed', 'chicken got definitely reheated ok wedge cold soggy', 'sorry not getting food anytime soon', 'absolute must visit', 'cow tongue cheek taco amazing', 'friend not like bloody mary', 'despite hard rate business actually rare give star', 'really want make experience good one', 'not return', 'chicken pho tasted bland', 'disappointing', 'grilled chicken tender yellow saffron seasoning', 'drive thru mean not want wait around half hour food somehow end going make u wait wait', 'pretty awesome place', 'ambience perfect', 'best luck rude non customer service focused new management', 'grandmother make roasted chicken better one', 'asked multiple time wine list time ignored went hostess got one', 'staff always super friendly helpful especially cool bring two small boy baby', 'four star food guy blue shirt great vibe still letting u eat', 'roast beef sandwich tasted really good', 'evening drastically sick', 'high quality chicken chicken caesar salad', 'ordered burger rare came done', 'promptly greeted seated', 'tried go lunch madhouse', 'proven dead wrong sushi bar not quality great service fast food impeccable', 'waiting hour seated not greatest mood', 'good joint', 'macarons insanely good', 'not eating', 'waiter attentive friendly informative', 'maybe cold would somewhat edible', 'place lot promise fails deliver', 'bad experience', 'mistake', 'food average best', 'great food', 'going back anytime soon', 'disappointed ordered big bay plater', 'great place relax awesome burger beer', 'perfect sit family meal get together friend', 'not much flavor poorly constructed', 'patio seating comfortable', 'fried rice dry well', 'hand favorite italian restaurant', 'scream legit book somethat also pretty rare vega', 'not fun experience', 'atmosphere great lovely duo violinist playing song requested', 'personally love hummus pita baklava falafel baba ganoush amazing eggplant', 'convenient since staying mgm', 'owner super friendly staff courteous', 'great', 'eclectic selection', 'sweet potato tot good onion ring perfection close', 'staff attentive', 'chef generous time even came around twice take picture', 'owner used work nobu place really similar half price', 'google mediocre imagine smashburger pop', 'dont go', 'promise disappoint', 'sushi lover avoid place mean', 'great double cheeseburger', 'awesome service food', 'fantastic neighborhood gem', 'wait go back', 'plantain worst ever tasted', 'great place highly recommend', 'service slow not attentive', 'gave star giving star', 'staff spends time talking', 'dessert panna cotta amazing', 'good food great atmosphere', 'damn good steak', 'total brunch fail', 'price reasonable flavor spot sauce home made slaw not drenched mayo', 'decor nice piano music soundtrack pleasant', 'steak amazing rge fillet relleno best seafood plate ever', 'good food good service', 'absolutely amazing', 'probably back honest', 'definitely back', 'sergeant pepper beef sandwich auju sauce excellent sandwich well', 'hawaiian breeze mango magic pineapple delight smoothy tried far good', 'went lunch service slow', 'much say place walked expected amazing quickly disappointed', 'mortified', 'needle say never back', 'anyways food definitely not filling price pay expect', 'chip came dripping grease mostly not edible', 'really impressed strip steak', 'going since every meal awesome', 'server nice attentive serving staff', 'cashier friendly even brought food', 'work hospitality industry paradise valley refrained recommending cibo longer', 'atmosphere fun', 'would not recommend others', 'service quick even go order like like', 'mean really get famous fish chip terrible', 'said mouth belly still quite pleased', 'not thing', 'thumb', 'reading please go', 'loved grilled pizza reminded legit italian pizza', 'pro large seating area nice bar area great simple drink menu best brick oven pizza homemade dough', 'really nice atmosphere', 'tonight elk filet special sucked', 'one bite hooked', 'ordered old classic new dish going time sorely disappointed everything', 'cute quaint simple honest', 'chicken deliciously seasoned perfect fry outside moist chicken inside', 'food great always compliment chef', 'special thanks dylan recommendation order yummy tummy', 'awesome selection beer', 'great food awesome service', 'one nice thing added gratuity bill since party larger expect tip', 'fly apple juice fly', 'han nan chicken also tasty', 'service thought good', 'food barely lukewarm must sitting waiting server bring u', 'ryan bar definitely one edinburgh establishment revisiting', 'nicest chinese restaurant', 'overall like food service', 'also serve indian naan bread hummus spicy pine nut sauce world', 'probably never coming back recommend', 'friend pasta also bad barely touched', 'try airport experience tasty food speedy friendly service', 'love decor chinese calligraphy wall paper', 'never anything complain', 'restaurant clean family restaurant feel', 'way fried', 'not sure long stood long enough begin feel awkwardly place', 'opened sandwich impressed not good way', 'not back', 'warm feeling service felt like guest special treat', 'extensive menu provides lot option breakfast', 'always order vegetarian menu dinner wide array option choose', 'watched price inflate portion get smaller management attitude grow rapidly', 'wonderful lil tapa ambience made feel warm fuzzy inside', 'got enjoy seafood salad fabulous vinegrette', 'wonton thin not thick chewy almost melt mouth', 'level spicy perfect spice whelm soup', 'sat right time server get go fantastic', 'main thing enjoy crowd older crowd around mid', 'side town definitely spot hit', 'wait minute get drink longer get arepas', 'great place eat', 'jalapeno bacon soooo good', 'service poor thats nice', 'food good service good price good', 'place not clean food oh stale', 'chicken dish ok beef like shoe leather', 'service beyond bad', 'happy', 'tasted like dirt', 'one place phoenix would definately go back', 'block amazing', 'close house low key non fancy affordable price good food', 'hot sour egg flower soup absolutely star', 'sashimi poor quality soggy tasteless', 'great time family dinner sunday night', 'food not tasty not say real traditional hunan style', 'bother slow service', 'flair bartender absolutely amazing', 'frozen margarita way sugary taste', 'good ordered twice', 'nutshell restaraunt smell like combination dirty fish market sewer', 'girlfriend veal bad', 'unfortunately not good', 'pretty satifying experience', 'join club get awesome offer via email', 'perfect someone like beer ice cold case even colder', 'bland flavorless good way describing barely tepid meat', 'chain fan beat place easily', 'nacho must', 'not coming back', 'many word say place everything pretty well', 'staff super nice quick even crazy crowd downtown jury lawyer court staff', 'great atmosphere friendly fast service', 'received pita huge lot meat thumb', 'food arrives meh', 'paying hot dog fry look like came kid meal wienerschnitzel not idea good meal', 'classic maine lobster roll fantastic', 'brother law work mall ate day guess sick night', 'good going review place twice hereas tribute place tribute event held last night', 'chip salsa really good salsa fresh', 'place great', 'mediocre food', 'get inside impressed place', 'super pissd', 'service super friendly', 'sad little vegetable overcooked', 'place nice surprise', 'golden crispy delicious', 'high hope place since burger cooked charcoal grill unfortunately taste fell flat way flat', 'could eat bruschetta day devine', 'not single employee came see ok even needed water refill finally served u food', 'lastly mozzarella stick best thing ordered', 'first time ever came amazing experience still tell people awesome duck', 'server negligent need made u feel unwelcome would not suggest place', 'service terrible though', 'place overpriced not consistent boba really overpriced', 'packed', 'love place', 'say dessert yummy', 'food terrible', 'seasonal fruit fresh white peach puree', 'kept getting worse worse officially done', 'place honestly blown', 'definitely would not eat', 'not waste money', 'love put food nice plastic container opposed cramming little paper takeout box', 'cr pe delicate thin moist', 'awful service', 'ever go', 'food quality horrible', 'price think place would much rather gone', 'service fair best', 'love sushi found kabuki priced hip service', 'favor stay away dish', 'poor service', 'one table thought food average worth wait', 'best service food ever maria server good friendly made day', 'excellent', 'paid bill not tip felt server terrible job', 'lunch great experience', 'never bland food surprised considering article read focused much spice flavor', 'food way overpriced portion fucking small', 'recently tried caballero back every week since', 'buck head really expect better food', 'food came good pace', 'ate twice last visit especially enjoyed salmon salad', 'back', 'could not believe dirty oyster', 'place deserves star', 'would not recommend place', 'fact going round star awesome', 'disbelief dish qualified worst version food ever tasted', 'bad day not low tolerance rude customer service people job nice polite wash dish otherwise', 'potato great biscuit', 'probably would not go', 'flavorful perfect amount heat', 'price reasonable service great', 'wife hated meal coconut shrimp friend really not enjoy meal either', 'fella got huevos rancheros look appealing', 'went happy hour great list wine', 'may say buffet pricey think get pay place getting quite lot', 'probably coming back', 'worst food service', 'place pretty good nice little vibe restaurant', 'talk great customer service course back', 'hot dish not hot cold dish close room temp watched staff prepare food bare hand glove everything deep fried oil', 'love fry bean', 'always pleasure dealing', 'plethora salad sandwich everything tried get seal approval', 'place awesome want something light healthy summer', 'sushi strip place go', 'service great even manager came helped table', 'feel dining room college cooking course high class dining service slow best', 'started review two star editing give one', 'worst sushi ever eat besides costco', 'excellent restaurant highlighted great service unique menu beautiful setting', 'boyfriend sat bar completely delightful experience', 'weird vibe owner', 'hardly meat', 'better bagel grocery store', 'go place gyro', 'love owner chef one authentic japanese cool dude', 'burger good pizza used amazing doughy flavorless', 'found six inch long piece wire salsa', 'service terrible food mediocre', 'definately enjoyed', 'ordered albondigas soup warm tasted like tomato soup frozen meatball', 'three different occasion asked well done medium well three time got bloodiest piece meat plate', 'two bite refused eat anymore', 'service extremely slow', 'minute wait got table', 'seriously killer hot chai latte', 'allergy warning menu waitress absolutely clue meal not contain peanut', 'boyfriend tried mediterranean chicken salad fell love', 'rotating beer tap also highlight place', 'pricing bit concern mellow mushroom', 'worst thai ever', 'stay vega must get breakfast least', 'want first say server great perfect service', 'pizza selection good', 'strawberry tea good', 'highly unprofessional rude loyal patron', 'overall great experience', 'spend money elsewhere', 'regular toasted bread equally satisfying occasional pat butter mmmm', 'buffet bellagio far anticipated', 'drink weak people', 'order not correct', 'also feel like chip bought not made house', 'disappointing dinner went elsewhere dessert', 'chip sals amazing', 'returning', 'new fav vega buffet spot', 'seriously cannot believe owner many unexperienced employee running around like chicken head cut', 'sad', 'felt insulted disrespected could talk judge another human like', 'call steakhouse properly cook steak understand', 'not impressed concept food', 'thing crazy guacamole like pur ed', 'really nothing postinos hope experience better', 'got food poisoning buffet', 'brought fresh batch fry thinking yay something warm', 'hilarious yummy christmas eve dinner remember biggest fail entire trip u', 'needle say going back anytime soon', 'place disgusting', 'every time eat see caring teamwork professional degree', 'ri style calamari joke', 'however much garlic fondue barely edible', 'could barely stomach meal complain business lunch', 'bad lost heart finish', 'also took forever bring u check asked', 'one make scene restaurant get definitely lost love one', 'disappointing experience', 'food par denny say not good', 'want wait mediocre food downright terrible service place', 'waaaaaayyyyyyyyyy rated saying', 'going back', 'place fairly clean food simply worth', 'place lacked style', 'sangria half glass wine full ridiculous', 'bother coming', 'meat pretty dry sliced brisket pulled pork', 'building seems pretty neat bathroom pretty trippy eat', 'equally awful', 'probably not hurry go back', 'slow seating even reservation', 'not good stretch imagination', 'cashew cream sauce bland vegetable undercooked', 'chipolte ranch dipping sause tasteless seemed thin watered heat', 'bit sweet not really spicy enough lacked flavor', 'disappointed', 'place horrible way overpriced', 'maybe vegetarian fare twice thought average best', 'busy know', 'table outside also dirty lot time worker not always friendly helpful menu', 'ambiance not feel like buffet setting douchey indoor garden tea biscuit', 'con spotty service', 'fry not hot neither burger', 'came back cold', 'food came disappointment ensued', 'real disappointment waiter', 'husband said rude not even apologize bad food anything', 'reason eat would fill night binge drinking get carbs stomach', 'insult profound deuchebaggery go outside smoke break serving solidify', 'someone order two taco think may part customer service ask combo ala cart', 'quite disappointed although blame need placed door', 'rave review wait eat disappointment', 'del taco pretty nasty avoided possible', 'not hard make decent hamburger', 'like', 'hell go back', 'gotten much better service pizza place next door service received restaurant', 'know big deal place back ya', 'immediately said wanted talk manager not want talk guy shot fireball behind bar', 'ambiance much better', 'unfortunately set u disapppointment entree', 'food good', 'server suck wait correction server heimer sucked', 'happened next pretty putting', 'bad cause know family owned really wanted like place', 'overpriced getting', 'vomited bathroom mid lunch', 'kept looking time soon become minute yet still food', 'place eat circumstance would ever return top list', 'started tuna sashimi brownish color obviously fresh', 'food average', 'sure beat nacho movie would expect little bit coming restaurant', 'ha long bay bit flop', 'problem charge sandwich bigger subway sub offer better amount vegetable', 'shrimp unwrapped live mile brushfire literally ice cold', 'lacked flavor seemed undercooked dry', 'really impressive place closed', 'would avoid place staying mirage', 'refried bean came meal dried crusty food bland', 'spend money time place else', 'lady table next u found live green caterpillar salad', 'presentation food awful', 'tell disappointed', 'think food flavor texture lacking', 'appetite instantly gone', 'overall not impressed would not go back', 'whole experience underwhelming think go ninja sushi next time', 'wasted enough life poured salt wound drawing time took bring check']\n",
      "[[1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]]\n",
      "[[19 19]\n",
      " [ 2 35]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.72"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "dataset = pd.read_csv('Restaurant_Reviews.tsv', delimiter = '\\t', quoting = 3)\n",
    "\n",
    "corpus = []\n",
    "for i in range(0, 1000):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    all_stopwords = stopwords.words('english')\n",
    "    all_stopwords.remove('not')\n",
    "    review = [lemmar.lemmatize(word) for word in review if not word in set(all_stopwords)]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)\n",
    "print(corpus)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features = 2500)\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "y = dataset.iloc[:, -1].values\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.075, random_state = 0)\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trying to apply the supervised model on RSS Feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 21)\t2\n",
      "  (0, 16)\t2\n",
      "  (0, 13)\t3\n",
      "  (0, 19)\t2\n",
      "  (0, 7)\t2\n",
      "  (0, 3)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 18)\t1\n",
      "  (0, 14)\t1\n",
      "  (0, 8)\t2\n",
      "  (0, 20)\t2\n",
      "  (0, 4)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 12)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 17)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 15)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 0)\t1\n",
      "  (0, 22)\t1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (1,23) (1767,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-121-530f52f18118>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mX_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mjll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_joint_log_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_joint_log_likelihood\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    454\u001b[0m             \u001b[0mjointi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_prior_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m             \u001b[0mn_ij\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigma_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m             n_ij -= 0.5 * np.sum(((X - self.theta_[i, :]) ** 2) /\n\u001b[0m\u001b[1;32m    457\u001b[0m                                  (self.sigma_[i, :]), 1)\n\u001b[1;32m    458\u001b[0m             \u001b[0mjoint_log_likelihood\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjointi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn_ij\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (1,23) (1767,) "
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(max_features = 2500)\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "y = dataset.iloc[:, -1].values\n",
    "\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X, y)\n",
    "\n",
    "\n",
    "for text in New_df[\"Lemmatized\"]:\n",
    "    cv = CountVectorizer(max_features = 5000)\n",
    "    X_text = cv.fit_transform([text])\n",
    "    print(X_text)\n",
    "    y_pred = classifier.predict(X_text.toarray())\n",
    "    print(text, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Got wrong values , this is because Resturant review Dataset is completely from RSS Feed data\n",
    "### It will be impossible to broadcast on over the other. Its not just resturant review but any other dataset will give this un-acceptable results. the only dataset that could applied would be RSS FEED Dataset(which is not available).\n",
    "## Planning to unsupervised Approach only."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
